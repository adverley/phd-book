% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

\providecommand{\home}{../..}
\documentclass[\home/main.tex]{subfiles}

\begin{document}
TODO: beschrijven dat je de termen reward function en progression metric interachangbly gebruikt. 
% LITERATUUR: [2021] how to train your robot with DRL heeft als sectie 4.4 nog interessante opbouw over waarom we niet zomaar lfd willen gaan toepassen. 

\chapter{Learning reward functions from demonstrations}\label{ch:reward_functions}

To employ our dataset of people folding clothes (\cref{ch:data_collection}), we propose distilling the task intent and expressing it as a scalar value. This scalar value is a metric representing task progression which can be used for monitoring processes or as reward function in \gls{RL}. Hence we will use the terms \enquote{reward function} and \enquote{task progression metric} interchangebly. 

We present a method to express the task progression from multiperspective video demonstrations of people folding clothes. A salient feature of our approach is that it does not require any manual-defined labels. First, we describe our rationale and related work in \cref{sec:rewards_rationale}. Next, we give a high-level overview of our proposed approach in \cref{sec:rewards_overview} and then discuss all components in \cref{sec:rewards_methodology}. We demonstrate quantitative results of learning progression metrics in \cref{sec:rewards_results}. The discussion sections of this book frequently touch on the subject that a progression metric for folding cloth is ill-defined. For this reason, we provide an in-depth discussion in \cref{sec:rewards_discuss}. This discussion decodes which visual features in the scene the progression metric is attending to. We also conduct case-based adversarial experiments to test which invariances are learned and how robust our method is. Finally, \cref{sec:rewards_conc} concludes our work on learning perceptual reward functions.

% ===================================================
\section{Rational and related work} \label{sec:rewards_rationale}

% ===================================================
\section{Overview of the proposed framework to learn reward functions} \label{sec:rewards_overview}

% ===================================================

\section{Methodology for unsupervised learning of reward functions}\label{sec:rewards_methodology}

\subsection{Learning semantic meaningful embeddings using TCNs}\label{subsec:rewards_tcn}

\subsection{Aligning expert video embeddings with query videos} \label{subsec:rewards_dtw}

\subsection{Extracting task progression from embeddings} \label{subsec:rewards_extract}

% ===================================================

\section{Results on folding clothing}\label{sec:rewards_results}

\subsection{Folding demonstration dataset}

\subsection{Training results}

\subsection{Reward function results}

% ===================================================

\section{Discussion}\label{sec:rewards_discuss}

\subsection{Semantic meaning of learned TCN embeddings {\tiny UMAP + clustering}}

\subsection{Case-based examples for post hoc interpretability}

% ===================================================

\section{Conclusion} \label{sec:rewards_conc}

\end{document}