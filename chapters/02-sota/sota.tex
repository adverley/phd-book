% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

\providecommand{\home}{../..}
\documentclass[\home/main.tex]{subfiles}

\begin{document}

\chapter{Background and review of related work} \label{ch:lit}

The following chapter provides the preliminaries and a review of relevant work in the field of robotic manipulation of deformable objects. To give historical context, we first discuss how \emph{standard robotic manipulation pipelines} can be used for manipulating deformable objects in \cref{sec:lit_traditional}. Next, in \cref{sec:lit_learning}, we introduce how the inherent limitations of engineered motor control architectures can be overcome by using \emph{learning-based methods}. We break this section down into subsections introducing supervised learning, deep neural networks and reinforcement learning. We follow up this introduction of machine learning methods by surveying their applications in recent robotic manipulation work. Given the general property that learning-based methods are data-hungry, we continue this discussion by reviewing the role of \emph{large datasets} for robotic learning in \cref{sec:lit_datasets}. An alternative approach to generating data is to use synthetic data. To this end, we discuss the role of \emph{simulation} and the corresponding transferability problems in \cref{sec:lit_simulation}. Critical to robotic learning of manipulation skills is some metric of task success, generally labelled as reward function. The role and methods to obtain \emph{reward functions} for robotic learning, and deformable objects manipulation in particular, is reviewed in \cref{sec:lit_reward_learning}. Finally, we discuss the idea and corresponding literature of \emph{instrumenting the process with sensors} to facilitate the learning process in the manipulation environment in \cref{sec:lit_instrumentation}.

\input{\home/chapters/02-sota/a-manipulating-deform-objects}

\section{Learning-based approaches to robotic manipulation} \label{sec:lit_learning}

Machine learning, a domain of artificial intelligence, is the study of algorithms that give computers the ability to learn from and make predictions based on data. For robotics, learning provides a way to deal with the inherent systematic and random errors in robotic systems and variability in unstructured environments. This is because in learning, you optimize for the grasping task, which implicitly adapts the behaviour to imperfections in the system, such as inaccurate sensor readings.

In this section, we provide a short review of the fundamentals of relevant machine learning techniques. We aim to provide background material on \gls{DRL}, i.e.\ the method used in this thesis. To do this, we introduce supervised learning methods, deep neural networks and reinforcement learning sequentially in the following subsections. We discuss their relevant applications in robotic manipulation with a focus on the manipulation of deformable objects. %For a comprehensive view on the machine learning field, the reader is referred to~\autocite{Hastie2001} and~\autocite{Bishop2006}.

\subsection{Supervised learning} \label{subsec:lit_sl}

Supervised learning is a machine learning paradigm that operates under the setting where there is a set of \textit{input} variables, for example image pixels, that exert influence over other \textit{output} variables, for example whether there is a shirt or trouser in the image.
The components building up a machine learning system are the dataset, the model, loss function and optimization algorithm.
More formally, we can denote the input data as a set $\mathcal{X}$ consisting of vector $x^{(i)} \in \mathcal{X} $ with the superscript $i$ referring to the $i$th observation. In the machine learning domain, this set of predictor variables is called \textit{features}. The set $\mathcal{Y}$ contains the output variables $y^{(i)} \in \mathcal{Y}$. Concatenating tuples of
$\left\{\left(x^{(i)}, y^{(i)}\right) , i \in 1,\dots,N \right\}$
, often called \textit{examples}, leads to a dataset which can be used for learning. Central in this learning procedure is the idea of \textit{function approximation} in which a function $f$, parametrized by $\theta$, maps an input $x^{(i)}$ to its corresponding output $y^{(i)}$:
\begin{equation*}
	f(x;\theta): \mathcal{X} \mapsto \mathcal{Y}\text{.}
\end{equation*}

This mapping, also called \textit{model} or \textit{hypothesis}, comes in many forms such as linear models, tree-based methods, support vector machines and neural networks\footnote{We refer to~\textcite{Murphy2012, Bishop2006, Hastie2001} for a thorough exposition on traditional supervised learning methods.}.
The goal of the learning procedure then becomes to adjust the parameters $\theta$ of the model such that a certain performance measure $\mathcal{P}$ is optimized. This metric, called \textit{loss function} $\mathcal{L}$ in machine learning jargon, is specific to the task and domain in which the learning is taking place. In robotic folding for example, the robot might be presented with a candidate grasping pose $\mathbf{u}$. The robot than has to predict the probability $\hat{y} = Q_{\theta}(\mathbf{u}, \mathbf{x}) = \mathbb{E}\left[ \mathcal{S} | \mathbf{u}, \mathbf{x} \right]$ of successfully grasping (success denoted with $S$) a shirt, given some input image $\mathbf{x}$\footnote{This example also implies a heavy assumption; the availability of a dataset containing tuples of (grasping pose, object configuration, probability of success).}.
In this situation, one could minimize the negative cross-entropy loss:
\begin{equation*}
	\mathcal{L}=-y^{(i)} \cdot \log \hat{y}^{(i)} +\left(1-y^{(i)}\right) \cdot \log \left(1-\hat{y}^{(i)}\right),
\end{equation*} with $\hat{y}^{(i)} = f(x;\theta)$ being the predicted output of the model for observation $i$.
The optimization problem then becomes to adjust the parameters $\theta$ of the model $f$ using the examples $\left(x^{(i)}, y^{(i)}\right)$:
\begin{equation*}
	\theta^{*}=\underset{\theta}{\argmin} \: \mathbb{E}_{p(S, \mathbf{u}, \mathbf{x})}\left[\mathcal{L}\left(S, Q_{\theta}(\mathbf{u})\right)\right].
\end{equation*}
The dominant way for heavy parametrized functions such as neural networks to optimize this objective is to use gradient descent. The gradient expresses the direction of the steepest decrease of the loss function $\mathcal{L}$ with respect to the model parameters $\theta$. By iteratively updating the parameters in the opposite direction of the gradient, in the case of a minimization objective, we gradually arrive at a local or global minimum:
\begin{equation*}
	\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} \mathcal{L}(\theta).
\end{equation*}
$\alpha$ determines how large steps we take towards the estimated direction of the closest local minimum. Adaptive methods such as Adam~\autocite{Kingma2014} allows taking variable step sizes per variable based on the historical directions of the gradient.

Supervised learning is an important paradigm for robotic learning because labelled data provides a clear learning signal. This is important because time spent on robots is expensive. Contrarily, the learning signal in reinforcement learning (\cref{subsec:lit_rl}) often does not optimize for direct task performance and might lead to expensive learning times on physical robot platforms. The following paragraphs discuss relevant work in applying machine learning methods for solving robotic manipulation tasks. Generally speaking, there are two main strategies for applying supervised learning in the robotic manipulation pipeline: (1) as a perception module or (2) to map states to actions using imitation learning. We postpone the discussion of the application of deep neural networks in robotics after introducing neural networks in \cref{subsec:lit_dnn}.

\paragraph{Traditional ML methods - Perception}
Traditionally, supervised learning methods are leveraged in the perception module of a robotic manipulation pipeline. For rigid body manipulation, the popularity of data-driven grasp synthesis approach took off with the work of~\textcite{Saxena2008}. There, a logistic regression classifier is trained on synthetic data using manually engineered features. This way, they demonstrate that a robot is able to unload a dishwasher. In the domain of deformable object manipulation, ~\textcite{Ramisa2012} searches for quality grasping points of crumbled cloth in order to maximize the unfolding upon lifting. They do this by first labelling a dataset of shirts with bounding boxes containing the appropriate grasping points. Next, they train a logistic regression model to obtain the probability of the desired grasping point in a given bounding box using a bag of features from the input image. By employing the logistic classifier in a sliding window over the image, they pass image patches containing local peaks to an \gls{SVM} to obtain more accurate grasping candidates. The candidate patch is converted to 3D space by working in a calibrated environment, and motion planning is executed using inverse kinematics. Similarly,~\textcite{Wang2011} folds socks by having a perception system that uses manually engineered features for training an \gls{SVM} with Gaussian kernel to determine the type of sock in front of the robot. The deformable nature of cloth leads to self-occlusions making the garment type and pose classification ambiguous. The presence of this hidden state is explicitly modelled using a \gls{HMM} in~\autocite{Cusumano2011}. \gls{SVM} have also been used for the purpose to identify garment category and pose~\autocite{Li2014, li2014volum}. Learning methods are also being used to find regions of interest on the cloth. For example,~\textcite{Doumanoglou2016} use random forests to learn garment-specific grasping points. In~\autocite{Maitin2010}, RANSAC~\autocite{RANSAC} is used to find the corners of the cloth. These corners are good candidate grasping points for unfolding and identifying the type of cloth. Nearest neighbours have been used to identify wrinkled regions in a washcloth in order to flatten it~\autocite{Willimon2011}.

\paragraph{Traditional ML methods - Imitation learning}
Another strategy to train controllers using supervised learning is behavioural cloning. Behavioural cloning is a type of imitation learning~\autocite{Argall2009} in which task demonstrations are used for learning task execution. In behavioural cloning, a sequence of states and actions, as executed by a demonstrator, is recorded as dataset for a supervised learning algorithm. The goal then becomes for the model to predict the action a demonstrator would choose, given a certain state. \emph{Voegen we hier nog een degelijkse uitleg toe wat de verschillende LfD aanpakken zijn? Lijkt me relevant om te weten dat methodes zoals kinesthetic-teach-in niet echt nuttig zijn voor cloth domein.} An in-depth view of the field of learning from demonstration is given in~\autocite{Argall2009}. In the case of robotic laundry,~\textcite{Jia2019} learns single robotic laundry tasks from imitation: flattening, folding and twisting. In contrast to the large bulk of robotic controllers trained in \citeyear{Jia2019} using neural networks, they represent the controller using random forests~\autocite{Breiman2001}. The rationale is given by the non-parametric nature of random forests to dynamically change the number of leaf nodes based on the given imitation data and new cloth configurations. Example demonstrations are also used in deformable object manipulation to tie knots. ~\textcite{Schulman2016learning} uses example demonstrations for non-rigid warping~\autocite{Chui2003} based on point-cloud registration of the scene to tie knots with a robotic manipulator. The general idea is to warp the demonstrated trajectory to match the current setting, which may vary in initial conditions and knot geometry. Closely related is the work in~\autocite{Morita2003} where examples and solutions strategies from knot theory is embedded to do motor control. Although imitation learning is a viable alternative for learning to manipulate objects, a general problem plaguing imitation learning methods is generalizing to unseen scenarios. In the case of trajectory execution, errors can accumulate drastically leading to task failures. This is why existing methods apply data augmentation, include teacher advice or use reinforcement learning(~\cref{subsec:lit_rl}).

\subsection{Unsupervised learning}

I am unsure whether to really discuss this.  It fits the general flow but it might also be overkill. On the other hand, it is used extensively in our TCN paper given the UMAP projections. It also helps understanding why the TCN self-supervised method is not unsupervised.

Toch relevant werk erbij halen want integraal deel van TCN methodologie.

Structure and content:
\begin{easylist}[itemize]
	& Definition
	& Relevance~:
	&& dimensionality reduction: SIFT, HOW, PCA, T-sne, UMAP
	&& clustering

	& Application in learning (very short): input features for learning algo, visualization of embeddings, separation of cloth, ...
\end{easylist}

% Zie ook literatuur van Jia2019 Cloth Manipulation Using Random-Forest-Based Imitation Learning


\subsection{Artificial Neural networks} \label{subsec:lit_dnn}
\todo{Zitten hier genoeg robot voorbeelden in?}
% Inspiraties:
% 	DL boek goodfellow
% 	aggarwal DL boek
% 	Boek lio
% 	Paper 2017 DL applications robotics 

% INTRO what is een artificieel neuron tov biologisch neuron
\Glspl{ANN} are the workhorse of modern \gls{AI}. \Glspl{ANN} are loosely inspired by the neural network in a biological brain and the mechanisms of learning in biological organisms. The human brain is build-up by interconnected processing units called neurons. The connection strength between these neurons changes in response to external stimuli. This way, neurons receive, process and send information through the body and brain of biological organisms. Although comparing artificial neural networks to their biological counterpart is criticized as a far-stretch from the inner workings of the human brain, insights and knowledge of the neuroscience field have been useful in designing neural network architectures. The most common computational model of neurons, visualized in~\cref{fig:neuron}, simulates biological neurons as a node consisting of inputs, weights, bias, activation function and an output value. An \gls{ANN} computes an output by propagating the computed values from the input neurons to the output neurons. The artificial neurons are connected through weights that scale the given input to the neuron. A single neuron performs a weighted sum of the inputs in order to arrive at the neuron's \textit{activation}. Next, it transforms the activation value through an activation function before passing it to the successor neurons. The neuron's activation function is the source of nonlinearity in the network and enables the handling of non-linear relationships between inputs and outputs.

% Wiskunde neuron
More formally, the output $y$ of an artificial neuron is computed by
\begin{equation}\label{eq:neuron}
	y = f\left(b + \sum_{n = 1}^{D} w_i x_i \right)\,,
\end{equation}
where $f$ is the activation function, $b$ is the bias, $x_i$ is the $i$th input of the neuron which is weighted by weight $w_i$ connecting the $i$th input. An example of an activation function $f$ is the \gls{RELU}~\autocite{glorot2011deep} activation $f(x) = \max(0, x)$. Other common activation functions are sigmoid, hyperbolic tangents or variations of \glspl{RELU}.

% Neuron in networks: ANN 
In order to perform computations with neurons, we organize them in sequential layers. This gives rise to the name of artificial neural \textit{networks} as they chain together many different functions in a directed acyclic graph. In fully-connected feedforward networks the input and output layer are separated by so-called \textit{hidden layers}. The length of this chain is called the \emph{depth} of the network. This architecture is visualized in \cref{fig:two-layer-nn}. The name of the layer often denotes the operation performed by the layer. For example, a softmax layer normalizes an input vector of real numbers to probability distribution proportional to the exponentials in the input numbers. Feed-forward architecture propagates the inputs sequentially from layer to layer with neurons performing calculations as given in \cref{eq:neuron} but in a vectorized way:
\begin{equation} \label{eq:neuron_output_vectorized}
	\mathbf{x}^{(k)}=f\left(\mathbf{b}^{(k)} + \mathbf{W}^{(k)} \mathbf{x}^{(k-1)}\right).
\end{equation}
In this \cref{eq:neuron_output_vectorized}, the vector $\mathbf{x}^{(k)}$ contains the outputs of all neurons in layer $k$ which is based on the input vector $\mathbf{x}^{(k-1)}$ from previous layer, multiplied with weight matrix $\mathbf{W}^{(k)}$. The computation occurring in the hidden layers solve a major problem in real-world problems where many applications require disentangling sources of variation by using high-level abstract features. Learned representation offer a solution for such problems because it often results in much better performance compared to hand-designed features. Stacking multiple hidden layers in a neural network allows learning representations based on raw data which solves this central problem of finding features at appropriate levels of abstraction. This paradigm of stacking layers of computational units is called deep learning. It takes a compositional learning approach: upstream representations are expressed in terms of other, simpler downstream representations. Early layers learn primitives which are combined later to form more complex features. In images for example, downstream neurons learn edges and corners which are used in upstream layers to learn to recognize for example the sleeve of a shirt.

% How do we train NNs?
Crucial in neural networks is that the representations are learned instead of crafted by hand. Learning in \glspl{NN} occurs by changing the strength of neurons' connections. The weight adjustment is a response to the network's error and has as goal to modify the computation to make the output maximize the given objective. Training of these network weights is done using gradient-based optimization as discussed in \cref{subsec:lit_sl}. Although alternative training methods exist such as evolutionary methods~\autocite{salimans2017evolution}, gradients provide the direction in which to change the weights in order to maximize the objective function. This is especially beneficial for highly parametrized functions such as neural networks. Differentiation of multi-layer neural networks, called backpropagation, was already figured out in~\autocite{rumelhart1986learning} but was rediscovered by running the costly matrix-vector multiplication step in parallel on GPU~\autocite{gpu-nn}. Although improved computational hardware and data availability was a crucial enabler of the success of deep learning, many "tweaks" have proven as important to stabilize the backpropagation algorithm. Gradients assume an infinitesimal small step in each direction whereas the actual step we make has a finite length in order to make any real progress in optimization. The problem is that the gradients do change during the course of this step. In the case of multivariable optimization problems of considerable size, which is the case in deep neural networks containing millions of parameters, the optimization landscape is highly non-convex. This treacherous optimization landscape can change the gradients drastically leading to unstable training. This is why gradient-descent strategies such as momentum-based learning, using parameter-specific learning rates and weight initializing schemes are standard tricks in the deep learning practitioners toolkit. Gradients have also been known to disappear and diverge in deep neural networks because of repeated matrix multiplications when propagating the information forward and backward through the network. This is why \gls{RELU} activation functions are popular given that this piecewise linear activation has a derivative of value $1$ in certain intervals and zero elsewhere. Another problem caused by the highly parametrized nature of deep neural networks is overfitting the data. In machine learning jargon, this means that a model can predict the training set well but performs poorly on hold-out samples. A popular way to deal with overfitting is to regularize the network weights. Regularization effectively reduces the network computational power by imposing a penalty on weights in the loss function:
\begin{equation}\label{eq:regularization}
	L(\boldsymbol{\theta})=\frac{1}{N} \sum_{n=1}^{N} E\left(f\left(\boldsymbol{x}^{(n)}, \boldsymbol{\theta}\right), y^{(n)}\right)+\Omega(\boldsymbol{\theta}).
\end{equation}
\todo{probleem met notatie (n) superscript hierboven is dat het zowel kan refereren naar voorbeeld als layer}
In \cref{eq:regularization}, the function $E(.)$ is a chosen optimization metric that takes the output of the network $f\left(\boldsymbol{x}^{(n)}, \boldsymbol{\theta}\right)$ and the real label $y^{(n)}$ and is domain-dependent. The other term $\Omega(\boldsymbol{\theta})$ is the regularization term which balances $\ell 1$ and $\ell 2$ norm of the weights:
\begin{equation} \label{eq:regularization-term}
	\Omega(\boldsymbol{\theta})=\gamma \underbrace{\sum_{k} \sum_{i} \sum_{j}\left|W_{i j}^{(k)}\right|}_{\ell 1 \text { regularization }}+\lambda \underbrace{\sum_{k} \sum_{i} \sum_{j}\left(W_{i j}^{(k)}\right)^{2}}_{\ell 2 \text {-regularization }}.
\end{equation}
The hyperparameters $\gamma$ and $\lambda$ in the regularization term of \cref{eq:regularization-term} trade-off the amount of $\ell 1$ and $\ell 2$ regularization. $\ell 1$ regularization achieves sparse weights while the $\ell 2$ norm leads to networks with smaller weights. Other popular methods to improve generalization properties of deep neural networks is dropout~\autocite{dropout}, which zeroes out different random neurons at training time, ensembling and using data augmentation.

% CNNs
The feedforward models described above connect the neurons between layers in a fully-connected manner: every neuron from a layer is connected to every neuron from the preceding and succeeding layer with an unique weight. This dense connectivity lead to an explosion in the amount of trainable parameters. However, when the input data contains topological structure, like the ordering of image pixels in a grid, constraining the connectivity pattern between layers is a useful method to reduce the amount of parameters and exploit correlation. The most common way to implement this is by replacing the matrix-vector product $\mathbf{W}^{(k)} \mathbf{x}^{(k-1)}$ of \cref{eq:neuron_output_vectorized} with a sum of convolutions. This operation is equivalent to sliding a low-dimensional filter or kernel over the input image while performing a dot product. This property leads to sparse connectivity and parameter sharing. Consequently, by connecting a local spatial region with a shared set of parameters to the full spatial resolution of the image, one imitates the cortical neurons in the visual cortex, which respond only to stimuli within a receptive field~\autocite{hubel1959receptive}. It has been argued that these properties explain the success of using trainable convolutions for computer vision~\autocite{Goodfellow2016}. Usually, the convolution operation is followed by a downsampling operation that provides a summary statistic of the nearby outputs. This is most frequently implemented with an aggregation function, for example max pooling that takes the maximum value of a rectangular neighborhood. The process of embedding convolution operations, optionally followed by pooling operations, is known as \gls{CNN}. After demonstrating the effectiveness of \glspl{CNN} for large-scale image classification~\autocite{Krizhevsky2012}, \glspl{CNN} have been omnipresent in computer vision and natural language processing. In robotics, convolutional layers can provide a perception module while the motor control module is implemented as fully-connected layers acting on the output of the filter banks. However, in the case of motor control, the pooling operation is ofted removed from the architecture as translational invariance is not a desired property. Invariance to the position of a detected object would not enable a robot to detect where the object is located in the image. For more details about the history and working of deep learning methods and architectures, we refer the reader to the textbook of~\textcite{Goodfellow2016}.

\paragraph{Early NN in deformable object domain}
The earliest work using supervised learning with neural networks for deformable object manipulation is~\textcite{Howard2000}. In their work, they train a small feedforward neural network that learns the required minimum grasping force for lifting a deformable object. They collect the data by iteratively using more lifting force on objects with certain masses, deformability and damping. A similar approach is proposed in~\autocite{Khalil2007} to fuse tactile and vision data in order to learn physical parameters of the deformable object model. In~\autocite{Foresti2004}, fur tails are grasped from a conveyor belt. To segment the different furs present in the image, they train self-organizing maps~\autocite{Kohonen1982} in which neurons compete to be activated by the input signal. This results in disconnected regions of interest that are joined using skeletonization. Finally, a heuristic is used to determine and grasp the largest fur.

\paragraph{Modern NN in rigid body domain which kicked of the other works}
With the breakthrough in deep learning in~\citeyear{Krizhevsky2012} by~\textcite{Krizhevsky2012}, deep neural networks have found their way into robotic manipulation, starting in the rigid body manipulation domain. A successful approach to training deep neural networks in a supervised setting for robotic manipulation is to use \glspl{CNN} as grasp success predictor.~\textcite{Levine2016} train a \gls{CNN} on a large dataset of $800.000$ grasping attempts to learn to predict the grasp success probability of a grasping pose, given an input image. To sample candidate grasping points, they employ CEM~\autocite{CEM}. Dex-Net~\autocite{dexnet2} also trains a \gls{CNN} to predict the quality of a grasping candidate. This network is trained using a simulated dataset where objects are put into randomized poses on a plane. They use simulation to evaluate different grasping wrenches using analytic grasp metrics. Their model shows impressive generalizability to the real world, on different models not seen during training. The Dex-Net framework has been extended to work with suction grippers~\autocite{dexnet3}, use dual-armed robots~\autocite{dexnet4} and generate grasping candidates in the network~\autocite{Satish2019}.

\paragraph{Modern NN in deformable object domain}

\subparagraph{NN - Learning dynamics model}
One approach to leveraging the expressiveness of deep neural networks in a supervised setting is to train a dynamics model. This  model, often called world model, is obtained by training on $<\text{state}, \text{action}, \text{next state}>$ tuples. The  problem to solve becomes to use the world model in order to find the optimal sequence of actions that brings to given input state to the desired output state. Or in other words, the robot knows \textit{how} to act while the user can tell the robot \textit{what} it should do. To manipulate a rope into a desired shape, for example an S-shape,~\autocite{Nair2017} let the robot make arbitrary manipulations on the rope while recording the state transitions. This data is then used for training the inverse dynamics model. Planning can then be done using the world model by giving transitionary keyframes that define subgoals to the robot. A similar world model is trained in~\autocite{Ebert2018} where the model learn to predict future pixels given the planned actions. The data collection is done in a self-supervised way in which the robot does motor babbling. The trained world model then enables model predictive control in which they show good performance for folding cloth and towels. Whereas the training data for the world model in~\autocite{Nair2017} is given by self-supervision, in~\autocite{Yang2016} the data is given by teleoperating a humanoid robot with a virtual reality headset. This training data is then passed into an autoencoder which produces a time series in latent space. A second stage neural network then learns cloth dynamics by sliding a window over the encoded time series. A further attempt to embed the world model into the control module can be done by sandwiching a fully-connected network between the encoder and decoder~\autocite{Tanaka2018}. Instead of embedding a controller module, other work embed physics priors in to the network architecture. By assuming the deformable objects are build up of small, connected particles, it is possible to represent their connectivity and interactions in a graph neural network. This allows to efficiently learn deformable object dynamics to solve for downstream control tasks such as poking deformable objects~\autocite{Mrowca2018} and merging liquids~\autocite{Li2018}.

\subparagraph{NN - Imitation learning.}
Another way neural networks have been employed for learning robotic manipulation tasks is by giving task demonstrations~\autocite{Ravichandar2020}. Given the cost associated with real robot rollouts, it is beneficial to lower the dataset requirements by having access to the motor control outputs or to decrease the input dimensionality by for example avoiding learning from pixels. This explains the popularity for teleoperated~\autocite{Zhang2018,Duan2017} and kinesthetic teaching~\autocite{finn2017one} approaches. However, it is difficult to teleoperate a robot or physically manipulate a robotic arm to fold clothing items. This is because the speed and forces associated while executing the task is relevant for achieving proper folds. One method to compensate for this difficulty is to solve the imitation learning task in simulation and trying to transfer it to the real world. This is explored in~\autocite{Seita2020} that generates example demonstrations in simulation and uses behavioral cloning to train a motor policy network to flatten a towel. To fine-tune this policy outside the seen dataset distribution, they employ dataset aggregation (DAgger) in which an oracle policy is used to label the unseen states during training. An alternative is given in ~\autocite{Sundaresan2020} that uses simulated data to learn visual object descriptors indicating the segments of a rope. Such embeddings implicitly encodes geometric structure which can then be used for knot-tying using example demonstrations.

% Overgang: rational voor RL
\paragraph{SL - conclusion.}
Framing robotic manipulation of rigid and deformable objects as a supervised learning problem has been successful for estimating the state of deformable objects or manipulating cloth by means of behavioral cloning. However, training with hand-labelled or generated data has two main issues~\autocite{pinto2016supersizing}. The first issue is the human bias towards preferring grasps poses that are similar to the way a person would grasp an object. This discourages exploration of unconventional grasp configurations. The second issue is the cost to exhaustively evaluate all possible grasps because an object can be grasped in multiple ways. Therefore, learning on robots requires a method that can work without human supervision.

\subsection{Reinforcement learning} \label{subsec:lit_rl}

% OP REIS TODO: 
% 		RL BESCHRIJVING: dat kan later adhv
% 			 [2017]Deep Reinforcement Learning for Robotic Manipulation-The state of the art
% 			 [2015]A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation
% 			 [2021]Reinforcement learning in robotic applications:a comprehensive survey
% 			 sutton-barto
% 			 lecture notes cs229
% 			 DL book IBM kerel 1.7.1 p64

% 	https://arxiv.org/pdf/2006.15009.pdf sectie 2.1
% 	https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a
% 	Thesis Jannick , Thomas , Matas? 
Structure\footnote{this is not a content table, but more a hierarchical tree what and where I would discuss.} and content:
\begin{easylist}

	& A Concise introduction to RL
	&& MDP formalisatie
	&&& state action transition reward discount factor
	&&&& model-based vs model-free RL
	&& episodic MDP
	&& State vs observation + markov property
	&& Goal: maximize expected reward
	&& RL loop and elements: state representation, reward function.
	&& Definition: what is a policy
	&& Definition what is a value function, q function
	&& from bellman equation to q-learning
	& contrast to SL: RL is much harder because (1) delayed rewards (credit assign problem) (2) non-stat data, (3)
	%(also check & suttonbarto and thesis NG https://web.archive.org/web/20141222084445/http://www.cs.ubc.ca/~nando/550-2006/handouts/andrew-ng.pdf)
	& why relevant for robotics - RL in the context of optimal control % zie ook RL in robotics: a survey sectie 1.2 p5 EN review of robot learning for manipulation p24 sectie 6.3
	&& model-free: tradeoffs % zie review of robot learning for manipulation met Peter p25
	&& model-based: tradeoffs
	&& Figuur toevoegen. Zie ~\cref{fig:DRL-robotics}
	& RL Flavours
	&& value based methods: q-learning
	&& policy based methods: vermelden maar niet al te diep op ingaan
	&& actor critic methods: vermelden maar niet al te diep op ingaan
	& Deep RL
	&& DQN
	&&& trick1: target network
	&&& trick2: experience replay
	&& policy-based and actor critics: DDPG en SAC vermelden
	& Literatuur: Applications in robotic manipulation and cloth

\end{easylist}


% TODO: misschien hoort de intro hieronder eerder thuis in H1: introduction.
%  Dan kunnen we als alternatief hier direct aanvangen met formele definitie van RL zonder het zo mooi in te leiden. 

In supervised learning, a clear learning signal allows to train models such that their output match the labels given in the training set. This requires to obtain a dataset that labels every observation with the correct response. For example, in the case of a robot folding a shirt, it would require to provide supervision on how to move every joint for all arm and shirt configurations in the training set. This sequential decision making process might alternatively better be solved by providing an indication on how good the model is behaving. For example, we might give a robot a positive reward when it has folded a shirt without wrinkles and penalize it when it throws the shirt of the table. This approach of giving agents rewards in an environment is formalized as \gls{RL} and is an eminent approach for learning control policies with minimum user intervention. RL has been succesfully applied in domains ranging from game playing~\autocite{mnih2015human}, helicopter flight~\autocite{ng2003autonomous}, autonomous driving~\autocite{sallab2017deep}, locomotion~\autocite{tan2018sim} and robotic manipulation~\autocite{levine2016end}.

\glsreset{MDP}
Sequential decision making is formalized as a \gls{MDP}: given a sequence of states, what is the action that will maximize the expected discounted future reward. This statement implies that an agent can observe its environment, exert influence on its environment through actions and has a notion of what constitutes good behavior. More formally, an \gls{MDP} is a tuple $\left(S, A,P, \gamma, R\right)$ where:
\begin{enumerate}
	\item $S$ is a set of states representing the environment. For example, the set of joint values of a robot arm and the position of the sleeve of a shirt.
	\item $A$ is a set of actions. These actions are performed by the agent and influence the environment. For example, moving the end-effector of the robotic arm up or downwards.
	\item $P$ are the state transition probabilities. This is a distribution over states and actions indicating the probability to arrive at a new state: $P\left(s_{t+1} \mid s_{t}, a_{t}\right)$.
	\item $\gamma \in [0, 1)$ is the discount factor and used to discount future rewards to the present.
	\item $R = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]$ is the reward function, based on the action taken and the resulting state the environment transitions to.
\end{enumerate}
Decision making in an MDP goes as follows: the environment is initialized in a certain state $s_t$. The agent chooses an action $a_t$ to execute in the MDP. This way, the state of the MDP transitions to a successor state $s_{t+1}$ governed by $P$. Based on this new state, the agent receives a reward $r_t$ and chooses a new action $a_{t+1}$. This process is visualized in \cref{fig:lit_rl_loop} and repeats until the environment signals a terminal state, for example, a robot manipulator succesfully folds a shirt. \Glspl{MDP} with terminal states are called \emph{episodic} \glspl{MDP}. Furthermore, a distinction is made between \emph{states} and \emph{observations}. A state is assumed to contain all relevant information to make an optimal decision. In technical jargon, this is called the \emph{Markov property}: the future depends only on the current state and action, but not on the past. However, it is hard to capture all task-relevant features with sensors. For example, a single camera image of a crumbled shirt does not satisfy the Markov property because the occlusions does not allow to make informed decisions about the occluded parts. In such cases, the states are called \emph{observations} $O$ and the formalism a \gls{POMDP}. 

\begin{figure}[htb]
	\includegraphics[width=\linewidth]{\home/chapters/02-sota/figures/DRL-in-robotics.png}
	\caption{Eigen figuur maken dat archetype generiek RL aanpak in robotic learning toont}
	\label{fig:lit_rl_loop}
\end{figure}

The eventual factor driving the decision of the agent is the reward function. The goal of an RL agent is to maximize the sum of expected rewards $G_t$:
\begin{equation}
	% TODO: align equation env
	G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T}
	=\sum_{k=t}^{T} \gamma^{k-t} R_{k+1}
\end{equation}
. 
% TODO: YOU ARE HERE. Verder uitwerken adhv notities Silver en notities Levine. Hierna uitbouwen naar concept van value en q functions. Zo opbouwen naar Bellman equation en eventueel de break-down doen zoals je voor AI vak had gedaan. 
% andere goede bronnen zijn:
%  	[2015]A survey of Deep Network Solutions for Learning Control In robotics
% 	cs229-notes-RL
% 	[2018]DRL overview survey p13


% Voor vhet value-based stuk naar DQN 
An early work showing the use of neural networks as function approximator in value-based \gls{RL} has been neural fitted Q-iteration \autocite{riedmiller2005neural}. These ideas have been transferred to image-based deep RL by \textcite{mnih2015human} in the domain of video games. 

 For a complete review of RL, we refer the reader to the textbook of~\textcite{Sutton2018}.

% TODO: Kijk nog is of je structuur niet zou wijzigen en SL - NN stukken zou mergen

\paragraph{Literature.}
Much of the work for learning manipulation skills for deformable objects has been inspired by work done in the rigid objects domain. These methods in turn often rely on work done on learning to play games and solve toy tasks in simulation environments.  First attempts at transferring vision-based DQN to visual servoing were attempted in~\autocite{Zhang2015} but failed. Their end-to-end approach to frame the inherent continuous action spaces of robotic manipulation to a discrete algorithm such as DQN is to discretize the joint motor outputs into $9$ motor actions bins. To further reduce the search space, they reduce the amount of joints of the robot from $7$ DOF to $4$ DOF. They train a DQN controller in simulation while adding noise and task variations. However, transfer to the real robot failed due to the low simulation fidelity and inappropriate reward functions. Later work~\autocite{James2016} managed to transfer virtually trained DQN agents on pixels by using high-fidelity simulation and careful reward tuning. With the introduction of continuous action spaces for value-based agents, more specifically DDPG~\autocite{Lillicrap2015}, DQN variants started being successfully applied in the robotic manipulation domain~\autocite{Gu2017}. This success was mainly driven by directly representing the continuous action space of robot actuators, reusing past experiences for training and working in parallel.

Deep RL for deformable object manipulation has arguably first been used in~\autocite{Matas2018}. By training a DDPG agent in simulation, they demonstrate limited transferability to the real world using domain randomization to fold a towel. They find that a lack of simulation tools for cloth is the main factor limiting their results. The same approach is speed up in~\autocite{Jangir2020} by warm-starting the learning with example demonstrations. An additional speed-up can be obtained by filling the replay buffer with example demonstrations~\autocite{Tsurumine2019}. ~\textcite{Wu2020} examines how to link two policies, i.e.\ picking and placing cloth which are normally independently trained. They opt for structurally encoding the relationship between picking and placing of cloth. This is done by having the second learned policy, i.e.\ the place policy, receive the last output of the picking policy. This way, the network can optimize the picking point that gives the most value during placing. Batch RL methods, a variant of RL in which the agent is trained once on a dataset without interacting with the environment, is explored in ~\autocite{lee2020learning}. Instead of generating an interaction dataset with random motor actions, they use a motor control heuristic. Such heuristics allow collecting data that contains more meaningful interactions compared to motor babbling. The train a fully convolutional DQN agent offline, resulting in a Q-value heatmap per possible action. This approach is successful in achieving simple folds in rectangular cloth, with one hour of real world data. Another DQN architecture is explored in~\autocite{seita2021learning}. They employ Transporter networks, an architecture that predicts the spatial displacement of a local area, to learn to fill deformable bags with objects in simulation.

\subsection{Self-supervised learning}
A large class of successful \gls{ML} methods rely on some form supervision. In robotic folding, this supervision can be labeling the type of clothing item in front of the robot, specifying which action a human would take, or which torques to exert on the motors. Collecting this data is expensive, sometimes difficult and prone to bias and errors~\autocite{mehrabi2021survey}. Unsupervised learning is on the other end of the \textit{supervision spectrum} but it is hard to know which signal it will pick up for learning. Providing supervision while building up a dataset, without manually labeling every sample is known as \emph{self-supervised learning}. Therefore, by having a data generation process that creates pseudolabels, one can use well-established supervised learning algorithms. The labeling process allows us to inject prior knowledge about the task by inventing mock tasks, also known as \emph{pretext tasks}. The goal of self-supervised learning is not to solve this invented task, but rather to learn meaningful representations that can be used for downstream tasks. In cloth folding for example, we can present the network with a shirt, rotate it and ask the model to predict how many degrees the shirt was turned. Although this example task is only deployable in a narrow set of tasks, we do not really care about the accuracy. Rather, we want the network to learn a meaningful latent space.

There are two broad categories of self-supervised learning. The first category, generative methods, aims to reconstruct the given input signal while learning a latent space. These methods are popularized by generative adversarial networks~\autocite{goodfellow2014generative}. The second category, discriminative methods, reframes the self-supervised problem as a classification task. In particular, \emph{contrastive} self-supervised learning looks at \textit{contrasting} positive and negative examples. More formally, let $h(x) \in \mathbb{R}^d$ be the encoding of an input sample $x$, called the anchor. We define $x_p$ as a positive sample and $x_n$ as a negative sample. The goal then becomes to encode the anchor and positive close in embedding space compared to the anchor and the negative:
\begin{equation*}
	\text{dist}\left( h(x),h(x_p) \right) \leq \text{dist}\left( h(x),h(x_n) \right) .
\end{equation*}
The distance function $\textit{dist}$ measures how similar samples are. For example, in the case of robotic folding, we would want the embedding to push frames of folded shirts closer together than completely crumbled shirts. The hypothesis is that by forcing similar items to be close in embedding space, the network has to learn relevant features and this way builds a semantically meaningful embedding. This is done by enforcing invariances in the embedding. For example, if the network is presented with an anchor image, a rotated anchor image and a random different image, the network learns to become rotational invariant. Other popular pretext tasks in literature to learn different type of invariances are learning to colorize images~\cite{Zhang2016Color}, reconstructing the original input~\cite{Pathak2016} and predicting the relative position of two random patches~\cite{Doersch2015}.

\key{Self-supervised learning aims to learn semantically meaningful embeddings by solving mock tasks that force the model to attend to task-relevant features while learning useful invariances.}

Using contrastive objectives to learn good representations have been popularized in the NLP domain. In~\autocite{mikolov2013distributed}, contrastive training was done by using co-occuring words as semantically similar for learning word embeddings. Extensions to images, video and speech data was popularized with contrastive predictive coding~\autocite{oord2018representation}. This method maximizes the mutual information between the predicted encodings and their corresponding positive samples. Similarly, SimCLR~\autocite{chen2020simple} maximizes agreement between an image and its applied data augmentations by using a cosine similarity loss.

In this work, we focus on extracting self-supervised signals from video task demonstrations. The inherent structure in videos is the temporal dimension, which can be used as a supervisory signal to provide contrasting examples. The goal then becomes to recover the temporal coherence of a video. One of the firsts works~\cite{Misra2016} leveraging time as contrastive signal inputs a sequence of frames and classifies whether the frames are in the correct order. Later work~\cite{Lee2017,Fernando2017} also frames self-supervised learning as a classification task in which the correct temporal order has to be determined. Subsequent work has looked at then using self-supervised embeddings as a reward or progression signal for learning agents.
% \paragraph{Gebruik van de geleerde SSL embedding voor agent learning task solving purposes}
% A number of prior works construct reward functions, or equivalently process monitoring metrics, in latent spaces trained with time as a supervisory signal.
In~\cite{Singh2019}, they construct a reward function based on an image classifier trained on successful goal states reached by teleoperating the robot towards the end state. In~\cite{Hartikainen2019}, time is used as a learned distance function for assigning environment rewards. However, their approach requires human intervention in order to select the desired goal states. \cite{Nair2018time} also uses time as a supervisory signal in videos of expert demonstrations to learn an optimal trajectory of states. However, they assume the possibility of visually removing the end-effector from the scene which is not possible for all tasks. For example, it is not possible to drop a shirt in midair while being folded. Other work~\cite{Nair2018visual} looks at expressing the reward function as the distance in latent space between the current state and the goal state. However, this is not possible when there is a trajectory in latent space that has to be followed in order to execute the task. This problem is enlarged when the start state is very similar to the end state. In this scenario, the agent would not be incentivized to leave the start state because it already receives high rewards due to being close to the end state. % bv. bij het cube stacking blok gaat bij de eerste stap (grijp een kubus) de embedding waarschijnlijk altijd op dezelfde plaats liggen als op het einde vd taak dus je moet hier tussen kunnen differnetieren.  
Alternatively, it is possible to add a contrastive loss as an auxiliary objective in RL as done in CURL~\cite{Srinivas2020CURL}. The authors show that their method outperforms other learning methods on the DM Control Suite.
An important self-supervised architecture we use in this work are \glspl{TCN}~\autocite{Sermanet2017TCN}. The central idea in this approach is to leverage crossmodal inputs, such as different camera viewpoints, in order to find differences in frames that cannot be attributed to a changing viewpoint. We discuss this in detail in \cref{ch:reward_functions}.

In the domain of deformable object manipulation, self-supervised methods have been used primarily to do the state estimation of objects. A latent space describing the state of a rope is learned using self-supervised learning on simulated images in~\autocite{yan2020learning}. This model is then used as forward dynamics model in which an action trajectory is sampled that minimizes the distance towards the given goal state. The actions are chosen such that only points on the rope are considered. A similar approach for bringing fabric into a desired state is explored in~\autocite{fabric_vsf_2020}. They learn an image prediction model in simulation, which can be used to solve arbitrary goals at test time via model predictive control. They apply domain randomization to transfer fabric smoothing policies to a real-world surgical robot. However, their approach fails to transfer folding tasks successfully due to the sim-to-real gap. An alternative sim2real solution is explored in~\autocite{Mengyuan2020} that also train an autoencoder to predict the dynamics of deformable linear objects. The encoder uses a transformer architecture~\autocite{vaswani2017attention} that iteratively refines the estimation of the image space coordinates of points on the rope. The network is then fine-tuned on real images using a self-supervised objective. This self-supervised goal encodes a color contrast cue: the target deformable linear object has a different color from the background. This prior can be modeled with a Gaussian mixture model that segments the input image. By rendering the output rope state from the network to image space, a differentiable loss can be defined on the segmented input image to refine the network. This method leverages simulated data for pretraining of the network, approximately $5000$ real images for finetuning and is able to accurately manipulate a rope into a given target state.

\section{Datasets for robotic learning} \label{sec:lit_datasets}
The robotics community in general have accepted deep learning to be a powerful tool~\autocite{Sunderhauf2018}. This acceptance is evident from the surge in \textit{deep learning} keywords in high-tier robotics conferences such as ICRA, the hosting of robotic application workshops at computer vision and machine learning conferences such as CVPR~\autocite{angelova2017computer} and NeurIPS~\autocite{Posner2017} and the advent of a dedicated Conference on Robot Learning\footnote{\url{http://www.robot-learning.org}}. However, neural networks are known to be data-hungry caused by their high parametrization. Together with the high cost associated with collecting data on real robotic platforms, there is a need for datasets for robotic learning. This eminent need for high-quality datasets is also apparent in the general deep learning community with the launch of the NeurIPS 2021 Datasets and Benchmarks Track\footnote{\url{https://blog.neurips.cc/2021/04/07/announcing-the-neurips-2021-datasets-and-benchmarks-track/}}. The applications in robotic manipulation for these datasets lie in applying supervised learning methods to train grasp quality predictors, identify grasping points and object states. Alternatively, offline RL, also known as batch RL and discussed in \cref{subsec:lit_rl}, can be employed to learn policies from gathered state-actions tuples without letting the policy interact with the environment itself.

A successful approach to generating datasets for robotic manipulation is Dex-Net~\autocite{dexnet2} in which a simulated dataset of $6.7$ million synthetic robust grasps are generated. Dex-Net contains RGB-D images with candidate grasping pose and grasping outcome of $1.500$ object meshes. Generating synthetic grasping data is also explored by other authors~\autocite{depierre2018jacquard,redmon2015real} but has the pitfall of potential transferability issues to the real world. This sim2real problem urges other authors to look at generating data in-vivo. However, human supervision for controlling robots is expensive and biased. Hence, a popular approach is to self-supervise the data collection process: use a heuristic motor control rule to execute grasps. In~\autocite{pinto2016supersizing}, a dataset consisting of $50.000$ grasp attempts with a dual-armed robot is collected. Afterwards, a network is trained to predict a good grasping orientation from a given image patch.
In~\autocite{Levine2016}, this approach is scaled-up. They generate two datasets containing $800.000$ and $900.000$ grasping attempts on two different clusters of robotic manipulators. They record images from a monocular RGB camera mounted over the shoulder of the arm. They synchronize the images with the delta end-effector pose and grasp success outcome. Their dataset consists primarily of rigid objects although some slightly deformable objects such as rubber ducks are present. This dataset is then employed for training a grasp quality predictor neural network.
Robonet~\autocite{dasari2019robonet} is a followup initiative that merges rollouts of 7 different robots from 4 different institutions in order to obtain 15 million frames containing variability across viewpoints, objects, robots and lighting conditions.
Other researchers~\autocite{mandlekar2018roboturk} argue that self-supervised data collection is inefficient and prone to errors. This critique has led to RoboTurk~\autocite{mandlekar2018roboturk}, an online platform that allows remotely operating robots for manipulation tasks. They collect $111$ hours of RGB-D data matched with joint and effector sensor readings from $54$ different robot operators for different tasks. One of the tasks consists of unfolding garments on a table and have been shown to be useful for training self-supervised embeddings.

Compared to rigid objects, the deformable object domain is less endowed with datasets. The largest cloth datasets are mainly constructed for automating retail applications such as clothing category identification, fashion landmark detection, image retrieval and recommendations systems. The first annotated cloth dataset is Fashion-MNIST~\autocite{fashionmnist} which is a drop-in replacement for the MNIST dataset, often used for benchmarking ML algorithms. Fashion-MNIST contains $70.000$ grayscale images of size $28\times28$ of fashion products from 10 categories. A modern variant of Fashion-MNIST is the DeepFashion~\autocite{DeepFashion} dataset which contains $800.000$ images scraped from the Google image search engine and from shopping websites. The dataset contains $8$ different landmarks (i.e.\ collar, sleeve, waistline, and hem for each sides), $46$ clothing categories and $1.000$ clothing attributes such as fabric material and fashion shape. This dataset is extended in~\autocite{DeepFashion2} which provides more images, richer annotations and multiple clothing per frame to improve real-world realism. To reduce the noise introduced by scraping data, the FashionAI~\autocite{FashionAI} dataset focuses on improving the automated annotation process by providing high-quality labels and a dedicated network architecture that assign attributes in a tree-like way. The attributes represents fashion semantics that are disassembled into hierarchical concepts. For example, a round collar bishop top is disenangled into "$\text{top look} \rightarrow \text{sleeve region} \rightarrow \text{style} \rightarrow \text{cuff} \rightarrow \text{bishop}$" and  "$\text{top look} \rightarrow \text{collar} \rightarrow \text{round}$". The neural network training uses a DAgger-like approach from behavioral cloning in which the network requests expert labels for cases in which it is uncertain. They collect $357.000$ images of $6$ categories of women's clothing and $245$ high-quality hierarchical structured annotations.

In contrast to annotated cloth data for retail applications, datasets for robotic learning of manipulation skills for clothing and deformable objects are not widely available. While a large body of research exists on cloth modeling and garment reconstruction from images~\autocite{bertiche2020cloth3d,deepfashion3d,Wang20183dgarment}, the availability of dedicated cloth folding data is scarce. To the best of our knowledge, the sole example containing a small set of deformable objects manipulations with a robot is in~\autocite{mandlekar2018roboturk}. Other work concerning datasets for learning to manipulate cloth or other deformable object generate data in simulation which is discussed in the following \cref{sec:lit_simulation}.

\todo{Kan je nog meer tot conclusie komen hier? Namelijk, dit stuk geeft aanleiding ttot je dataset paper. Zie ook de  blog van neurips.  }

\section{Simulation environments to accelerate learning} \label{sec:lit_simulation}

\key{definitie van simulatie hier.}

Physics simulators are a crucial tool for robotics researchers. The community often first tests hypotheses and methodologies in simulation and optionally transfers results to the real world. The main reason for using virtual platforms is their low cost, reproducibility and availability compared to real robots. Simulations can run faster than real time, do not need an active operator and can provide the large dataset requirement for deep learning algorithms.

While a wide array of physics simulators can model and simulate a diverse set of phenomena, it is important for robotic researchers to have access to this physics engine within a robotic simulator. This requires the engine to support common robotic tools such as ROS integration for transfer to a real robotic platform, inverse kinematics, URDF import and models for joints, actuators and sensors. Another important but often forgotten feature is the ability for headless rendering. Running the simulation without having a physical display attached allows to use computation farms that often come without display. Common robotic simulators~\autocite{review paper robotic simulators} such as Gazebo~\autocite{gazebo}, MuJoCo~\autocite{mujoco} and PyBullet~\autocite{pybullet} support robotic object manipulation tasks, however the deformable object simulation functionality is limited.

The limited support for cloth simulation in existing robotics simulators has led to many researchers implementing custom cloth simulators. \textcite{Matas2018} for example extend the functionality of PyBullet to train a robot agent to fold cloth completely in simulation. DeformableRavens~\autocite{seita2021learning} and DEDO~\autocite{dedo} are other soft body simulation implemented in the PyBullet physics engine that allows simulating robotic interaction with ropes, fabrics and bags. Unfortunately, the visual and physical fidelity is significantly lower compared to other synthetically generated cloth data. SoftGym~\autocite{softgym} provides fluid, cloth and rope simulation simulations benchmark environments. It uses Nvidia FleX particle system simulation, which is hardware-accelerated on GPU. Because SoftGym runs on the GPU, it allows to do more calculations in parallel, leading to physically and visually more realistic cloth behavior. However, the authors anticipate fundamental challenges when transferring policies trained on SoftGym to the real world. Finally, other relevant work examine to make the simulation itself differentiable. Differentiable physics simulation is a powerful technique that applies gradient-based methods to simulating physical systems. This way, it enables gradient-based optimization for control\todo{zorg dat dit uitgelegd bij engineered approaches sectie. Het mag niet overkomen alsof dit iets nieuws is. Tradtionele controle heeft dit ook!}. Backpropagating gradients through a neural network controller and through the physical system has shown to speed up the learning process for robot control tasks~\autocite{Degrave2019}. Extending this work to soft body simulations has been explored in~\autocite{liang2019differentiable,huang2021plasticinelab} and have shown that gradient-based optimization method outperform RL but fails on multi-stage tasks that require long-term planning.

\subsection{Deformable object simulation methods} \label{subsec:lit_cloth_sim}

Forces applied to a deformable object both move the object and change the shape making high-fidelity modelling more difficult and computational expensive compared to rigid object simulation. Deformable bodies require reasoning about the shape, dynamics and material properties of the object. Techniques in modeling cloth-like behavior falls on a spectrum based on the computational budget allocated to the simulation. Offline simulation is on one side of the spectrum that prioritizes visual quality by means of physical realism. A prominent example can be found in the movie industry where \gls{GPU} farms render multiple days for a two-hour video clip. In contrast, real-time systems are interactive and require running at a fixed frame rate. An important application of real-time simulation systems is found in video games. Games typically run between \SI{30}{\hertz} and \SI{60}{\hertz} leaving \SI{15}{\milli\second} and \SI{30}{\milli\second} computational budget per frame. Subtracting the time needed for core game features such handling user input, game logic and \gls{AI} leaves only a few milliseconds remaining for physical simulation. Unfortunately, reducing the resolution of the simulated object often leads to unsatisfying results~\autocite{NVidia flex kerel Muller}. This is why real-time methods focus on reproduction of the visual properties of physical processes. These constraints have driven the game industry to develop methods outside the offline physical simulation domain.

Physical simulation of deformable objects for a robotic learning environment requires characteristics of both of real-time and offline simulation. For learning in simulation, we need physical realism for transfer to the real world and also needs to share resources for learning purposes and robot simulation. Offline cloth simulation are notoriously slow. For example ArcSim~\autocite{narain2012adaptive}, a realistic cloth simulator using \gls{FEM}, requires \SI{50}{\second} to render a single frame of a fully dressed human character. Using these expensive simulations in a robotic learning environment would consume the computational budget, leaving little time for learning. To understand the rationale involved in selecting an appropriate simulator, we discuss the two major steps for implementing a deformable object simulation.

A first step in modelling deformable objects is choosing a shape representation. Choosing a representation changes the flexibility of the model and has an impact on the modelling of the dynamics. There are three major approaches for representing the shape of a deformable object. \todo{ENUMERATE want je hebt hierarchie in hierarchie.} First, implicit curves and surfaces can be used as representation. These shapes are defined by an implicit equation $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ with $f(\mathbb{p}) = 0$ for which all points are on the surface. Implicit equation representations are primarily used in medical imaging in which level set methods are used for tracking deformable objects~\autocite{Cremers2006}. Parametric curves and surfaces, the second option for shape representation, are shapes controlled by a limited set of parameters. A 3D parametric surface is generated by a set of functions $\mathbb{p}: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}$ with all cartesian points evaluated directly from their functional expression $\mathbb{p}(u, v) =\\ \{x(u, v), y(u, v), z(u, v)\}^{T}$ where $u$ and $v$ are parameters. Splines and extension such as B-splines and NURBS are parametric surfaces that can represent any type of deformable object. They allow a compact representation defined by control points that can move and deform the surface. However, depending on the spline parametrization, moving control points can lead to erroneous deformations if for example the resulting curves do not lie on the convex hull of the control points. Learning in such non-linear dynamical environment is difficult and a general solution for this is to date not available~\autocite{rios2020}. A third possible shape representation is a mesh. A mesh stores the topology and geometry of an object represented by connected vertices. Meshes are the most used shape representation for a deformable object in simulation.

The second step in simulating deformable objects is choosing a dynamics model that will calculate how the chosen shape representation will deform on interaction with forces exerted on the object. In robotics, the important characteristics influencing the choice of dynamics model are computational cost, physical accuracy, visual fidelity and ease-of-use. In the following paragraphs, we consider two main approaches: \gls{FEM} and particle-based methods.

The \gls{FEM} is a widely used method for offline simulation of solid objects. \Gls{FEM} incorporates real physical material properties making simulations fairly realistic. This method generally works by reducing the general partial differential equations ,describing the physical reality, to systems of algebraic equations. Because these equations are often non-linear, solving the system in real-time becomes non-trivial. A way to speed-up the solver is linearizing the equations. This works for cases where deformations are small such as the analysis of buildings. However, the artifacts caused by linear approximations become significant in the case of cloth. This is why in real-time applications, particle-based methods are often desired above \gls{FEM}. Particle simulations consist of atomic masses called particles that undergo forces such as gravity or a robot end-effector grasp. These forces drive particles to new positions solved by using Newton's second law of motion and a time integration scheme to solve the resulting differential equations. In the case of cloth, the particles are connected with their second-order neighbors and exert forces on each other in order to preserve the shape. These forces are modeled as springs, leading a mass-spring system of particles. Compared to \gls{FEM}, particle systems are faster to simulate and easier to implement. However, they are harder to tune and can be plagued by instabilities. In particular, choosing a right spring constant for the springs is notoriously difficult. Nonetheless, a spring-mass system approach for cloth simulation is a favorable approach as it conveniently handles the two-dimensional structure of cloth. This finding is also apparent in the robotic learning literature: modern research using cloth simulations implements a particle-based approach~\autocite{Matas2018,seita2021learning,dedo,softgym}. An exception is~\autocite{liang2019differentiable} that utilizes \gls{FEM}. We formally describe and implement a complete particle-based cloth simulation in \cref{ch:simulation}.

\subsection{Transferring simulation results to the real world}  \label{sec:lit_sim2real}
% TODO: zoek een review paper en de twitter/blog/workshop rond sim2real veld en gebruik dat als houvast. 
Structure and content:
\begin{easylist}
	& sim2real problem uitleggen.
	& Verschillende aanpakken uitleggen:
	&& Meta learning
	&& simulatoin randomization
	&& system identifitcation
	&& domain adaptation
	&& progressive networks (Rusu et al)
	&& James et al: real to sim
	& literatuur bespreken.

\end{easylist}

% PAPERS:
% System idemtification
% In~\autocite{Howard2000}, a mass-spring-damper simulation of deformable object is tuned to the real world by probing the object with the gripper containing a force sensor.

% Domain randomization:
% Matas2018 for transfering cloth manipulation tasks and Yan2019 for bringing policies to learned in simulation to the real world among others.

\section{Reward learning}  \label{sec:lit_reward_learning}
% Zie ook p37 [2018]DRL overview survey
\begin{easylist}
	& General introduction
	& Motivatie: Reward hacking, impossible to capture all ingredients
	& Literatuur:
	&& 	IRL
	&& 	Reward learning
	&&  Reward = distance in embedding space, popularized by Nair
\end{easylist}

\section{State perception through instrumentation} \label{sec:lit_instrumentation}
Motivatie.
Definitie.
literatuur.
Multimodal learning.


\section{Kort conclusie.}

\end{document}