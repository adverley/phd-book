% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

\providecommand{\home}{../..}
\documentclass[\home/main.tex]{subfiles}

\begin{document}

\chapter{Background and review of related work} \label{ch:lit}

The following chapter provides the preliminaries and a review of relevant work in the field of robotic manipulation of deformable objects.
First, we introduce motor control architectures for rigid body manipulation and review how they differ from deformable object manipulation in \cref{sec:lit_traditional}.
To give historical context, we then discuss how \emph{standard robotic manipulation pipelines} can be used for manipulating deformable objects in \cref{sec:lit_cloth_folding_pipelines}.
Next, we introduce how the inherent limitations of engineered motor control architectures can be overcome by using \emph{learning-based methods}. We distinguish between learning with (\cref{sec:lit_sl}) and without (\cref{sec:lit_learning_without_labels}) examples, and from interaction with the environment (\cref{subsec:lit_rl}).
Critical to robotic learning of manipulation skills is some metric of task success, generally labelled as the reward function. The role and methods to obtain \emph{reward functions} for robotic learning, and deformable objects manipulation in particular, is reviewed in \cref{subsec:lit_reward_learning}.
Given the general property that learning-based methods are data-hungry, we continue this discussion by reviewing the role of \emph{large datasets} for robotic learning in \cref{sec:lit_datasets}. An alternative approach to generating data is to use synthetic data. To this end, we discuss the role of \emph{simulation} and the corresponding transferability problems in \cref{sec:lit_simulation}. Finally, we discuss the idea and related literature of \emph{instrumenting the process with sensors} to facilitate the learning process in the manipulation environment in \cref{sec:lit_instrumentation}.

\input{a-manipulating-deform-objects.tex}
\input{b-engineering-cloth-folding-pipelines.tex}


\section{Learning robotic manipulation tasks with labels} \label{sec:lit_sl}
Machine learning, a domain of artificial intelligence, is the study of algorithms that allow computers to learn from and make predictions based on data. Machine learning provides a way to deal with the inherent systematic and random errors in robotic systems and variability in unstructured environments.
This is because learning-based methods optimize for the grasping task, which implicitly adapts the behaviour to imperfections in the system, such as inaccurate sensor readings.

The following sections provide a short review of the fundamentals of relevant machine learning techniques. We discuss learning with and without labels and by trial-and-error. We discuss their relevant applications in robotic manipulation with a focus on the manipulation of deformable objects.

\subsection{Supervised learning}

Supervised learning is a machine learning paradigm that operates under the setting where there is a set of \textit{input} variables, for example image pixels, that exert influence over other \textit{output} variables, for example whether there is a shirt or trouser in the image.
\keyWithTitle{Supervised learning}{Supervised learning involves learning a mapping from input data $\vec{x}$ to the output data $\var{y}$, provided by the supervisor.}
The components building up a machine learning system are the dataset, the model, loss function and optimization algorithm. These components are discussed next.
Formally, we can denote the input data as a set $\set{X}$ consisting of vector $\vec{x}^{(i)} \in \set{X} $ with the superscript $i$ referring to the $i$th observation. In the machine learning domain, this set of predictor variables is called \textit{features}. The set $\set{Y}$ contains the output variables $\var{y}^{(i)} \in \set{Y}$. Concatenating tuples of
$\left\{\left(\vec{x}^{(i)}, \var{y}^{(i)}\right) , i \in 1,\dots,\const{N} \right\}$
, often called \textit{examples}, leads to a dataset which can be used for learning. Central in this learning procedure is the idea of \textit{function approximation} in which a function $f$, parametrized by $\vec{\theta}$, maps an input $\vec{x}^{(i)}$ to its corresponding output $\var{y}^{(i)}$:
\begin{equation*}
	f(\vec{x};\vec{\theta}): \set{X} \mapsto \set{Y}\text{.}
\end{equation*}

This mapping, also called \textit{model} or \textit{hypothesis}, comes in many forms such as linear models, tree-based methods, support vector machines and neural networks\footnote{We refer to~\textcite{Murphy2012, Bishop2006, Hastie2001} for a thorough exposition on supervised learning methods.}.
The goal of the learning procedure then becomes to adjust the parameters $\vec{\theta}$ of the model such that a certain performance measure $\mathcal{P}$ is optimized. This metric, called \textit{loss function} $\mathcal{L}$ in machine learning jargon, is specific to the task and domain in which the learning is taking place. In robotic folding for example, the robot might be presented with a candidate grasping pose $\vec{u}$. The robot then has to predict the probability $\hat{\var{y}} = Q_{\vec{\theta}}(\vec{u}, \vec{x}) = \mathbb{E}\left[ \mathcal{S} | \vec{u}, \vec{x} \right]$ of successfully grasping (success denoted with $S$) a shirt, given some input image $\vec{x}$. Note that this example implies a heavy assumption; the availability of a dataset containing tuples of $ \langle\text{grasping pose}, $ $\text{object configuration},$ $\text{probability of success} \rangle$.
In this situation, one could minimize the negative cross-entropy loss:
\begin{equation*}
	\mathcal{L}=-\var{y}^{(i)} \cdot \log \hat{\var{y}}^{(i)} +\left(1-\var{y}^{(i)}\right) \cdot \log \left(1-\hat{\var{y}}^{(i)}\right),
\end{equation*} with $\hat{\var{y}}^{(i)} = f(\vec{x};\vec{\theta})$ being the predicted output of the model for observation $i$.
The optimization problem then becomes to adjust the parameters $\vec{\theta}$ of the model $f$ using the examples $\left(\vec{x}^{(i)}, \var{y}^{(i)}\right)$:
\begin{equation*}
	\vec{\theta}^{*}=\argmax_{\vec{\theta}} \: \mathbb{E}_{p(S, \vec{u}, \vec{x})}\left[\mathcal{L}\left(S, Q_{\vec{\theta}}(\vec{u})\right)\right].
\end{equation*}
The dominant way for heavy parametrized functions such as neural networks to optimize this objective is to use gradient descent. The gradient expresses the direction of the steepest decrease of the loss function $\mathcal{L}$ with respect of the model parameters $\vec{\theta}$. By iteratively updating the parameters in the opposite direction of the gradient, in the case of a minimization objective, we gradually arrive at a local or global minimum:
\begin{equation} \label{eq:gd_update_rule}
	\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} \mathcal{L}(\vec{\theta}).
\end{equation}
$\alpha$ determines how large steps we take towards the estimated direction of the closest local minimum. Adaptive methods such as Adam \autocite{Kingma2014} allows taking variable step sizes per variable based on the historical directions of the gradient. Gradient-based optimization methods have proven to be a crucial for training highly parametrized functions such as deep neural networks, which are discussed in the following paragraphs.

\input{NN-theory.tex}

Supervised learning is an important paradigm for robotic learning because labelled data provides a clear learning signal. This is important because time spent on robots is expensive. Contrarily, the learning signal in reinforcement learning (\cref{subsec:lit_rl}) often does not optimize for direct task performance and might lead to expensive learning times on physical robot platforms. The following paragraphs discuss relevant work in applying machine learning methods for solving robotic manipulation tasks.

\subsection{Supervised learning and neural networks in robotic manipulation research}
% Generally speaking, there are two main strategies for applying supervised learning in the robotic manipulation pipeline: (1) as a perception module or (2) to map states to actions using imitation learning. 
% ---- PERCEPTION MODULE ----- %
% standard ML METHODS
Traditionally, supervised learning methods are leveraged in the \textbf{perception module} of a robotic manipulation pipeline. For rigid body manipulation, the popularity of data-driven grasp synthesis approach took off with the work of~\textcite{Saxena2008}. In their work, a logistic regression classifier is trained on synthetic data using manually engineered features. This way, they demonstrate that a robot is able to unload a dishwasher. In the domain of deformable object manipulation, ~\textcite{Ramisa2012} searches for quality grasping points of crumbled cloth in order to maximize the unfolding upon lifting. They do this by first labelling a dataset of shirts with bounding boxes containing the appropriate grasping points. Next, they train a logistic regression model to obtain the probability of the desired grasping point in a given bounding box using a bag of features from the input image. By employing the logistic classifier in a sliding window over the image, they pass image patches containing local peaks to an \gls{SVM} to obtain more accurate grasping candidates. The candidate patch is converted to 3D space by working in a calibrated environment, and motion planning is executed using inverse kinematics. Similarly,~\textcite{Wang2011} folds socks by having a perception system that uses manually engineered features for training an \gls{SVM} with Gaussian kernel to determine the type of sock in front of the robot. \Glspl{SVM} have also been used for the purpose to identify garment category and pose \autocite{Li2014, li2014volum}.
The deformable nature of cloth leads to self-occlusions making the garment type and pose classification ambiguous. The presence of this hidden state is explicitly modelled using a \gls{HMM} in \autocite{Cusumano2011}. Learning methods are also being used to find regions of interest on the cloth. For example, \textcite{Doumanoglou2016} use random forests to learn garment-specific grasping points. In \autocite{Maitin2010}, RANSAC \autocite{RANSAC} is used to find the corners of the cloth. These corners are good candidate grasping points for unfolding and identifying the type of cloth. Finally, nearest neighbours have been used to identify wrinkled regions in a washcloth in order to flatten it \autocite{Willimon2011}.

% NEURAL NETWORKS: supervised learning in perception module
The earliest work using neural networks for deformable object manipulation is \autocite{Howard2000}. In their work, they train a small feedforward neural network that learns the required minimum grasping force for lifting a deformable object. They collect the data by iteratively using more lifting force on objects with certain masses, deformability and damping.
With the breakthrough in deep learning in~\citeyear{Krizhevsky2012} by~\textcite{Krizhevsky2012}, deep neural networks have found their way into robotic manipulation, starting in the rigid body manipulation domain. A successful approach to training deep neural networks in a supervised setting for robotic manipulation is to use \glspl{CNN} as grasp success predictor.~\textcite{Levine2016} train a \gls{CNN} on a large dataset of $800.000$ grasping attempts to learn to predict the grasp success probability of a grasping pose, given an input image. To sample candidate grasping points, they employ CEM \autocite{CEM}. Dex-Net \autocite{dexnet2} also trains a \gls{CNN} to predict the quality of a grasping candidate. This network is trained using a simulated dataset where objects are put into randomized poses on a plane. They use simulation to evaluate different grasping wrenches using analytic grasp metrics. Their model shows impressive generalizability to the real world, on different models not seen during training. The Dex-Net framework has been extended to work with suction grippers \autocite{dexnet3}, use dual-armed robots \autocite{dexnet4} and generate grasping candidates in the network \autocite{Satish2019}.

% ---- IMITATION LEARNING ----- %
% Standard ML methods
Another strategy to train controllers using supervised learning is \textbf{behavioural cloning}. Behavioural cloning is a type of imitation learning \autocite{Argall2009} in which task demonstrations are used for learning task execution. In behavioural cloning, a sequence of states and actions, as executed by a demonstrator, is recorded as dataset for a supervised learning algorithm. The goal then becomes for the model to predict the action a demonstrator would choose, given a certain state.
% \todo{Voegen we hier nog een degelijkse uitleg toe wat de verschillende LfD aanpakken zijn? Lijkt me relevant om te weten dat methodes zoals kinesthetic-teach-in niet echt nuttig zijn voor cloth domein.} 
An in-depth view of the field of learning from demonstration is given in \autocite{Argall2009}. In the case of robotic laundry,~\textcite{Jia2019} learns single robotic laundry tasks for flattening, folding and twisting cloth by imitating human examples. Notably in their work is representing the robotic controller using random forests \autocite{Breiman2001} instead of neural networks. The rationale is given by the non-parametric nature of random forests to dynamically change the number of leaf nodes based on the given imitation data and new cloth configurations.
Example demonstrations are also used in deformable object manipulation to tie knots. ~\textcite{Schulman2016learning}, for example, uses example demonstrations for non-rigid warping \autocite{Chui2003} based on point-cloud registration of the scene to tie knots with a robotic manipulator. The general idea is to warp the demonstrated trajectory to match the current setting, which may vary in initial conditions and knot geometry. Closely related is the work in \autocite{Morita2003} where examples and solutions strategies from knot theory is embedded to perform motor control.
% NN for imitation learning
Neural network models have also been a strong candidate for robot learning from demonstration \autocite{Ravichandar2020}. Given the cost associated with real robot rollouts, it is beneficial to lower the dataset requirements by having access to the motor control outputs or to decrease the input dimensionality by, for example, avoiding learning from pixels. This explains the popularity for teleoperated \autocite{Zhang2018,Duan2017} and kinesthetic teaching \autocite{finn2017one} approaches. However, it is difficult to teleoperate a robot or physically manipulate a robotic arm to fold clothing items. This is because the speed and forces associated while executing the task is relevant for achieving proper folds. One method to compensate for this difficulty is to solve the imitation learning task in simulation and trying to transfer it to the real world. This is explored in \autocite{Seita2020} that generates example demonstrations in simulation and uses behavioural cloning to train a motor policy network to flatten a towel. To fine-tune this policy outside the seen dataset distribution, they employ dataset aggregation (DAgger) in which an oracle policy is used to label the unseen states during training. An alternative is given in \autocite{Sundaresan2020} that uses simulated data to learn visual object descriptors indicating the segments of a rope. Such embeddings implicitly encode geometric structure which can then be used for knot-tying using example demonstrations.
Although imitation learning is a viable alternative for learning to manipulate objects, a general problem plaguing imitation learning methods is generalizing to unseen scenarios. In the case of trajectory execution, errors can accumulate which drastically increases the probability of task failures. This is why existing methods apply data augmentation, include teacher advice or use reinforcement learning (\cref{subsec:lit_rl}).

% ---- LEARN DYNAMICS MODEL ----- %
Another approach to leverage the expressiveness of deep neural networks in a supervised setting is to train a \textbf{dynamics model}. This model, often called world model, is obtained by training on $\tuple{\text{state}, \text{action}, \text{next state}}$ tuples. The problem that world models try to solve is finding the optimal sequence of actions that brings the given input state to the desired output state by querying the world model. Or in other words, the robot knows \textit{how} to act while the user can tell the robot \textit{what} it should do. To manipulate a rope into a desired shape, for example an S-shape,~\textcite{Nair2017} let the robot make arbitrary manipulations on the rope while recording the state transitions. This data is then used for training the inverse dynamics model. Planning can then be done using the world model by giving transitionary keyframes that define subgoals to the robot. A similar world model is trained in \autocite{Ebert2018} where the model learns to predict future pixels given the planned actions. The data collection is done in a self-supervised way in which the robot does motor babbling. The trained world model then enables model predictive control in which they show good performance for folding cloth and towels. Whereas the training data for the world model in \autocite{Nair2017} is given by self-supervision, in \autocite{Yang2016} the data is given by teleoperating a humanoid robot with a virtual reality headset. This training data is then passed into an autoencoder which produces a time series in latent space. A second stage neural network then learns cloth dynamics by sliding a window over the encoded time series. A further attempt to embed the world model into the control module can be done by sandwiching a fully-connected network between the encoder and decoder \autocite{Tanaka2018}. Instead of embedding a controller module, other work embed physics priors into the network architecture. By assuming the deformable objects are build-up of small, connected particles, it is possible to represent their connectivity and interactions in a graph neural network. This allows to efficiently learn deformable object dynamics to solve for downstream control tasks such as poking deformable objects \autocite{Mrowca2018} and merging liquids \autocite{Li2018}.

% Overgang: rational voor RL
% \paragraph{Conclusion} TODO: check of dit nu niet raar is! hier stond eerste tussentital
\keyWithTitle{Main finding}{Label-based learning for robotic manipulation tasks is efficient but not always effective or possible.}

To conclude, framing robotic manipulation of rigid and deformable objects as a supervised learning problem has been successful for estimating the state of deformable objects or manipulating cloth by means of behavioural cloning. However, training with hand-labelled or generated data has two main issues \autocite{pinto2016supersizing}. The first issue is the human bias towards preferring grasps poses that are similar to the way a person would grasp an object. This discourages exploration of unconventional grasp configurations. The second issue is the cost to exhaustively evaluate all possible grasps because an object can be grasped in multiple ways. Therefore, learning on robots requires a method that can work without human supervision.


\section{Learning robotic manipulation tasks without user-provided labels} \label{sec:lit_learning_without_labels}
Annotating datasets is an expensive effort that requires manual labour in order to ensure high-quality labels. For example, labelling all wrinkles in a cloth of a human folding clothing in a video of \qty[mode=text]{60}{\second} recorded at \qty{60}{\unit{fps}}, requires annotating \qty{3600}{frames}. Within machine learning, there exists a body of research that avoids this costly annotation effort. We discuss two approaches of learning without manually provided labels, using unsupervised and self-supervised learning methods, next.

\subsection{Unsupervised learning}
Whereas supervised learning methods search for a mapping between inputs and labels of observations, unsupervised learning uncovers structure in the input data without the use of any labels.
\keyWithTitle{Unsupervised learning}{Unsupervised learning extracts regularities from unlabelled\\ datasets in order to reduce the description of the data to their most characteristic elements.}
The first class of unsupervised learning applications is \emph{clustering}: segmenting data into similar groups based on a similarity metric that calculates the distance between observations. In the case of robotic folding, clustering is a popular method for segmenting an input image in order to separate different clothing articles in the scene \autocite{Doumanoglou2016,Maitin2010,Jia2018}. The second class of unsupervised learning is dimensionality reduction methods, which reduces the dimensionality of the features of a dataset. Feature extraction, in particular, looks at projecting the original input features to a new space of smaller size while preserving as much of the significant structure of the input space as possible. Reducing the input dimensionality is important for machine learning algorithms because their complexity not only increases with the size of the input dataset but also with the number of features.
% Feature extraction in cloth manipulation domain
The practice of feature extraction is present in the deformable object manipulation domain, given that high-dimensional camera streams are often used as a sensor for estimating the state of cloth. \textcite{li2016multi} for example, considers the problem of cloth flattening for robotic ironing. To reduce the dimensionality of the image of a discontinuity scan of a cloth, they employ SIFT descriptors \autocite{lowe1999object} as input for a support vector machine classifier to assign the probability of a discontinuity to be a permanent wrinkle. Other work \autocite{Jia2018} observes that deformations on cloth can be detected as shadows and shape variability. To exploit this visual property, they apply a set of Gabor filters on the input image and accumulate the filtered images into a histogram as a high-level state representation of the cloth.
A noticably early work of using neural networks is the work of \textcite{Foresti2004} in which the problem of grasping fur tails from a conveyor belt is considered. To segment the different furs present in the image, they train self-organizing maps \autocite{Kohonen1982}; a neural network in which neurons compete to be activated by the input signal. This results in disconnected regions of interest that are joined using skeletonization. Finally, a heuristic is used to determine and grasp the largest fur.
Autoencoders, i.e.\ neural networks trained with reconstruction tasks, are another useful feature extractor. \textcite{Yang2017} for example, use the latent space of a deep convolutional autoencoder as input for a deep fully-connected neural network that acts as dynamics model of the cloth. Depth sensor streams is another high-dimensional input modality for cloth state estimation, which can be reduced to a lower dimension. \textcite{Ramisa2013} for example, transform depth images to SIFT-like descriptors that describe a patch based on the distance between the normals in the patch and a reference set of normal directions.

% VIZ, umap, tsne. Kort uitleggen wat hun doel is, hoe ze verschillen van met bv PCA (zie intro van umap paper) en hoe ze gebruikt worden. Geef als voorbeeld van TCN paper waar ze embedding analyzeren. 
Together with data preprocessing, dimensionality reduction methods also play an important role in data visualization. The analysis of the underlying structure of a high-dimensional dataset can be facilitated when the data is represented in fewer dimensions. In the context of deep learning, non-linear dimension reduction methods are being used to study the semantics of what the hidden units are encoding. These methods, in particular, look at reducing the input space to the $2$D plane in order to enable the plotting of the underlying patterns in the data. Amongst non-linear dimension reduction methods, t-SNE and UMAP are often chosen techniques for visualization purposes. Both methods construct a graph and optimize a subsequent low-dimensional embedding that preserves the structure of that graph. At the core, these methods employ loss functions that make similar points attract each other and push dissimilar points away. From a high-level perspective, \gls{TSNE} defines distances between samples as conditional probabilities and optimizes a low-dimensional embedding in which the relative distances between samples match those of the original high-dimensional distances. This optimization is defined as minimizing a Kullback-Leibler divergence between the pairwise distances ins the low-dimensional embedding and the high-dimensional input space. \gls{TSNE} is, for example, being used in \glspl{TCN} of \textcite{Sermanet2017TCN} to show that the learned embeddings semantically encode the same pose of robots and humans close in embedding space. For example, both images of a robot and a human crouching and extending the same arm are encoded proximate in embedding space. Similarly, in Atari DQN \autocite{mnih2015human} the last hidden layer is represented with \gls{TSNE} embeddings to discover that visually dissimilar states are close in embedding space due to having the same expected reward.
\Gls{UMAP} is another state-of-the-art dimensionality reduction method that has been successfully employed in biology \autocite{cao2019single}, machine learning \autocite{carter2019activation} and social sciences \autocite{diaz2019umap} to discover underlying structures in high-dimensional datasets. \Gls{UMAP} constructs a weighted graph of nearest neighbours with the weights representing the probability that two points are connected. Then, UMAP optimizes a low-dimensional representation of this graph that is structural as similar as possible. This is done by minimizing the cross entropy in order to measure the distance between the high-dimensional and low-dimensional graph. \gls{UMAP} has been argued to better preserve global structure compared to \gls{TSNE} \autocite{becht2019dimensionality} resulting in semantically more meaningful clusters.

\subsection{Self-supervised learning} \label{subsec:lit_ssl}
A large class of successful \gls{ML} methods rely on some form of supervision. In robotic folding, this supervision can be labelling the type of clothing item in front of the robot, specifying which action a human would take or which torques to exert on the motors. Collecting this data is expensive, sometimes difficult and prone to bias and errors \autocite{mehrabi2021survey}. Unsupervised learning is on the other end of the \textit{supervision spectrum}, but it is hard to know which signal it will pick up for learning. Providing supervision while building up a dataset, without manually labelling every sample, is known as \emph{self-supervised learning}. Therefore, by having a data generation process that creates pseudolabels, one can use well-established supervised learning algorithms. The labelling process allows us to inject prior knowledge about the task by inventing mock tasks, also known as \emph{pretext tasks}. The goal of self-supervised learning is not to solve this invented task but rather to learn meaningful representations that can be used for downstream tasks. In cloth folding, for example, we can present the network with a shirt, rotate it and ask the model to predict how many degrees the shirt was turned. Although this example task is only deployable in a narrow set of tasks, we do not care about the accuracy. Instead, we want the network to learn a meaningful latent space.

There are two broad categories of self-supervised learning. The first category, generative methods, aims to reconstruct the input signal while learning a latent space. These methods are popularized by generative adversarial networks \autocite{goodfellow2014generative}. The second category, discriminative methods, reframes the self-supervised problem as a classification task. In particular, \emph{contrastive} self-supervised learning looks at \textit{contrasting} positive and negative examples. More formally, let \( h( \vec{x} ) \in \mathbb{R}^d \) be the encoding of an input sample \( \vec{x} \), called the anchor. We define \( \vec{x}_p \) as a positive sample and \( \vec{x}_n \) as a negative sample. The goal then becomes to encode the anchor and positive close in embedding space compared to the anchor and the negative:
\begin{equation*}
	\operatorname{dist}{\left( h(\vec{x}),h(\vec{x}_p) \right)} \leq \operatorname{dist}{\left( h(\vec{x}),h(\vec{x}_n) \right)} .
\end{equation*}
The distance function $\operatorname{dist}{}$ measures how similar samples are. For example, in the case of robotic folding, we would want the embedding to push frames of folded shirts closer together than completely crumbled shirts. The hypothesis is that by forcing similar items to be close in embedding space, the network has to learn relevant features and builds a semantically meaningful embedding this way.
This is done by enforcing invariances in the embedding. For example, if the network is presented with an anchor image, a rotated anchor image and a random different image, the network learns to become rotational invariant. Other popular pretext tasks in literature to learn different types of invariances are learning to colourize images \autocite{Zhang2016Color}, reconstructing the original input \autocite{Pathak2016} and predicting the relative position of two random patches \autocite{Doersch2015}.

\keyWithTitle{Self-supervised learning}{Self-supervised learning aims to learn semantically meaningful embeddings by solving mock tasks that force the model to attend to task-relevant features while learning useful invariances.}
% \key{Self-supervised learning aims to learn semantically meaningful embeddings by solving mock tasks that force the model to attend to task-relevant features while learning useful invariances.}

Using contrastive objectives to learn good representations have been popularized in the NLP domain. In \autocite{mikolov2013distributed}, contrastive training was done by using co-occurring words as semantically similar for learning word embeddings. Extensions to images, video and speech data were popularized with contrastive predictive coding \autocite{oord2018representation}. This method maximizes the mutual information between the predicted encodings and their corresponding positive samples. Similarly, SimCLR \autocite{chen2020simple} maximizes agreement between an image and its applied data augmentations by using a cosine similarity loss.

In this work, we focus on extracting self-supervised signals from video task demonstrations. The inherent structure in videos is the temporal dimension, which can be used as a supervisory signal to provide contrasting examples. The goal then becomes to recover the temporal coherence of a video. One of the first works \autocite{Misra2016} leveraging time as contrastive signal inputs a sequence of frames and classifies whether the frames are in the correct order. Later work \autocite{Lee2017,Fernando2017} also frames self-supervised learning as a classification task in which the correct temporal order has to be determined. Follow-up work has looked at using self-supervised embeddings as a reward or progression signal for learning agents.
% \paragraph{Gebruik van de geleerde SSL embedding voor agent learning task solving purposes}
% A number of prior works construct reward functions, or equivalently process monitoring metrics, in latent spaces trained with time as a supervisory signal.
In~\cite{Singh2019}, they construct a reward function based on an image classifier trained on successful goal states reached by teleoperating the robot towards the end state. In~\cite{Hartikainen2019}, time is used as a learned distance function for assigning environment rewards. However, their approach requires human intervention in order to select the desired goal states. \cite{Nair2018time} also uses time as a supervisory signal in videos of expert demonstrations to learn an optimal trajectory of states. However, they assume the possibility of visually removing the end-effector from the scene which is not possible for all tasks. For example, it is not possible to drop a shirt in mid-air while being folded. Other work~\cite{Nair2018visual} looks at expressing the reward function as the distance in latent space between the current state and the goal state. However, this is not possible when there is a trajectory in latent space that has to be followed in order to execute the task. This problem is enlarged when the start state is very similar to the end state. In this scenario, the agent would not be incentivized to leave the start state because it already receives high rewards due to being close to the end state. % bv. bij het cube stacking blok gaat bij de eerste stap (grijp een kubus) de embedding waarschijnlijk altijd op dezelfde plaats liggen als op het einde vd taak dus je moet hier tussen kunnen differnetieren.  
Alternatively, it is possible to add a contrastive loss as an auxiliary objective in RL as done in CURL~\cite{Srinivas2020CURL}. The authors show that their method outperforms other learning methods on the DM Control Suite, i.e.\ a set of continuous control tasks in simulation.
An important self-supervised architecture we use in this work are \glspl{TCN} \autocite{Sermanet2017TCN}. The central idea in this approach is to leverage crossmodal inputs, such as different camera viewpoints, in order to find differences in frames that cannot be attributed to a changing viewpoint. We discuss this in detail in \cref{ch:reward_functions}.

In the domain of deformable object manipulation, self-supervised methods have been used primarily to do the state estimation of objects. A latent space describing the state of a rope is learned using self-supervised learning on simulated images in \autocite{yan2020learning}. This model is then used as forward dynamics model in which an action trajectory is sampled that minimizes the distance towards the given goal state. The actions are chosen such that only points on the rope are considered. A similar approach for bringing fabric into a desired state is explored in \autocite{fabric_vsf_2020}. They learn an image prediction model in simulation, which can be used to solve arbitrary goals at test time via model predictive control. They apply domain randomization to transfer fabric smoothing policies to a real-world surgical robot. However, their approach fails to transfer folding tasks successfully due to the sim-to-real gap. An alternative sim2real solution is explored in \autocite{Mengyuan2020} that also train an autoencoder to predict the dynamics of deformable linear objects. %TO DO: je schrijft 2x sim 2 real op een andere manier. 
The encoder uses a transformer architecture \autocite{vaswani2017attention} that iteratively refines the estimation of the image space coordinates of points on the rope. The network is then fine-tuned on real images using a self-supervised objective. This self-supervised objective encodes a colour contrast cue: the target deformable linear object has a different colour from the background. This prior can be modelled with a Gaussian mixture model that segments the input image. By rendering the output rope state from the network to image space, a differentiable loss can be defined on the segmented input image to refine the network. This method leverages simulated data for pretraining of the network, approximately \qty{5000}{} real images for fine-tuning and is able to accurately manipulate a rope into a given target state.

\section{Learning robotic manipulation tasks from interaction} \label{subsec:lit_rl}
\subsection{Reinforcement learning}

\glsreset{RL} % because this is the RL part. 
In supervised learning, a clear learning signal allows training models such that their output matches the labels given in the training set. This requires obtaining a dataset that labels every observation with the correct response. For example, in the case of a robot folding a shirt, supervised learning requires providing supervision on how to move every joint for all arm and shirt configurations in the training set. This sequential decision-making process might alternatively better be solved by providing an indication on how good the model is behaving. For example, we might give a robot a positive reward when it has folded a shirt without wrinkles and penalize it when it throws the shirt of the table. This approach of giving agents rewards in an environment is formalized as \gls{RL} and is an eminent approach for learning control policies with minimum user intervention. RL has been successfully applied in domains ranging from game playing \autocite{mnih2015human}, helicopter flight \autocite{ng2003autonomous}, autonomous driving \autocite{sallab2017deep}, locomotion \autocite{tan2018sim} and robotic manipulation \autocite{levine2016end}.
\keyWithTitle{Reinforcement Learning}{Reinforcement learning is interactively observing and influencing the environment while receiving rewards in order to learn what to do in order to solve a task.}

\glsreset{MDP}
Sequential decision making is formalized as a \gls{MDP}: given a sequence of states, determine the action that will maximize the expected discounted future reward. This statement implies that an agent can observe its environment, exert influence on its environment through actions and has a notion of what constitutes good behaviour. More formally, an \gls{MDP} is a tuple $\left(S, A,P, \gamma, R\right)$ where:
\begin{enumerate}
	\item $S$ is a set of states representing the environment. For example, the set of joint values of a robot arm and the position of the sleeve of a shirt.
	\item $A$ is a set of actions. These actions are performed by the agent and influence the environment. For example, moving the end-effector of the robotic arm up or downwards.
	\item $P$ are the state transition probabilities. This is a distribution over states and actions indicating the probability to arrive at a new state: $P\left(s_{t+1} \mid s_{t}, a_{t}\right)$.
	\item $\gamma \in \mathopen[0, 1\mathclose[$ is the discount factor and used to discount future rewards to the present.
	\item $R = \mathbb{E}[R_{t+1} \mid S_t=s, A_t=a]$ is the reward function, based on the action taken and the resulting state the environment transitions to.
\end{enumerate}
Decision making in an MDP goes as follows: the environment is initialized in a certain state $s_t$. The agent chooses an action $a_t$ to execute in the MDP. This way, the state of the MDP transitions to a successor state $s_{t+1}$ governed by $P$. Based on this new state, the agent receives a reward $r_{t+1}$\footnote{Both notation $r_{t}$ and $r_{t+1}$ are used in literature. We use $r_{t+1}$ to denote that $s_{t+1}$ and $r_{t+1}$ are jointly determined.} and chooses a new action $a_{t+1}$. This process is visualized in \cref{fig:RL_loop_robotics} and repeats until the environment signals a terminal state, for example, a robot manipulator successfully folds a shirt. \Glspl{MDP} with terminal states are called \emph{episodic} \glspl{MDP}. Furthermore, a distinction is made between \emph{states} and \emph{observations}. A state is assumed to contain all relevant information to make an optimal decision. In technical jargon, this is called the \emph{Markov property}: the future depends only on the current state and action, but not on the past. However, it is hard to capture all task-relevant features with sensors. For example, a single camera image of a crumbled shirt does not satisfy the Markov property because the occlusions do not allow to make informed decisions about the occluded parts. In such cases, the states are called \emph{observations} $O$ and the formalism a \gls{POMDP}.

\begin{figure}[htb]
	\centering
	\subfile{figures/RL-loop-robotics}
	\caption{Canonical flow of the RL loop in the context of robotics.}
	\label{fig:RL_loop_robotics}
\end{figure}

The eventual factor driving the decision of the agent is the reward function. The goal of an RL agent is to maximize the sum of expected discounted rewards $G_t$:
\begin{equation}
	\begin{split}
		G_{t} & =R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1} R_{T} \\
		&		=\sum_{k=t}^{T} \gamma^{k-t} R_{k+1}.
	\end{split}
\end{equation}
To cope with the difference between the immediate reward $R_t(s_t, a_t)$ of a state-action pair and the long-term value of taking action $a_t$ in state $s_t$, we introduce the notion of \emph{value functions}. A value function denotes how good it is to be in a certain state. More formally, the value function $v_{\pi}(s)$ expresses the expected, cumulative, discounted, future reward of a state:
\begin{equation} \label{eq:value_function}
	v_{\pi}(s)=\mathbb{E}\left[G_{t} \mid S_{t}=s\right],
\end{equation}
or of a state-action pair:
\begin{equation}
	q_{\pi}(s, a)=\mathbb{E}\left[G_{t} \mid S_{t}=s, A_{t}=a\right].
\end{equation}
The function $q_{\pi}(s, a)$ is called the Q-function. This notation introduces the policy $\pi$ in the subscripts $v_{\pi}(s)$ and $q_{\pi}(s, a)$ . This is due to agents their rewards depending on which actions they will take in the future. A policy is a distribution over $a \in \mathcal{A}(s)$ for each state $s \in \mathcal{S}$: it maps the probability of selecting an action given a certain state. Hence a Q-value denotes the total reward the agent receives in state $s_t$ for taking action $a_t$ and then following policy $\pi(s,a)$ in expectation. The goal of the agent then becomes to find the policy that maximizes the value functions:
\begin{equation}
	v_{*}(s)=\max _{\pi} v_{\pi}(s)=\max _{a} q_{\pi^{*}}(s, a).
\end{equation}
Writing out the expectation of \cref{eq:value_function} gives rise to the Bellman equation:
\begin{equation}
	v_{*}(s)=\max _{a} \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right],
\end{equation}
with $s^{\prime}$ being the successor state of state $s$ after taking action $a$. In the case of Q-values the Bellman equation becomes:
\begin{equation}
	q_{*}(s, a)=\sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right)\left[r+\gamma \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)\right].
\end{equation}
The Bellman equation provides recursive decomposition and value functions that allow to reuse sub-solutions. Hence, the Bellman equation can be solved using dynamic programming. However, dynamic programming algorithms, such as value iteration and policy iteration \autocite{Sutton2018}, requires knowing the transition and reward models of the MDP. Modern RL algorithms are often divided on a spectrum ranging from exclusively finding the value of states to finding how to behave optimally without inferring any value function. This distinction is discussed next.

\paragraph{Value-based methods}
The first category of RL algorithms are value-based methods, also called critics, which try to discover the value function by interacting with the environment. This principle is known as \gls{TD} learning \autocite{boyan2002technical} and is argued to be the most unique contribution to RL \autocite{Sutton2018}. \gls{TD} learning combines the ideas of dynamic programming (problem decomposition and reusing subsolutions) and Monte Carlo methods (averaging complete returns by sampling the probability distribution). The central idea is to learn the value function directly from experience with bootstrapping, in a model-free and online manner. A popular instantiation is Q-learning \autocite{watkins1992q} which iteratively updates the Q-function as follows:
\begin{equation} \label{eq:q-learning-update-rule}
	Q(s, a) \leftarrow Q(s, a)+\alpha\left[\underbrace{r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)}_{\text{TD error}}\right]
\end{equation}
Representing the value function $Q(s, a)$ has originally been done with tables \autocite{watkins1992q} but do not scale to real problem state and action spaces. The memory and time required to fill a table representing all possible states of, for example, a camera image are infeasibly high. This is where reinforcement learning borrows the idea of supervised learning, or more generally function approximation, from the machine learning domain. By using machine learning models as Q-function representations, an agent can use seen data from interaction with the environment to learn to generalize to states never seen before. A broad handling and dicussion of both tabular and approximate RL methods is discussed in the textbook of~\textcite{Sutton2018}. Neural networks in particular have been early candidates as function approximators for value functions, often called Q-networks. Gradient-based optimization of the Q-network is then possible by backpropagating the TD error indicated in \cref{eq:q-learning-update-rule}. Neural networks are notoriously difficult to train in an TD setting because updating weights has unpredictable changes at other places in the stace-action space. Mitigating this non-stationary property of the data has been central to make value-based RL methods work on real-life tasks. In the context of control, \textcite{riedmiller2005neural} developed \gls{NFQ} in which a three-layered feedforward neural network is trained to predict the state-action values of control tasks such as cartpole regulation. \Gls{NFQ} is characterized by using a replay buffer  \autocite{lin1992reinforcement}: storing the experience tuples $(s_t, a_t, r_t, s_{t+1})$ in memory for training purpose. \gls{NFQ} employs this replay buffer in a batch RL manner: the neural network is trained from scratch on the whole replay buffer each time the training loop is called. Scaling this system up to working with image-based inputs using \glspl{CNN} is called \gls{DQN} \autocite{mnih2015human}. \Gls{DQN} learns online by sampling the replay buffer and computes future Q-values ($r+\gamma\max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)$) in \cref{eq:q-learning-update-rule}) with older versions of the Q-network, called the target network. A major problem with value-based methods is that they do not optimize the task objective directly. Instead, value-based agents search for the correct value of state-action pairs instead of searching how to behave optimally. This problem is solved by the next category of RL agents, policy-based methods, which is discussed in the next subsection.

\paragraph{Policy-based methods}
The second category of RL algorithms are labelled policy-based methods, also called actor and actor-critic methods\footnote{Historically, a distinction was made between actor methods and actor-critic methods depending on whether a critic with parametrized value function approximation was used. However, modern policy-based methods all use some form of value function approximation rendering this distinction obsolete.}, which optimize a parametrized policy directly. Assuming a differentiable policy $\pi_{\symbfup{\theta}}(a \mid s)$ parametrized by $\symbfup{\theta}$, the goal of policy search becomes to find the optimal policy parameters $\symbfup{\theta}^{*}$:
\begin{equation} \label{eq:policy-max}
	\symbfup{\theta}^{*} = \argmax_{\symbfup{\theta}} \mathbb{E}_{\symbfup{\tau} \sim P_{\symbfup{\theta}}(\symbfup{\tau}) } \left[ \sum_{t}^{T} r(s_t, a_t) \right].
\end{equation}
In this notation, $\symbfup{\tau}$ denotes a trajectory of state-action pairs $(s_0, a_0, s_1, a_1, \cdots)$. Parametrization of the policy offers a natural way to deal with applications in continuous action spaces, compared to value-based methods that require evaluating all actions. Policy-based methods also optimize for task performance directly. However, policy-based methods are less sample efficient compared to value-based methods because new data needs to be sampled each time the policy changes. Common policy search methods are A3C \autocite{mnih2016asynchronous}, PPO \autocite{schulman2017proximal} and TRPO \autocite{schulman2015trust}.

% DDPG and SAC: do both q-learning and policy optimization
Value-based methods have borrowed ideas from policy-based methods to extend Q-learning to a continuous action space. Notably, DDPG \autocite{lillicrap2015continuous} and SAC \autocite{haarnoja2018soft} which learns a parametrized policy such that the $\max$ operation in \gls{DQN} of \cref{eq:q-learning-update-rule} is differentiable with respect to the action argument: $\max_{a^{\prime}} Q_{\phi}(s, a^{\prime}) \approx Q_{\phi} (s, \mu_{\theta}(s))$, with $Q_{\phi}$ being the value function neural network parametrized by $\phi$ and $\mu_{\theta}$ the policy neural network parametrized by $\theta$. DDPG have been succesfully used to  train a seven degrees of freedom arm for reaching and door opening tasks \autocite{gu2017deep}.

\paragraph{Comparison to supervised learning.}
The sequential decision making of RL introduces some challenges not present in supervised learning.
In RL, the consequences of actions are often delayed, making it hard to assign actions to outcomes. For example, if a robot fails to fold a piece of cloth after $100$ steps, it is hard to know whether this was due to end-effector losing grip of the cloth at step $99$ or the fast motor accelerations at step $42$. This problem is known as the credit assignment problem.
Additionally, the sequential nature of RL makes it difficult to reuse data, making RL more data-intensive. This is due to many RL algorithms having an \emph{on-policy} nature: each time the policy that we want to optimize changes, we need to collect new data of it.
Another aspect of decision making is trading-off exploration and exploitation: if the robot knows that folding a shirt via the sleeves leads to success, why would it try other methods such as lifting the shirt in the air and risk failing? This exploration-exploitation dilemma has been coped with by introducing randomness in policies or curiosity \autocite{pathak2017curiosity} but remains an unsolved problem \autocite{Sutton2018}.

\paragraph{RL in the context of optimal control.}
\Gls{RL} and classical optimal control both address the problem of finding an optimal policy that optimizes an objective function in a system described by states, actions and a model governing the state transitions. However, optimal control assumes perfect knowledge of the state transitions while RL operates directly on measured data (i.e.\ rewards from environment interaction). RL fills the gap in optimal control for tasks that are analytically intractable and can be viewed as adaptive optimal control \autocite{sutton1992reinforcement}.

\paragraph{RL in the context of robotics.}
Much of the work for learning manipulation skills for deformable objects has been inspired by work done in the rigid object's domain. These methods, in turn, often rely on work done on learning to play games and solve toy tasks in simulation environments. First attempts at transferring vision-based DQN to visual servoing were attempted in \autocite{Zhang2015} but failed. Their end-to-end approach to frame the inherent continuous action spaces of robotic manipulation to a discrete algorithm such as DQN is to discretize the joint motor outputs into $9$ motor actions bins. To further reduce the search space, they reduce the number of joints of the robot from $7$ DOF to $4$ DOF. They train a DQN controller in simulation while adding noise and task variations. However, transfer to the real robot failed due to the low simulation fidelity and inappropriate reward functions. Later work \autocite{James2016} managed to transfer virtually trained DQN agents on pixels by using high-fidelity simulation and careful reward tuning. With the introduction of continuous action spaces for value-based agents, more specifically DDPG \autocite{Lillicrap2015}, DQN variants started being successfully applied in the robotic manipulation domain \autocite{Gu2017}. This success was mainly driven by directly representing the continuous action space of robot actuators, reusing past experiences for training and working in parallel.

Deep RL for deformable object manipulation has arguably first been used in \autocite{Matas2018}. By training a DDPG agent in simulation, they demonstrate limited transferability to the real world using domain randomization to fold a towel. They find that a lack of simulation tools for cloth is the main factor limiting their results. The same approach is improved in \autocite{Jangir2020} by warm-starting the learning with example demonstrations. An additional speed-up can be obtained by filling the replay buffer with example demonstrations \autocite{Tsurumine2019}. ~\textcite{Wu2020} examines how to link two policies, i.e.\ picking and placing cloth which are normally independently trained. They opt for structurally encoding the relationship between picking and placing of cloth. This is done by having the second learned policy, i.e.\ the place policy, receive the last output of the picking policy. This way, the network can optimize the picking point that gives the most value during placing. Batch RL methods, a variant of RL in which the agent is trained once on a dataset without interacting with the environment, is explored in \autocite{lee2020learning}. Instead of generating an interaction dataset with random motor actions, they use a motor control heuristic. Such heuristics allow collecting data that contains more meaningful interactions compared to motor babbling. The train a fully convolutional DQN agent offline, resulting in a Q-value heatmap per possible action. This approach is successful in achieving simple folds in rectangular cloth, with one hour of real-world data. Another DQN architecture is explored in \autocite{seita2021learning}. They employ Transporter networks, an architecture that predicts the spatial displacement of a local area, to learn to fill deformable bags with objects in simulation.

Transitioning from solving problems with supervised learning to using RL carries problems concerning data efficiency, generalizability, delayed rewards and exploration-exploitation trade-off. These problems are enlarged in the robotics domain.
Experience on a real physical system is expensive to obtain, so reusing past data and trading-off exploration and exploitation becomes even more important.
Additionally, while RL is already faced with reproducibility issues \autocite{henderson2018deep}, reproduction and repetition on real robots are non-trivial due to robot wear, noisy state observation and stochastic action execution. This real-world reproducibility issue also makes it difficult to establish a common ground for benchmarking RL algorithms on physical robots.
A category of RL algorithms improves the sample efficiency of RL by learning the system dynamics from interaction. However, small errors under this model-based RL approach accumulate, making it hard to transfer the results to the real world.
Similarly, latency issues also plague effective learning: delays between the real state and state observation and between the action choice and action execution requires some form of memory in order for the state to be Markovian.
A problem often neglected in the general RL community is safe exploration: a cloth folding robot, for example, might tear a shirt to pieces.
Finally, although RL offers a significantly easier approach to control: specify the reward instead of the behaviour, finding an appropriate and implementable reward function have proven difficult. Specifying and finding a suitable reward function for RL problems is discussed next.

\subsection{Reward learning}  \label{subsec:lit_reward_learning}

Reinforcement learning holds a major advantage over supervised learning when it comes to output specification: generating a reward signal requires less knowledge about the domain compared to specifying which actions to take. However, the reward function is a crucial ingredient that needs to be tuned in order for an RL agent to perform well \autocite{Sutton2018}. In particular, the episodic nature of robotics tasks makes reward sparse and difficult to learn from. Additionally, many robotic manipulation tasks are multi-objective. For example, folding a shirt often adheres to the subtasks discussed in ~\cref{sec:lit_cloth_folding_pipelines}: isolating the clothing piece, unfolding, folding, flattening and stacking. Therefore, the RL practitioner weights features that describes task performance. For example, in the peg-hole insertion task in \autocite{vecerik2018leveraging}, the reward function weight the distance between the peg-socket and the peg-hole. Notably, the authors conclude that assigning weights to the features of the reward function is delicate and crucial for task performance. In practice, constructing reward function components and weighting them is often done by trial-and-error or a sweep over weights.
\key{Learning reward functions is a viable alternative to manually specifying rewards as a way to avoid bias, overfitting and underspecification.}
This process is known as reward shaping \autocite{laud2004theory}: tuning the reward function based on empirical task performance. Hence, reward shaping is a method to embed prior task knowledge in the reward function in order to accelerate the learning process. However, even this trial-and-error process requires specifying which components constitute the reward function and how they are measured. For some domains, specifying and measuring the reward function components is non-trivial. A reward function for autonomous driving for example, should take collision with other objects, passenger comfort and in-lane driving into consideration. Cloth folding suffers from the same specification problem in addition to being hard to measure due to difficult state estimation caused by the deformations. Behavioural cloning is not a suitable alternative to learning the task from demonstration because it learns to copy the task execution instead of learning the task intent. Additionally, differences between the learning agent and the demonstrator's morphology make copying behaviour ambiguous.

% Reward estimation in cloth RL
Defining a reward function for the robotic folding task is difficult because it requires the state estimation of the cloth. Previous work \autocite{Doumanoglou2016,Miller2012} has extracted the contour of the textile using colour segmentation. In \autocite{Balaguer2011}, a marked towel is tracked, allowing the calculation of the distance between points on the towel from the training sample and the example demonstrations so that it can be used as a reward for the agent. Their method requires prior information about the shape of the object in order to reconstruct the missing market points. More recent work in using (deep) reinforcement learning for robotic folding also used vision-based methods to define the reward function for the agent \autocite{Tsurumine2019, Matas2018}. However, relying solely on visual inputs and marker clues does not scale well.

%\paragraph{Inverse reward learning}
Another methodology to leverage example demonstrations is inverse reinforcement learning. Inverse RL is a category of algorithms that learn reward functions from demonstrations \autocite{Ng2000}. Formally, inverse RL can be defined using the \gls{MDP} formalization of RL. RL tries to discover an optimal policy $\pi^*$ by collecting rollouts $\mathcal{D}:\left\{\tau_{i}\right\} \sim \pi$ from the environment, with $\tau_{i}$ being a trajectory of state-actions pairs $\tuple{s_0, a_0, \cdots, s_T, a_T}$, by maximizing the sum of expected rewards (cfr. \cref{eq:policy-max}). By contrast, inverse RL tries to discover the reward function $\mathcal{R}$ that explains the observed behaviour $\pi$ in demonstrations $\mathcal{D}$. This learned reward function can then be used by the agent to learn policies. Inverse RL is challenging because it is an \emph{underdefined problem}: there are multiple reward functions that can explain example behaviour. It is also difficult to evaluate the learned reward function because it requires training the agent until convergence. Many IRL algorithms assume optimal behaviour in the example demonstrations which is not always the case. Finally, in inverse reinforcement learning, there is an outer loop learning the reward function while the inner loop executes a learning procedure for finding an optimal policy given the current reward function. Recent methods have looked at integrating deep neural networks as a representation layer in inverse reinforcement learning \autocite{Finn2016,Ho2016,Fu2018}. However, due to the two loops taking place, a lot of computational power is required for training. Speeding up the training process with a kinesthetic teach-in and updating instead of optimizing the reward function is explored in \autocite{Finn2016}. Unfortunately, manually moving the end-effector of a robot proves to be unfeasible for difficult tasks like knot tying or folding clothing.
% TODO: hier kunnen we nog IRL algoritmes zelf aan toevoegen zoals: GAIL, VINE Variational Inverse Control with Events, zie ook (https://github.com/justinjfu/inverse_rl)

Ultimately, decoupling the dependency between reward learning and policy learning can speed up the process significantly, which is the topic explored in this work in \cref{ch:reward_functions}. We focus in particular on harnessing the expressiveness of deep neural networks by using their latent space for constructing reward functions. The underlying hypothesis is that if the latent space contains relevant semantics, it can be used to extract reward functions by labelling data, using distance in embedding space or unsupervised learning. Labelling data is explored in \autocite{Singh2019}: they construct a reward function based on an image classifier trained on successful goal states reached by teleoperating the robot towards the end state. A potential way for arriving at semantic-meaningful embedding that can be used for reward extraction is the self-supervised learning paradigm discussed in \cref{subsec:lit_ssl}. Time, in particular, is a useful signal for self-supervision. This idea was pioneered in \autocite{Sermanet2017TCN} in which they introduce \gls{TCN}: generating contrasting examples based on the temporal dimensions in videos. \glspl{TCN} uses multi-perspective video demonstrations as input and time as a supervisory signal. \Glspl{TCN} are demonstrated to learn meaningful semantic embeddings, which can be used for robotic pose imitation of humans. This is done by aligning video frames using nearest neighbours in embedding space. This is problematic in case a certain state machine or trajectory in embedding space has to be followed in order to solve the task. In \autocite{Dwibedi2018mfTCN}, TCNs are trained over multiple input frames such that the network is able to encode the position and velocity of objects in the scene. In \autocite{Hartikainen2019}, time is used as a learned distance function for assigning environment rewards. However, their approach requires human intervention in order to select the desired goal states.  \autocite{Nair2018time} also uses time as a supervisory signal in videos of expert demonstrations to learn an optimal trajectory of states. However, they assume the possibility of visually removing the end-effector from the scene, which is not possible for all tasks. In the cloth folding example, it is not possible to stop and remove the end-effector in mid-air as the cloth would drop.
\autocite{Sermanet2017TCN} and other work \autocite{Nair2018visual} looks at expressing the reward function as the distance in latent space between the current state and the goal state. However, this is not possible when there is a trajectory in latent space that has to be followed in order to execute the task. For example, a cloth folding task can start the same way it ends: some pieces of cloth lie crumbled on the table while others are neatly folded and stacked on top of each other.
Although \glspl{TCN} are shown to be capable of robotic imitation of human poses, there is to the best of our knowledge no work that exclusively distills process monitoring metrics or reward functions from self-supervised representations trained on video demonstrations. % Deze laatste zin is eigenlijk de crux: hoe gebruik je SSL representaties in RL zonder reward functie en ontwijk je de nadelen opgesomd hierboven. Ander werk dat SSL-time gebruikt aanschouwt het geburiken als reward functie er als neveneffect door in embedding space distances te vergelijken.


\section{Datasets for robotic learning} \label{sec:lit_datasets}
The robotics community in general have accepted deep learning to be a powerful tool \autocite{Sunderhauf2018}. This acceptance is evident from the surge in \textit{deep learning} keywords in high-tier robotics conferences such as ICRA, the hosting of robotic application workshops at computer vision and machine learning conferences such as CVPR \autocite{angelova2017computer} and NeurIPS \autocite{Posner2017} and the advent of a dedicated Conference on Robot Learning\footnote{\url{http://www.robot-learning.org}}. However, neural networks are known to be data-hungry due to their high parametrization. Additionally, \gls{RL} requires many interactions with the environment to learn meaningful behaviour. Together with the high cost associated with collecting data on real robotic platforms, there is a need for datasets for robotic learning. This eminent need for high-quality datasets is also apparent in the general deep learning community with the launch of the NeurIPS 2021 Datasets and Benchmarks Track\footnote{\url{https://blog.neurips.cc/2021/04/07/announcing-the-neurips-2021-datasets-and-benchmarks-track/}}. In addition to the availability of data, datasets need to uphold a high quality for successful training, generalizability and to avoid unwanted biases. A method to ensure quality is for example by standardizing the documentation process through datasheets \autocite{gebru2018datasheets}.
\key{The availability of high-quality, real-life datasets enables to train robotic controllers that are unbiased, work on real problems and can be benchmarked.}
The applications in robotic manipulation for datasets lie in applying supervised learning methods to train grasp quality predictors, identify grasping points and object states. Alternatively, offline RL, also known as batch RL and discussed in \cref{subsec:lit_rl}, can be employed to learn policies from gathered state-actions tuples without letting the policy interact with the environment itself.
A successful approach to generate datasets for robotic manipulation is implemented in the Dex-Net \autocite{dexnet2} dataset containing $6.7$ million synthetic robust grasps. Dex-Net offers RGB-D images with candidate grasping pose and grasping outcome of $1.500$ object meshes. Generating synthetic grasping data is also explored by other authors \autocite{depierre2018jacquard,redmon2015real} but has the pitfall of potential transferability issues to the real world. This sim2real problem urges other authors to look at generating data in-vivo. However, human supervision for controlling robots is expensive and biased. Hence, a popular approach is to self-supervise the data collection process: use a heuristic motor control rule to execute grasps. In \autocite{pinto2016supersizing}, a dataset consisting of $50.000$ grasp attempts with a dual-armed robot is collected. Afterwards, a network is trained to predict a good grasping orientation from a given image patch.
In \autocite{Levine2016}, this approach is scaled-up. They generate two datasets containing $800.000$ and $900.000$ grasping attempts on two different clusters of robotic manipulators. They record images from a monocular RGB camera mounted over the shoulder of the arm. They synchronize the images with the delta end-effector pose and grasp the success outcome. Their dataset consists primarily of rigid objects, although some slightly deformable objects such as rubber ducks are present. This dataset is then employed for training a grasp quality predictor neural network.
Robonet \autocite{dasari2019robonet} is a follow-up initiative that merges rollouts of 7 different robots from 4 different institutions in order to obtain 15 million frames containing variability across viewpoints, objects, robots and lighting conditions.
Other researchers \autocite{mandlekar2018roboturk} argue that self-supervised data collection is inefficient and prone to errors. This critique has led to RoboTurk \autocite{mandlekar2018roboturk}, an online platform that allows remotely operating robots for manipulation tasks. They collect $111$ hours of RGB-D data matched with joint and effector sensor readings from $54$ different robot operators for different tasks. One of the tasks consists of unfolding garments on a table and have been shown to be useful for training self-supervised embeddings.

Compared to rigid objects, the deformable object domain is less endowed with datasets. The largest cloth datasets are mainly constructed for automating retail applications such as clothing category identification, fashion landmark detection, image retrieval and recommendations systems. The first annotated cloth dataset is Fashion-MNIST \autocite{fashionmnist} which is a drop-in replacement for the MNIST dataset, often used for benchmarking ML algorithms. Fashion-MNIST contains $70.000$ grayscale images of size $28\times28$ of fashion products from 10 categories. A modern variant of Fashion-MNIST is the DeepFashion \autocite{DeepFashion} dataset, which contains $800.000$ images scraped from the Google image search engine and from shopping websites. The dataset contains $8$ different landmarks (i.e.\ collar, sleeve, waistline, and hem for each side), $46$ clothing categories and $1.000$ clothing attributes such as fabric material and fashion shape. This dataset is extended in \autocite{DeepFashion2} which provides more images, richer annotations and multiple clothing per frame to improve real-world realism. To reduce the noise introduced by scraping data, the FashionAI \autocite{FashionAI} dataset focuses on improving the automated annotation process by providing high-quality labels and a dedicated network architecture that assign attributes in a tree-like way. The attributes represent fashion semantics that are disassembled into hierarchical concepts. For example, a round collar bishop top is disentangled into "$\text{top look} \rightarrow \text{sleeve region} \rightarrow \text{style} \rightarrow \text{cuff} \rightarrow \text{bishop}$" and  "$\text{top look} \rightarrow \text{collar} \rightarrow \text{round}$". The neural network training uses a DAgger-like approach from behavioural cloning in which the network requests expert labels for cases in which it is uncertain. They collect $357.000$ images of $6$ categories of women's clothing and $245$ high-quality hierarchical structured annotations.

In contrast to annotated cloth data for retail applications, datasets for robotic learning of manipulation skills for clothing and deformable objects are not widely available. While a large body of research exists on cloth modelling and garment reconstruction from images \autocite{bertiche2020cloth3d,deepfashion3d,Wang20183dgarment}, the availability of dedicated cloth folding data is scarce. To the best of our knowledge, the sole example containing a small set of deformable objects manipulations with a robot is in \autocite{mandlekar2018roboturk}. Other work concerning datasets for learning to manipulate cloth or other deformable object generate data in simulation, which is discussed in the following \cref{sec:lit_simulation}.

\section{Simulation environments to accelerate learning} \label{sec:lit_simulation}

Physics simulators are a crucial tool for robotics researchers. The community often first tests hypotheses and methodologies in simulation and optionally transfers results to the real world. The main reason for using virtual platforms is their low cost, reproducibility and availability compared to real robots. Simulations can run faster than real-time, do not need an active operator and can provide the large dataset requirement for deep learning algorithms.
\keyWithTitle{Simulation environments}{Simulation environments are a virtual counterpart of the real physical environment that enables fast and save experimentation of robotic experiments but require transferring to the real world.}

While a wide array of physics simulators can model and simulate a diverse set of phenomena, it is important for robotic researchers to have access to this physics engine within a robotic simulator. This requires the engine to support common robotic tools such as ROS integration for transfer to a real robotic platform, inverse kinematics, URDF import and models for joints, actuators and sensors. Another important but often forgotten feature is the ability for headless rendering. Running the simulation without having a physical display attached allows to use computation farms that often come without display. Common robotic simulators \autocite{Collins2021} such as Gazebo \autocite{gazebo}, MuJoCo \autocite{mujoco} and PyBullet \autocite{pybullet} support robotic object manipulation tasks, however the deformable object simulation functionality is limited.

The limited support for cloth simulation in existing robotics simulators has led to many researchers implementing custom cloth simulators. \textcite{Matas2018} for example extend the functionality of PyBullet to train a robot agent to fold cloth completely in simulation. DeformableRavens \autocite{seita2021learning} and DEDO \autocite{dedo} are other soft body simulation implemented in the PyBullet physics engine that allows simulating robotic interaction with ropes, fabrics and bags. Unfortunately, the visual and physical fidelity is significantly lower compared to other synthetically generated cloth data. SoftGym \autocite{softgym} provides simulated benchmarks for fluid, cloth and rope simulation. It uses Nvidia FleX particle system simulation, which is hardware-accelerated on \gls{GPU}. Because SoftGym runs on the \gls{GPU}, it allows doing more calculations in parallel, leading to physically and visually more realistic cloth behaviour. However, the authors anticipate fundamental challenges when transferring policies trained on SoftGym to the real world. Finally, other relevant work examines to make the simulation itself differentiable. Differentiable physics simulation is a powerful technique that applies gradient-based methods to simulating physical systems. This way, it enables gradient-based optimization for control.
% \todo{zorg dat dit uitgelegd bij engineered approaches sectie. Het mag niet overkomen alsof dit iets nieuws is. Tradtionele controle heeft dit ook!}
Backpropagating gradients through a neural network controller and through the physical system has been shown to speed up the learning process for robot control tasks \autocite{Degrave2019}. Extending this work to soft body simulations has been explored in \autocite{liang2019differentiable,huang2021plasticinelab} and have shown that gradient-based optimization method outperforms RL but fails on multi-stage tasks that require long-term planning.

\subsection{Deformable object simulation methods} \label{subsec:lit_cloth_sim}

Forces applied to a deformable object both move the object and change the shape making high-fidelity modelling more difficult and computational expensive compared to rigid object simulation. Deformable bodies require reasoning about the shape, dynamics and material properties of the object. Techniques in modelling cloth-like behaviour fall on a spectrum based on the computational budget allocated to the simulation. Offline simulation is on one side of the spectrum that prioritizes visual quality by means of physical realism. A prominent example can be found in the movie industry where \gls{GPU} farms render multiple days for a two-hour video clip. In contrast, real-time systems are interactive and require running at a fixed frame rate. An important application of real-time simulation systems is found in video games. Games typically run between \SI{30}{\hertz} and \SI{60}{\hertz} leaving \SI{15}{\milli\second} and \SI{30}{\milli\second} computational budget per frame. Subtracting the time needed for core game features such as handling user input, game logic and \gls{AI} leaves only a few milliseconds remaining for physical simulation. Unfortunately, reducing the resolution of the simulated object often leads to unsatisfying results \autocite{muller2008real}. This is why real-time methods focus on reproducing the visual properties of physical processes. These constraints have driven the game industry to develop methods outside the offline physical simulation domain.

Physical simulation of deformable objects for a robotic learning environment requires characteristics of both real-time and offline simulation. For learning in simulation, we need physical realism for transfer to the real world and also needs to share resources for learning purposes and robot simulation. Offline cloth simulations are notoriously slow. For example ArcSim \autocite{narain2012adaptive}, a realistic cloth simulator using \gls{FEM}, requires \SI{50}{\second} to render a single frame of a fully dressed human character. Using these expensive simulations in a robotic learning environment would consume the computational budget, leaving little time for learning. To understand the rationale involved in selecting an appropriate simulator, we discuss the two major steps for implementing a deformable object simulation.

A first step in modelling deformable objects is choosing a shape representation. Choosing a representation changes the flexibility of the model and impacts the modelling of the dynamics. There are three major approaches for representing the shape of a deformable object:
\begin{enumerate}
	\item First, implicit curves and surfaces can be used as a representation. These shapes are defined by an implicit equation $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ with $f(\vec{p}) = 0$ for which all points are on the surface. Implicit equation representations are primarily used in medical imaging in which level set methods are used for tracking deformable objects \autocite{Cremers2006}.
	\item Parametric curves and surfaces, the second option for shape representation, are shapes controlled by a limited set of parameters. A 3D parametric surface is generated by a set of functions $\vec{q}: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}$ with all cartesian points evaluated directly from their functional expression\\$\vec{q}(u, v) = \{x(u, v), y(u, v), z(u, v)\}^{T}$ where $u$ and $v$ are parameters. Splines and extensions such as B-splines and NURBS are parametric surfaces that can represent any type of deformable object. They allow a compact representation defined by control points that can move and deform the surface. However, depending on the spline parametrization, moving control points can lead to erroneous deformations if for example the resulting curves do not lie on the convex hull of the control points. Learning in such a non-linear dynamical environment is difficult and a general solution for this is to date not available \autocite{rios2020}.
	\item A third possible shape representation is a mesh. A mesh stores the topology and geometry of an object represented by connected vertices. Meshes are the most used shape representation for a deformable object in simulation.
\end{enumerate}

The second step in simulating deformable objects is choosing a dynamics model that will calculate how the chosen shape representation will deform on interaction with forces exerted on the object. In robotics, the important characteristics influencing the choice of dynamics model are computational cost, physical accuracy, visual fidelity and ease of use. In the following paragraphs, we consider two main approaches: \gls{FEM} and particle-based methods.

The \gls{FEM} is a widely used method for offline simulation of solid objects. \Gls{FEM} incorporates real physical material properties making simulations fairly realistic. This method generally works by reducing the general partial differential equations that describe the physical reality to systems of algebraic equations. Because these equations are often non-linear, solving the system in real-time becomes non-trivial. A way to speed up the solver is by linearizing the equations. This works for cases where deformations are small such as the analysis of buildings. However, the artefacts caused by linear approximations become significant in the case of cloth. This is why in real-time applications, particle-based methods are often desired above \gls{FEM}. Particle simulations consist of atomic masses called particles that undergo forces such as gravity or a robot end-effector grasp. These forces drive particles to new positions solved by using Newton's second law of motion and a time integration scheme to solve the resulting differential equations. In the case of cloth, the particles are connected with their second-order neighbours and exert forces on each other in order to preserve the shape. These forces are modelled as springs, giving rise to a mass-spring system of particles. Compared to \gls{FEM}, particle systems are faster to simulate and easier to implement. However, they are harder to tune and can be plagued by instabilities. In particular, choosing the right spring constant for the springs is notoriously difficult. Nonetheless, a spring-mass system approach for cloth simulation is a favourable approach as it conveniently handles the two-dimensional structure of cloth. This finding is also apparent in the robotic learning literature: modern research using cloth simulations implements a particle-based approach \autocite{Matas2018,seita2021learning,dedo,softgym}. An exception is \autocite{liang2019differentiable} that utilizes \gls{FEM}. We formally describe and implement a complete particle-based cloth simulation in \cref{ch:simulation}.

\subsection{Transferring simulation results to the real world}  \label{sec:lit_sim2real}

Simulation-based training methods allow for fast and safe experimentation for robotic tasks. However, differences between simulated physics and real-world physics exist due to undermodelling and errors in system parameter identification. This \emph{reality gap} causes policies trained in simulation to suffer performance when deployed on the real platform. The resulting issue is labelled as the \emph{Sim2Real} problem in robotics.
\keyWithTitle{Sim2Real problem}{The Sim2Real problem deals with overcoming undermodelling and system parameter identification errors in order to transfer policies trained in simulation to the real world.}
% Sim2Real is recognized as an important field of research given the risk of \emph{Sim2Null} \autocite{hfer2020perspectives}: simulation results undeployable in the real world because of the low entry barrier of current simulation environments such as OpenAI Gym \autocite{brockman2016openai}. 

The reality gap for training robotic controllers in simulation with deep RL became apparent in \autocite{Zhang2015}, although the problem is not new. In the field of control theory, calibrating mathematical models of physical systems has been common practice \autocite{ljung1983theory}. This approach is known as \emph{system identification} and consists of tuning the simulation parameters such that the simulated behaviour is as close as possible to the real behaviour. Control practitioners have calibrated models using local search techniques that search for the optimum by using a gradient-following technique \autocite{ljung1983theory}. Later, genetic algorithms have been shown to be competitive to deal with non-differentiable and non-linear search spaces \autocite{Kristinsson1992}. Modern versions of system identification use machine learning methods \autocite{chebotar2019closing} and differentiable physic simulation \autocite{heiden2021neuralsim} to learn complex dynamics, such as friction and contact parameters, from real data. A notable example in the deformable object manipulation domain is the early work of \textcite{Howard2000} that deals with the system identification. In their work, a mass-spring-damper simulation of a deformable object is tuned to the real world by probing the object with the gripper containing a force sensor.

Another view on the reality gap is that it is an instance of a domain adaptation problem in which the distribution of the data of the source domain (i.e. the domain in which training is taking place) is being transformed to resemble the distribution of the data of the target domain (i.e. the test-time domain). There are three approaches to domain adaptation \autocite{Zhao2020}. (1) Discrepancy-based adaptation which measures feature distance between source and target domain and aligns their representation vector. (2) Adversarial-based adaptation that uses \glspl{GAN} as domain-invariant feature representation for training downstream neural network policies. (3) Reconstruction-based adaptation which learns domain-invariant features by having a reconstruction task.
In deep RL for robotics, domain adaptation is used to improve the data efficiency of the work of \autocite{Levine2016} by introducing synthetic data. The synthetic and real data are used to train a \gls{GAN} to transform synthetic images to real images \autocite{bousmalis2018using}. These pseudo-real images reduce some of the reality gaps but operate at the pixel level and risk changing the semantics of the scene. Future work \autocite{rao2020rl} alleviates this issue by introducing a cycle-consistency loss which encourages the preservation of semantics when inverse transforming the adapted image. Another approach is to learn a joint feature representation for source and target domain using \glspl{GAN}.\Textcite{james2019sim} for example, train a generator that transforms both simulated, randomized images and real-world images to a canonical representation which is used as a feature vector for policy training of a robotic grasping task.

Another category of Sim2Real methods is domain randomization. Domain randomization introduces variability in the elements constituting the simulation environment \autocite{tobin2017domain}. The hypothesis underlying domain randomization is that if the agent learns a policy successful in multiple variations of the same environment, then it should transfer to the real world which is just another permutation of all the variants seen during training. More formally, the goal of domain randomization is to find a parametrized policy $\pi_{\theta}$ that maximizes the expected reward over a distribution of simulation models $\rho_{\xi}$:
\begin{equation}
	\theta^{*}=\argmax_{\theta} \mathbb{E}_{\xi \sim \rho_{\xi}}\left[\mathbb{E}_{\tau \sim p(\tau | \pi_{\theta}, \xi)}[R(\tau)]\right],
\end{equation}
where $\xi$ represents a simulation configuration and $p(\tau | \pi_{\theta}, \xi)$ the likelihood of trajectory $\tau$ under policy $\pi_{\theta}$ and simulation configuration $\xi$. Simulation parameters can be categorized into visual randomizations and physical randomizations. Examples of simulation parameters are given in \cref{table:simulation_parameters_example}.
Domain randomization has been successfully applied to transfer policies trained in simulation in a zero-shot manner to the real world \autocite{tobin2017domain, Peng2018, openai2019solving}. However, domain randomization is known to require massive amounts of computational power. An alternative is guided domain randomization methods which uses domain randomization schedules, curricula \autocite{raparthy2020generating}, real-world calibration \autocite{chebotar2019closing} or finds useful and informative randomizations \autocite{adr2019}. Domain randomization is a popular approach for learning deformable object manipulation. For example, \autocite{Matas2018, Wu2020} transfer cloth manipulation tasks trained in simulation to the real world using domain randomization.

\begin{table}[htb]
	\centering
	\caption{Non-exhaustive list of simulation parameters to vary in domain randomization.}
	\begin{tabular}[t]{c c}
		\toprule
		Category & Simulation parameters                          \\
		\midrule
		Visual   & Object color, shape and position               \\
		Visual   & Robot color, and position                      \\
		Visual   & Material texture                               \\
		Visual   & Lighting conditions                            \\
		Visual   & Camera pose                                    \\
		Physical & Object mass and friction                       \\
		Physical & Robot morphology, mass and joint friction      \\
		Physical & Damping of physics integrator                  \\
		Physical & Latency between action selection and execution \\
		Physical & Observation noise                              \\
		\bottomrule
	\end{tabular}
	\label{table:simulation_parameters_example}
\end{table}

Finally, meta-learning can be considered as a Sim2Real method that takes the opposite approach to domain randomization. Instead of learning robust policies across a distribution of simulation environments, meta-learning learns a policy that is able to adapt rapidly to a changing environment and task. Domain randomization can then provide a task distribution for meta-learning.

\section{Deformable object state perception through instrumentation} \label{sec:lit_instrumentation}

A typical approach for the robotic folding of textile relies on the use of vision in order to detect grasping points and to perform texture segmentation and pose estimation \autocite{Maitin2010, Doumanoglou2016, Bersch2011}, as well as to estimate the state \autocite{Matas2018} and---in the case of reinforcement learning-based approaches---the reward function \autocite{Tsurumine2019}. However, while~vision is instrumental for recognizing and localizing objects, touch and force measurements become important once contact occurs, and the object is explored using the end-effector \autocite{Billard2019}. Recent~work \autocite{Tian2019} has illustrated this idea by applying a touch-based control method for a ball repositioning task, rolling dice and the deflection of a stick. In line with other authors \autocite{Tian2019, Lee2019}, we believe the use of tactile input and its fusion with other sensory information is crucial to learn complex robot manipulation tasks in an efficient way. We distinguish three functional locations where sensors can be added: instrumentation on the robot, on the target object and in the environment. We discuss these applications in the robotic manipulation context.

\keyWithTitle{Instrumentation}{Instrumentation is the process of applying sensors in the learning environment in order to accelerate learning.}

% instrument the ROBOT
A first location to instrument is on the robot itself.
%% Vision-only on robot
Vision-only sensors is the canonical approach to robotic manipulation task given the popularity of visual servoing. \textcite{Lin2015} for example, pick up deformable objects by using a 3D laser scanner that discretizes objects into a tetrahedral mesh. In \autocite{Navarro-Alarcon2016}, the shape of a volumetric deformable is adapted by quantifying the deformations using visual input.
%%% Tactile-only on robot
Tactile-only sensing has also been employed in, for example, \autocite{Drimus2014} that equipped a parallel gripper with a tactile array sensor to classify rigid and deformable objects. An example exploded view of an implementation is shown in \cref{fig:instrumentation_example}. \textcite{Kappassov2015} stresses the importance of having high-resolution tactile sensing in order to detect small variations in the object's shape. Similarly, \textcite{Naveen2020soft} pioneer soft-bubble grippers: parallel air-filled finger membranes whose deformability allow both compliant grasping and sensing. The instrumentation of the finger with internal markers enable visuotactile sensing by tracking the optical flow pattern of the membrane deformation. They demonstrate how tactile information can be used for material and object classification. In \autocite{Naveen2020fast}, the soft-bubble gripper technology is used to estimate the pose of an object is by developing a contact patch tracking model. \Textcite{Yuen2017} develop sensory fabric sleeves that can be attached to end-effector joints for determining joint angles and end-effector position. \autocite{Case2019, case2018state} implement a robotic skin containing sensors and actuators using stretchable, flexible substrates. The sensory information of the robotic skin enables estimating physical properties such as the state and stiffness of the objects, which can be used for motor control tasks such a controlling the segments of a finger.
%% Multi-modal on robot 
Fusing multiple heterogenous sensor sources for deformable object manipulation is proposed in \autocite{khalil2010integrated}. This setup uses binary tactile pads and a camera for contour tracking of deformable objects. \Textcite{Frank2010} estimate physical properties for simulating deformable objects using multi-perspective visual and tactile information. Multiple modalities can also be incorporated in the learning frameworks discussed in \cref{sec:lit_sl}. For a survey on using deep neural networks for multimodal learning, we refer to \autocite{ramachandram2017deep}. In the context of object manipulation, \autocite{lee2020making} learn a self-supervised representation of pixels, force sensing and proprioception for peg insertion task. Each input modality is processed through a dedicated architecture; a \gls{CNN} for the pixel input, a \gls{CNN} with causal convolutions similar to WaveNet \autocite{WaveNet} and a fully connected neural network for proprioception and sensor readings. Each of these architectures produces a feature vector which is merged into a single feature vector using a Product of Experts \autocite{hinton2002training}. Their ablation study reveals that all modalities are used as the system performance drops significantly when removing input streams. Similar approaches and conclusions have been found in \autocite{Calandra2018, Droniou2015, balakuntala2021learning}. \Textcite{zambelli2016online} adds a microphone in addition to cameras and tactile cells for a humanoid robot to play the piano. They find that multimodal sensing is crucial for a multimodal task such as piano playing.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/velostat-sensor.jpg}
	\caption[Example of instrumenting the robot.]{Example of instrumenting a robot's finger with tactile sensors. A grid-like sensor array is attached to the finger and protected by silicone pads.}
	\label{fig:instrumentation_example}
\end{figure}

% instrument the OBJECT
A second location where instrumentation is applied is on the target object to manipulate itself. A prominent example is the instrumented Rubik's cube in \autocite{openai2019solving}. To avoid letting the robot learn to infer the Rubik's cube state from the camera, they equip the cube with rotary encoders to track face rotations. In the deformable object manipulation domain, \Textcite{Bersch2011, Elbrechter2012} apply fiducial markers on cloth and paper for state estimation. However, visual markers suffer from occlusion and make it hard to generalize for other clothing articles without markers. Smart textiles, i.e.\ textiles that are able to sense stimuli from the environment and react accordingly \autocite{schnee17}, are being used in numerous applications such as health monitoring \autocite{Cochrane2019}, sports \autocite{Presti2019} and robotics \autocite{Yuen2017}. An example of textile with integrated electronics is shown in \cref{fig:smart_textile_example}. Textile able to infer its own state is of particular interest for learning to manipulate clothing articles. To the best of our knowledge, there is no work of physical smart textiles for learning deformable object manipulation with robots.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{figures/smart-textile-polymer}
	\caption[Example of a smart textile implementation.]{Example of a smart textile implementation using a piezoresistive polymer attached inside the fabric.}
	\label{fig:smart_textile_example}
\end{figure}

% instrument the ENVIRONMENT
A final entity to instrument are elements found in the environment. For example, a table can be equipped with force sensors that provide the location of objects to a robot. A concrete example can be found in \autocite{Kimura2013} who place a scale on the table with objects that have to be recognized and add the weight as an input modality for control tasks.

\section{Conclusion}

% geen meta inzichten
% de inhoud hiervan condenseren en een brug maken van h1 RO's naar rest van het boek (het werk dat zelf  gedaan is). 
% 	Data collection 	
%   Simulation
% 	Reward learning 
% 		SSL
% 	Instrumentation 
% 	

This chapter has reviewed, in a tutorial and survey style, the transition from traditional control pipelines for deformable object manipulation to leveraging learning methods. We have discussed methodologies, tools and approaches to building a cloth folding pipeline while identifying strengths, weaknesses and gaps.
One of those gaps is the state estimation of cloth in the modern deep learning era. We found that the majority of work employ vision for estimating the state of clothing articles. In our research, we step away from this vast focus on vision-only sensing and integrate tactile sensors in order to create a smart cloth that can tell a robot its state.
We stressed the importance of simulation environments for robotic learning and the importance of realistic cloth simulation. We will examine a basic implementation and discuss its usefulness, Sim2Real pitfalls and application for learning to fold clothing.
We highlighted the potential for example-based learning and learning from interaction using expressive representation methods in deep learning. In this research, we combine both approaches to exploit their strengths: we use human demonstrations as examples for distilling task intent while letting the robot interact with the environment. To avoid expensive data labelling, we will introduce a self-supervised approach.
We will start introducing our research in the next chapter that explores the use of simulation for robotic manipulation of deformable objects.
% We will start introducing our research in the next chapter that deals with addressing the problem that the prerequisite for learning is unavailable: gathering a high-quality, high-volume dataset of folding demonstrations. 

% \printbibliography

\end{document}