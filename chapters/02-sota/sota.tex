% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode


% TODO: ik heb een goed idee waar het naar toe moet hoor, tis gewoon de uitvoering en flow dat moet kloppen. Ik ben van plan het eerst eens uit te werken voor een bepaald onderdeel van het literatuur hoofdstuk, te reviewen met jou en dan die stijl\flow\template toepassen voor de andere secties in dat hoofdstuk

\providecommand{\home}{../..}
\documentclass[\home/main.tex]{subfiles}

\begin{document}

\chapter{Background and review of related work} \label{ch:lit}

The goal of the following chapter is to provide the preliminaries and a review of relevant work in the field of robotic manipulation of deformable objects. To provide some historical context, we first discuss how \emph{standard robotic manipulation pipelines} can be used for manipulating deformable objects in \cref{sec:lit_traditional}. Next, in \cref{sec:lit_learning}, we introduce how the inherent limitations of engineered motor control architectures can be overcome by using \emph{learning-based methods}. We break this section down into subsections introducing supervised learning, deep neural networks and reinforcement learning. This is followed by surveying their applications in recent robotic manipulation work. Given the general property that learning-based methods are data-hungry, we continue this discussion by reviewing the role of \emph{large datasets} for robotic learning in \cref{sec:lit_datasets}. An alternative approach to generating data is to use synthetic data. To this end, we discuss the role of \emph{simulation} and the corresponding transferability problems in \cref{sec:lit_simulation}. Critical to robotic learning of manipulation skills is some metric of task success, generally labelled as reward function. The role and methods to obtain \emph{reward functions} for robotic learning, and deformable objects manipulation in particular, is reviewed in \cref{sec:lit_reward_learning}. Finally, we discuss the idea and corresponding literature of \emph{instrumenting the process with sensors} to facilitate the learning process in the manipulation environment in \cref{sec:lit_instrumentation}.

\input{\home/chapters/02-sota/a-manipulating-deform-objects}

\section{Learning-based approaches to robotic manipulation} \label{sec:lit_learning}
% Houvasten: Matthias, Jim, Gabriel 

Machine learning, a domain of artificial intelligence, is the study of algorithms that give computers the ability to learn from and make predictions based on data. For robotics, learning provides a way to deal with the inherent systematic as well as random errors in robotic systems and variability in unstructured environments. This is because in learning, you optimize for the grasping task, which implicitly adapts the behaviour to imperfections in the system such as inaccurate sensor readings.

In this section, we provide a short review on the fundamentals of relevant machine learning techniques. We aim to provide background material on \acrfull{DRL}, i.e. the method used in this thesis. To do this, we introduce supervised learning methods (\cref{subsec:lit_sl}), deep neural networks (\cref{subsec:lit_dnn}) and reinforcement learning (\cref{subsec:lit_rl}). We discuss their relevant applications in robotic manipulation with a focus on the manipulation of deformable objects. %For a comprehensive view on the machine learning field, the reader is referred to~\autocite{Hastie2001} and~\autocite{Bishop2006}.

\subsection{Supervised learning} \label{subsec:lit_sl}

\newcommand*{\prob}{\mathrm{P}}

Supervised learning is a machine learning paradigm that operates under the setting where there is a set of \textit{input} variables, for example image pixels, that exert influence over other \textit{output} variables, for example whether there is shirt or trouser in the image.
The components building up a machine learning system are the dataset, the model, loss function and optimization algorithm. 
More formally, we can denote the input data as a set $\mathcal{X}$ consisting of vector $x^{(i)} \in \mathcal{X} $ with the superscript $i$ referring to the $i$th observation. In the machine learning domain, this set of predictor variables is called \textit{features}. The set $\mathcal{Y}$ contains the output variables $y^{(i)} \in \mathcal{Y}$. Concatenating tuples of 
$\left\{\left(x^{(i)}, y^{(i)}\right) , i \in 1,\dots,N \right\}$
, often called \textit{examples}, leads to a dataset which can be used for learning. Central in this learning procedure is the idea of \textit{function approximation} in which a function $f$, parametrized by $\theta$, maps an input $x^{(i)}$ to its corresponding output $y^{(i)}$:

\begin{equation*}
	f(x;\theta): \mathcal{X} \mapsto \mathcal{Y}.
\end{equation*}

This mapping, also called \textit{model} or \textit{hypothesis}, comes in many forms such as linear models, tree-based methods, support vector machines and neural networks\footnote{We refer to~\textcite{Murphy2012, Bishop2006, Hastie2001} for a thorough exposition on traditional supervised learning methods.}.
The goal of the learning procedure then becomes to adjust the parameters $\theta$ of the model such that a certain performance measure $\mathcal{P}$ is optimized. This metric, called \textit{loss function} $\mathcal{L}$ in machine learning jargon, is specific to the task and domain in which the learning is taking place. In robotic folding for example, the robot might be presented with a candidate grasping pose $\mathbf{u}$. The robot than has to predict the probability $\hat{y} = Q_{\theta}(\mathbf{u}, \mathbf{x}) = \mathbb{E}\left[ \mathcal{S} | \mathbf{u}, \mathbf{x} \right]$ of successfully grasping (success denoted with $S$) a shirt, given some input image $\mathbf{x}$\footnote{This example also implies a heavy assumption; the availability of a dataset containing tuples of (grasping pose, object configuration, probability of success).}.
In this situation, one could minimize the negative cross-entropy loss:
\begin{equation*}
	\mathcal{L}=-y^{(i)} \cdot \log \hat{y}^{(i)} +\left(1-y^{(i)}\right) \cdot \log \left(1-\hat{y}^{(i)}\right),
\end{equation*} with $\hat{y}^{(i)} = f(x;\theta)$ being the predicted output of the model for observation $i$.
The optimization problem then becomes to adjust the parameters $\theta$ of the model $f$ using the examples $\left(x^{(i)}, y^{(i)}\right)$:
\begin{equation*}
	\theta^{*}=\underset{\theta}{\argmin} \: \mathbb{E}_{p(S, \mathbf{u}, \mathbf{x})}\left[\mathcal{L}\left(S, Q_{\theta}(\mathbf{u})\right)\right].
\end{equation*} 
The dominant way for heavy parametrized functions such as neural networks to optimize this objective, is to use gradient descent. The gradient expresses the direction of the steepest decrease of the loss function $\mathcal{L}$ with respect to the model parameters $\theta$. By iteratively updating the parameters in the opposite direction of the gradient, in the case of a minimization objective, we gradually arrive at a local or global minimum:
\begin{equation*}
	\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} \mathcal{L}(\theta).
\end{equation*}
$\alpha$ determines how large steps we take towards the estimated direction of the closest local minimum. Adaptive methods such as Adam~\autocite{Kingma2014} allows taking variable step sizes per variable based on the historical directions of the gradient. 

Supervised learning is an important paradigm for robotic learning because labelled data provides a clear learning signal. This is important because time spent on robots is expensive. Contrarily, the learning signal in reinforcement learning(\cref{subsec:lit_rl}) often does not optimize for direct task performance and might lead to expensive learning times on physical robot platforms. In the following paragraphs, we discuss relevant work in applying machine learning methods for solving robotic manipulation tasks. Generally speaking, there are two main strategies for applying supervised learning in the robotic manipulation pipeline: (1) as perception module or (2) to map states to actions using imitation learning. We postpone the discussion of the application of deep neural networks in robotics after introducing neural networks in~\cref{subsec:lit_dnn}.

Traditionally, supervised learning methods are leveraged in the perception module of a robotic manipulation pipeline. In the domain of deformable object manipulation, ~\textcite{Ramisa2012} searches for quality grasping points of crumbled cloth in order to maximize the unfolding upon lifting. They do this by first labeling a dataset of shirts with bounding boxes containing the suitable grasping points. Next, they train a logistic regression model to obtain the probability of the desired grasping point in a given bounding box using a bag of features from the input image. By employing the logistic classifier in a sliding window over the image, they give image patches containing local peaks to an \acrshort{SVM} to obtain more accurate grasping candidates. The candidate patch is converted to 3D space by working in a calibrated environment, and motion planning is executed using inverse kinematics. Similarly,~\textcite{Wang2011} folds socks by having a perception system that uses manually engineered features for training an \acrshort{SVM} with Gaussian kernel to determine the type of sock in front of the robot. The deformable nature of cloth leads to self-occlusions making the garment type and pose classification ambiguous. The presence of this hidden state is explicitly modelled using a \acrshort{HMM} in~\autocite{Cusumano2011}. \acrshort{SVM} have also been used for the purpose to identify garment category and pose~\autocite{Li2014, li2014volum}. Learning methods are also being used to find regions of interests on the cloth. For example,~\textcite{Doumanoglou2016} use random forests to learn garment-specific grasping point. In~\autocite{Maitin2010}, RANSAC~\autocite{RANSAC} is used to find the corners of the cloth. These corners are good candidate grasping points for unfolding and identifying the type of cloth. Nearest neighbors have been used to identify wrinkled regions in a washcloth in order to flatten it~\autocite{Willimon2011}. 


Another strategy to train controllers using supervised learning, is imitation learning. In imitation learning, a sequence of states and actions, as executed by a demonstrator, is recorded as dataset for a supervised learning algorithm. An in-depth view of the field of imitation learning is given in~\autocite{Argall2009}. In the case of robotic laundry,~\textcite{Jia2019} learns single robotic laundry tasks from imitation: flattening, folding and twisting. In contrast to the large bulk of robotic controllers trained in \citeyear{Jia2019} using neural networks, they represent the controller using random forests~\autocite{Breiman2001}. The rationale is given by the non-parametric nature of random forests to dynamically change the number of leaf nodes based on the given imitation data and new cloth configurations. Example demonstrations are also used in deformable object manipulation to tie knots. ~\textcite{Schulman2016learning} uses example demonstrations for non-rigid warping~\autocite{Chui2003} based on point-cloud registration of the scene to tie knots with a robotic manipulator. The general idea is to warp the demonstrated trajectory to match the current setting, which may vary in initial conditions and knot geometry. Closely related is the work in~\autocite{Morita2003} where examples and solutions strategies from knot theory is embedded to do motor control. Although imitation learning is a viable alternative for learning to manipulate objects, a general problem plaguing imitation learning methods is generalizing to unseen scenarios. In the case of trajectory execution, errors can accumulate drastically leading to task failures. This is why existing methods apply data augmentation, include teacher advice or use reinforcement learning(~\cref{subsec:lit_rl}).
		
\subsection{Unsupervised learning}

Unsure whether to really discuss this.  It fits the general flow but is also overkill. On the other hand, it is used extensively in our TCN paper. 

Structure:
\begin{easylist}[itemize]
	& Definition
	& Relevance~:
	&& dimensionality reduction: SIFT, HOW, PCA, T-sne, UMAP 
	&& clustering 

	& Application in learning (very short): input features for learning algo, visualization of embeddings, separation of cloth, ...
\end{easylist}

% Zie ook literatuur van Jia2019 Cloth Manipulation Using Random-Forest-Based Imitation Learning

\subsection{Neural networks} \label{subsec:lit_dnn}
Structure: 
\begin{easylist}
	& Motivation: representation matters
	& ANN, MLP basics: neurons, activations, nonlinearities, layers, feedforward, recurrent --> ik zou dit kort houden! 
	& optimization: backpropagation 
	& CNNS
	& Increasingly abstract representation through deep layers 
		&& motivatie voor perceptie module en motoriek module (cfr levine)
	& Applications in robotics:
		&& perceptual module in pipelines
		&& dexnet, google levine paper
		&& cfr review deep learning in robotics 
\end{easylist}

Comprehensive review on~\acrshortpl{DNN}, see the textbook of~\textcite{Goodfellow2016}.

--------------
EARLY WORK
--------------
The earliest work using supervised learning with neural networks for deformable object manipulation is~\textcite{Howard2000}. In their work, they train a small feedforward neural network that learns the required minimum grasping force for lifting a deformable object. They collect the data by iteratively using more lifting force on objects with certain masses, deformability and damping. A similar approach is proposed in~\autocite{Khalil2007} to fuse tactile and vision data in order to learn physical parameters of the deforamble object model. In~\autocite{Foresti2004}, fur tails are grasped from a conveyor belt. To segment the different furs present in the image, they train self-organizing maps~\autocite{Kohonen1982} in which neurons compete to be activated by the input signal. This results in disconnected regions of interest that are joined using skeletonization. Finally, a heuristic is used to determine and grasp the largest fur. 


%  YOU ARE HERE
% TODO: #CONTINUE HERE PLZ. 
% TODO: papers dat openstaan: Sommige hiervan moeten beschrevne worden in SL stuk. 
%   (Saxena et al., 2008
%  	Dexterous Robotic Manipulation of Deformable Objects with Multi-Sensory Feedback - a Review -> zoek achter "learn"



A successful approach to training deep neural networks in a supervised setting for robotic manipulation is to use \acrshortpl{CNN} as grasp success predictor.~\textcite{Levine2016} train a \acrshort{CNN} on a large dataset of $800.000$ grasping attempts to learn to predict the grasp success probability of a grasping pose, given an input image. To sample candidate grasping points, they employ CEM~\autocite{CEM}. Dex-Net~\autocite{dexnet2} also trains a \acrshort{CNN} to predict the quality of a grasping candidate. This network is trained using a simulated dataset where objects are put into randomized poses on a plane. They use simulation to evaluate different grasping wrenches using analytic grasp metrics. Their model shows impressive generalizability to the real world, on different models not seen during training. The Dex-Net framework has been extended to work with suction grippers~\autocite{dexnet3}, use dual-armed robots~\autocite{dexnet4} and generate grasping candidates in the network~\autocite{Satish2019}.



% Overgang: rational voor RL
Pinto and Gupta (2016) argue that framing grasp detection as a supervised learning problem and training it with hand-labelled or generated data has some issues. The first issue is the human bias towards preferring grasps poses that are similar to the way a person would grasp an object. This discourages exploration of unconventional grasp configurations. Another issue is the cost to exhaustively evaluate all possible grasps because an object can be grasped in multiple ways. Therefore, learning on robots requires a method that can work without human supervision.

\subsection{Reinforcement learning} \label{subsec:lit_rl}
contrast to SL: RL is much harder because (1) delayed rewards (credit assign problem) (2) non-stat data, (3) 
%(also check suttonbarto and thesis NG https://web.archive.org/web/20141222084445/http://www.cs.ubc.ca/~nando/550-2006/handouts/andrew-ng.pdf)
why relevant for robotics
MDP formalisatie
from bellman equation to q-learning 
RL loop and elements: state representation, reward function. 
RL architectures and categorization: value based, policy methods, actor critics 
Applications in robotic manipulation and cloth 

Reinforcement Learning is an eminent approach for learning control policies with no user intervention. A complete review of RL is outside the scope of this thesis. Therefore, we refer the reader to the standard textbook of~\textcite{Sutton2018}.

\section{Datasets for robotic learning} \label{sec:lit_datasets}
Motivatie:
	Link met SL
	Link met RL, offline RL 

Kan simulatie of echte data zijn. 

Relevante werken:
	a large scale dataset for robotic grasp detection depierre
	cornell grasping dataset
	Kleeberger K, Landgraf C, Huber MF. Large-scale 6D object pose estimation dataset for industrial bin-picking
	DexNet, Levine2016
	CLOTH:
		benchmark paper voor deform object manip Seita 


~\textcite{Levine2016} generate a dataset of $800.000$ real executed grasps. They record ...
Dex-Net~\autocite{dexnet2} contains an simulated dataset of $6.7$ million robust grasps. They generate a depth image with candidate grasping pose and grasping outcome of $1.500$ meshes. 

\section{Simulation environments to accelerate learning} \label{sec:lit_simulation}

Motivatie voor simulatie: dure, trage robot tijd 
Componenten: physics engine, robot simulatie, link naar fysische platform.

\subsection{Cloth simulation methods} \label{subsec:lit_cloth_sim}
%Zie ook "Robotic manipulation and sensing of deformable objects in domestic and industrial applications: a survey" p4 voor overzicht om te introduceren maar focus op particle based methods.
%  zie ook Dexterous Robotic Manipulation of Deformable Objects with Multi-Sensory Feedback - a Review van khalil

Wat maakt cloth simulatie moeilijk. 
Aanpakken tot cloth simulatie (kort de categorisatie)
Particle based method. 
Integrators. 
Ander werk dat gebruik maakt van cloth simulatie voor robotic maniulation.

\subsection{Transferring simulation results to the real world}  \label{sec:lit_sim2real}
sim2real problem uitleggen.
Verschillende aanpakken uitleggen: Meta learning, simulatoin randomization, system identifitcation, domain adaptation
literatuur bespreken. 

PAPERS:
	System idemtification
		In~\autocite{Howard2000}, a mass-spring-damper simulation of deformable object is tuned to the real world by probing the object with the gripper containing a force sensor. 

\section{Reward learning}  \label{sec:lit_reward_learning}
General introduction 
Motivatie: Reward hacking, impossible to capture all ingredients
Literatuur: 
	IRL 
	Reward learning 

\section{State perception through instrumentation} \label{sec:lit_instrumentation}
Motivatie.
Definitie.

literatuur:


\end{document}