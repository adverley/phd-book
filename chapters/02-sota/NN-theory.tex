\subsubsection{Artificial Neural networks} \label{subsubsec:lit_dnn}

% INTRO what is een artificieel neuron tov biologisch neuron
\Glspl{ANN} are the workhorse of modern \gls{AI}. \Glspl{ANN} are loosely inspired by the neural network in a biological brain and the mechanisms of learning in biological organisms. The human brain is build-up by interconnected processing units called neurons. The connection strength between these neurons changes in response to external stimuli. This way, neurons receive, process and send information through the body and brain of biological organisms. Although comparing artificial neural networks to their biological counterpart is criticized as a far-stretch from the inner workings of the human brain, insights and knowledge of the neuroscience field have been useful in designing neural network architectures. The most common computational model of neurons, visualized in \cref{fig:neuron}, simulates biological neurons as a node consisting of inputs, weights, bias, activation function and an output value. An \gls{ANN} computes an output by propagating the computed values from the input neurons to the output neurons. The artificial neurons are connected through weights that scale the given input to the neuron. A single neuron performs a weighted sum of the inputs in order to arrive at the neuron's \textit{activation}. Next, it transforms the activation value through an activation function before passing it to the successor neurons. The neuron's activation function is the source of nonlinearity in the network and enables the handling of non-linear relationships between inputs and outputs.

\begin{figure}[htbp]
	\centering
	\subfile{figures/fig-neuron.tex}
	\caption[Computational model of a single neuron.]{\textbf{ Computational model of a single neuron.} $x_0, x_1, x_2$ represent input examples or signals from other units within the network. The bias term $b$ represents an external input to the unit. The activation function is denoted by $f$ and applied on the weighted input entering the unit.}
	\label{fig:neuron}
\end{figure}

% Wiskunde neuron
More formally, the output $y$ of an artificial neuron is computed by
\begin{equation}\label{eq:neuron}
	y = f\left(b + \sum_{n = 1}^{D} w_i x_i \right)\,,
\end{equation}
where $f$ is the activation function, $b$ is the bias, $x_i$ is the $i$th input of the neuron which is weighted by weight $w_i$ connecting the $i$th input. An example of an activation function $f$ is the \gls{RELU}~\autocite{glorot2011deep} activation $f(x) = \max(0, x)$. Other common activation functions are sigmoid, hyperbolic tangents or variations of \glspl{RELU}.

\begin{figure}[htbp]
	\centering
	\subfile{figures/three-layered-mlp.tex}
	\caption[A three-layered, feedforward, fully-connected neural network.]{\textbf{A three-layered, feedforward, fully-connected neural network.} This particular network contains three inputs, two hidden layers and one output layer. All neurons of successive layers are interconnected but there are no connections within in a layer. The inputs are propagated from front to back.}
	\label{fig:three-layer-nn}
\end{figure}

% Neuron in networks: ANN 
In order to perform computations with neurons, we organize them in sequential layers. This gives rise to the name of artificial neural \textit{networks} as they chain together many different functions in a directed acyclic graph. In fully-connected feedforward networks the input and output layer are separated by so-called \textit{hidden layers}. The length of this chain is called the \emph{depth} of the network. This architecture is visualized in \cref{fig:three-layer-nn}. The name of the layer often denotes the operation performed by the layer. For example, a softmax layer performs a \emph{softmax} operation: normalize an input vector of real numbers to probability distribution proportional to the exponentials in the input numbers. Feedforward architectures propagate the inputs sequentially from layer to layer with neurons performing calculations as given in \cref{eq:neuron} but in a vectorized way:
\begin{equation} \label{eq:neuron_output_vectorized}
	\vec{x}^{(k)}=f\left(\vec{b}^{(k)} + \mat{W}^{(k)} \vec{x}^{(k-1)}\right).
\end{equation}
In this \cref{eq:neuron_output_vectorized}, the vector $\vec{x}^{(k)}$ contains the outputs of all neurons in layer $k$ which is based on the input vector $\vec{x}^{(k-1)}$ from previous layer, multiplied with weight matrix $\mat{W}^{(k)}$. The computation occurring in the hidden layers solve a major problem in real-world problems where many applications require disentangling sources of variation by using high-level abstract features. Learned representation offer a solution for such problems because it often results in much better performance compared to hand-designed features. Stacking multiple hidden layers in a neural network allows learning representations based on raw data which solves this central problem of finding features at appropriate levels of abstraction. This paradigm of stacking layers of computational units is called deep learning. It takes a compositional learning approach: upstream representations are expressed in terms of other, simpler downstream representations. Early layers learn primitives which are combined later to form more complex features. In images for example, downstream neurons learn edges and corners which are used in upstream layers to learn to recognize for example the sleeve of a shirt.

% How do we train NNs?
Crucial in neural networks is that the representations are learned instead of crafted by hand. Learning in neural networks occurs by changing the strength of neurons' connections. The weight adjustment is a response to the network's error and has as goal to modify the computation to make the output maximize the given objective. Training of these network weights is done using gradient-based optimization as discussed in \cref{subsec:lit_sl}. Although alternative training methods exist such as evolutionary methods~\autocite{salimans2017evolution}, gradients provide the direction in which to change the weights in order to maximize the objective function. This is especially beneficial for highly parametrized functions such as neural networks. Differentiation of multi-layer neural networks, called backpropagation, was already figured out in~\autocite{rumelhart1986learning} but was rediscovered by running the costly matrix-vector multiplication step in parallel on \gls{GPU}~\autocite{gpu-nn}. Although improved computational hardware and data availability was a crucial enabler of the success of deep learning, many "tweaks" have proven as important to stabilize the backpropagation algorithm. Gradients assume an infinitesimal small step in each direction whereas the actual step we make has a finite length in order to make any real progress in optimization. The problem is that the gradients do change during the course of this step. In the case of multivariable optimization problems of considerable size, which is the case in deep neural networks containing millions of parameters, the optimization landscape is highly non-convex. This treacherous optimization landscape can change the gradients drastically leading to unstable training. This is why gradient-descent strategies such as momentum-based learning, using parameter-specific learning rates and weight initializing schemes are standard tricks in the deep learning practitioners toolkit. Gradients have also been known to disappear and diverge in deep neural networks because of repeated matrix multiplications when propagating the information forward and backward through the network. This is why \gls{RELU} activation functions are popular given that this piecewise linear activation has a derivative of value $1$ in certain intervals and zero elsewhere. Another problem caused by the highly parametrized nature of deep neural networks is overfitting the data. In machine learning jargon, this means that a model can predict the training set well but performs poorly on hold-out samples. A popular way to deal with overfitting is to regularize the network weights. Regularization effectively reduces the network computational power by imposing a penalty on weights in the loss function:
\begin{equation}\label{eq:regularization}
	L(\vec{\theta})=\frac{1}{N} \sum_{n=1}^{N} P\left(f\left(\vec{x}^{(n)}, \vec{\theta}\right), y^{(n)}\right)+\Omega(\vec{\theta}).
\end{equation}
In \cref{eq:regularization}, the function $P(.)$ is a chosen optimization metric that takes the output of the network $f\left(\boldsymbol{x}^{(n)}, \boldsymbol{\theta}\right)$ and the real label $y^{(n)}$ of sample $n$ and is domain-dependent. The other term $\Omega(\boldsymbol{\theta})$ is the regularization term which balances $\ell 1$ and $\ell 2$ norm of the weights:
\begin{equation} \label{eq:regularization-term}
	\Omega(\vec{\theta})=\gamma \underbrace{\sum_{k} \sum_{i} \sum_{j}\left|\mat{W}_{i j}^{(k)}\right|}_{\ell 1 \text { regularization }}+\lambda \underbrace{\sum_{k} \sum_{i} \sum_{j}\left(\mat{W}_{i j}^{(k)}\right)^{2}}_{\ell 2 \text {-regularization }}.
\end{equation}
The hyperparameters $\gamma$ and $\lambda$ in the regularization term of \cref{eq:regularization-term} trade-off the amount of $\ell 1$ and $\ell 2$ regularization. The term $\mat{W}_{i j}^{(k)}$ refers to the weight in layer $k$ connecting neuron $i$ to neuron $j$. $\ell 1$ regularization achieves sparse weights while the $\ell 2$ norm leads to networks with smaller weights. Other popular methods to improve generalization properties of deep neural networks is dropout~\autocite{dropout}, which zeroes out different random neurons at training time, ensembling and using data augmentation.

% CNNs
The feedforward models described above connect the neurons between layers in a fully-connected manner: every neuron from a layer is connected to every neuron from the preceding and succeeding layer with an unique weight. This dense connectivity lead to an explosion in the amount of trainable parameters. However, when the input data contains topological structure, like the ordering of image pixels in a grid, constraining the connectivity pattern between layers is a useful method to reduce the amount of parameters and exploit correlation. The most common way to implement this is by replacing the matrix-vector product $\mat{W}^{(k)} \vec{x}^{(k-1)}$ of \cref{eq:neuron_output_vectorized} with a sum of convolutions. This operation is equivalent to sliding a low-dimensional filter or kernel over the input image while performing a dot product. This property leads to sparse connectivity and parameter sharing. Consequently, by connecting a local spatial region with a shared set of parameters to the full spatial resolution of the image, one imitates the cortical neurons in the visual cortex, which respond only to stimuli within a receptive field~\autocite{hubel1959receptive}. It has been argued that these properties explain the success of using trainable convolutions for computer vision~\autocite{Goodfellow2016}. Usually, the convolution operation is followed by a downsampling operation that provides a summary statistic of the nearby outputs. This is most frequently implemented with an aggregation function, for example max pooling that takes the maximum value of a rectangular neighborhood. The process of embedding convolution operations, optionally followed by pooling operations, is known as a \gls{CNN}. After demonstrating the effectiveness of \glspl{CNN} for large-scale image classification~\autocite{Krizhevsky2012}, \glspl{CNN} have been omnipresent in computer vision and natural language processing. In robotics, convolutional layers can provide a perception module while the motor control module is implemented as fully-connected layers acting on the output of the filter banks. However, in the case of motor control, the pooling operation is ofted removed from the architecture as translational invariance is not a desired property. Invariance to the position of a detected object would not enable a robot to detect where the object is located in the image. For more details about the history and working of deep learning methods and architectures, we refer the reader to the textbook of~\textcite{Goodfellow2016}.
