% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

\providecommand{\home}{../..}
\documentclass[\home/main.tex]{subfiles}

\begin{document}

\chapter{Introduction}\label{ch:introduction}
Robotics promises to relieve humanity from repetitive tasks. Yet, why is there currently no robot that ties our shoelaces, weeds our gardens, and folds our shirts?
These tasks break the boundaries of the scripted environment for which existing robot technologies are programmed. Programmed robots follow routines to assemble cars and weld miniaturized electrical components at astounding speeds and fine-grained precision. However, these robots operate in a cage where every instruction is precisely defined with minimal room for deviation. Robots that do operate outside a safety cage, are persistently supervised by humans. For example, the da Vinci\textregistered\ robot allows performing surgery by teleoperating robotic arms through a hand-operated console.
Bringing these robotic automations to our daily lives will significantly impact industry and society. Robots will sort and pack all types of objects in warehouses, install electrical wiring in our cars, helping elderly people dress, and clean up our houses.

For the robot butler to become a reality, we need to endow robots with dexterous manipulation skills. Similar to how you effortlessly turn the page of this book by grasping and reorienting the corner of the page, robot hands require dexterity to perform the same impressive repertoire of tasks as the human hand. This revolution for robotic hand dexterity will occur at the frontiers of both hardware and software. First, mechanical engineering need to incorporate actuators, links, joints and sensors into the limited space of a hand while using soft materials similar to the human skin. Second, control algorithms need to loosen assumptions about the environment in which robots operate. Nowadays, robotic manipulation solutions require environments to be fully specified and the object's configuration to be fixed.
However, the real world presents an infinite supply of configurations.
To find solutions for this problem, we study the manipulation of objects that provide an infinite amount of configurations: deformable objects.

\section{Deformable object manipulation and robotic laundry}
% what is it 
Many everyday objects deform upon force interaction: the wires we use to charge our phones, the clothes we carry, and the sutures doctors use for stitching wounds.
% Robot laundry
In this research, we consider the problem of \textbf{automating the task of folding clothing}. Folding clothing is part of the laundry cycle, visualized in \cref{fig:intro_laundry_cycle}, and a potential task for future household robots.
\begin{figure}[htbp]
    \centering
    \subfile{figures/robotic-laundry-flow.tex}
    \caption[Task flow for robotic laundry.]{\textbf{Task flow for robotic laundry.} Figure adapted from~\autocite{Hamajima1996}.}
    \label{fig:intro_laundry_cycle}
\end{figure}
% \textbf{Hier kort over robotic laundry, het proces en waarom het moeilijk en interessant is. Eventueel economische cijfers toevoegen.}
% WAAROM ECONOMISCH + SOCIAAL
With ageing populations, increasing quality of life and importance of work-life balance, automating dull and repetitive tasks in households merits benefits from a social and financial point of view. Outsourcing tasks to robots allows freeing up time or budget if a domestic helper is employed. For older people, or people with limited mobility, domestic task automation can give a feeling of independence as the need for assistance of home care workers is reduced.
Additionally, there is a significant portion of industry working with garments, textiles and other deformable objects that could also benefit from developments in general-purpose robotic automation.

% why is it relevant?
Although automating domestic tasks offer a sound business case, developments in automation have primarily focussed on industrial automation. This is because industrial tasks are much simpler to automate compared to domestic environments where it is hard to exert exact control and enforce necessary prerequisites. Even in research, folding clothing has received little attention. The main reason is that robotics research focuses primarily on grasping and manipulating rigid objects. Rigid objects do not deform during interaction and significantly reduce the required information. For example, grasping your morning cup of coffee requires to only know the position of the cup's handle.
On the contrary, if the handle is highly deformable, you also need to reason about its shape and how it deforms depending on the planned manipulation. Unfortunately, planning for deformations is not part of traditional robotic control pipelines.

\section{Traditional control pipelines do not work for folding clothes}
A daily-life example found in common kitchens illustrates the workings of today's industrial robots. Instead of manually slicing food into smaller pieces for cooking, one can use a food processor to automate the slicing process.
Using the food processor requires structuring the environment: one needs to equip the correct blade on the motor shaft, preprocess the food into a manageable size for the blade and feed it through the tube while activating the motor. Deviating from this setting can lead to minor failures and even harmful situations. For example, inserting oversized food slices might lead to motor stalling. More dangerously, there is no intelligence in consumer-grade appliances stopping the motor from turning the blades if you decide to insert your hand into the feeding tube. Luckily, a manufacturer can easily prevent such dangerous operations through the mechanical design of the tube.

Current robotic control pipelines are organized similarly to the given food processor example: structuring the environment and decomposing the problem. Decomposing a large problem into subproblems is a general problem-solving and engineering paradigm that allows making assumptions that simplify the solution strategy.
In robotics, this modular approach is omnipresent and has led to incredible levels of automation and an increase in productivity \autocite{Graetz2018}. For example, the pick-and-place contest in the Amazon Robotics Challenges provided the participants with prior knowledge about which objects that need to be grasped\footnote{In the last edition of 2017, the teams knew \qty{50}{\percent} of the items only 45 minutes beforehand.} and allowed them to design their own storage system. Winning solutions \autocite{ijcai2017,morrison2018cartman} solve this structured problem by breaking it down into simpler problems, solved by individual modules for perception, planning and control. These modules are then in turn factorized further into systems performing tasks like labelling pixels to objects, segmenting the target object, and fitting it with bounding boxes.
% However, the innate fear that robots will soon take over our jobs \autocite{cave2019hopes} is unwarranted. %TODO: deze zin eventueel veranderen door een andere koppelzin en steek deze in het begin. Hou het hier eerder 'technisch' ipv maatschappelijk. 
However, fully modularized architectures with no adaptability towards deviations and failures elsewhere in the control pipeline will repeatedly fail when variations occur in the routine. Some of the most advanced robotics research and development teams demonstrate this in the 2015 DARPA Robotics Challenge Finals \autocite{DARPA2015}. The consequences of exposing robots to unstructured environments can be viewed in published video clips \autocite{darpaVideos}: million-dollar robots tumbling to the ground in, frankly, hilarious ways. In one example, a robot is supposedly turning a valve. However, the robot's walking path is misaligned, making it stand next to the valve instead of in front of it. Consequently, the robot is performing a rotating movement with its arm in thin air while assuming resistance of the valve for balancing. The missing counterbalance makes the robot falls to the ground. The rotating valve failure brings the inner working of control loops to light: a sequence of isolated modules that solve decomposed tasks with insufficient regard for failures down- or upstream in the pipeline.

Robots that manipulate deformable objects employ similar pipelines: a sequence of modules with functionalities ranging from garment type identification to corner point detection and executing preprogrammed folding sequences. We visualize a canonical pipeline for folding clothing in \cref{subfig:intro_canonical_control_pipeline}. These approaches are assumption-heavy and can have limited generalization. Engineered pipelines also have difficulty scaling: successful implementations require 24 minutes to fold a single shirt \autocite{Maitin2010}. This motivates our research to look in the direction of \emph{employing learning-based methods} to create autonomous systems that can perform robust grasp synthesis when faced with the high amount of variations that occur in the real world.

\begin{sidewaysfigure}
    \centering
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \subfile{figures/fig-canonical-manipulation-pipeline-folding.tex}
        \caption{Canonical robotic control pipeline for folding clothing. Each module solves an isolated task and passes the result to the next module.}
        \label{subfig:intro_canonical_control_pipeline}
    \end{subfigure}
    \par\bigskip % force a bit of vertical whitespace
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
        \subfile{figures/fig-end-to-end.tex}
        \caption{End-to-end robotic control pipeline for folding clothing. An input image is passed to a black-box deep neural network which sends direct motor commands.}
        \label{subfig:intro_end2end}
    \end{subfigure}

    \caption[]{Standard robotic control pipelines versus end-to-end architectures.}
    \label{fig:intro_pipelines}
\end{sidewaysfigure}

\section{From engineered pipelines to end-to-end learning}
Machine learning is a subset of \gls{AI} methods that can learn patterns from examples. The successes of deep reinforcement learning, a method that learns by trial-and-error, in playing Atari video games \autocite{Mnih2015} and beating human champions in the game of Go \autocite{gaemofGo2016}, has driven robotics researchers to adopt machine learning across the whole pipeline. This led to an end-to-end learning approach in which the robot has to figure out how to move its arms using a single camera image. Every functional module in the pipeline on the top in \cref{subfig:intro_end2end}, for example, is integrally replaced with a deep neural network.

Unfortunately, end-to-end learning for robotics is privileged to organizations containing massive computational power and expensive robotic farms. The robots in these farms operate \emph{interactively} in an environment: they perform actions by trial-and-error to maximize a predefined success criteria. Although robots learning from scratch yield entertaining footage, similar to the falling robots of DARPA 2015, it tires quickly as learning from scratch on a real robot takes tremendous amounts of time and is potentially unsafe. From the perspective of how humans learn, it makes no sense to have a system learn from scratch. Imagine asking a human baby to do the laundry while it still has to learn how to move its limbs. If we want physical systems to solve tasks in dynamic environments, they require some form of prior knowledge to accelerate learning.

For cloth folding robots, we have, on the one hand, robots that use engineered pipelines and take 24 minutes to fold a shirt \autocite{Maitin2010}. On the other hand, there exist robots that learn to fold from scratch but require over \qty{100000}{} physical robot-environment interactions \autocite{Matas2018}. This research explores solutions in the middle of the modelling and learning paradigm.

%  This slow learning time motivates our research to accelerate robot learning for folding clothing. 

% TODO: ontbreken opening naar andere modaliteiten. Meer aanhalen dat alles momenteel vision-based is.  WONT FIX

\section{Accelerating learning of robotic manipulation of deformable objects} \label{sec:intro_acc_learning}
Learning is considered to be a central component for autonomous systems to deal with real-world variability \autocite{kroemer2021review}. However, learning-based methods generally require many examples and interactions with the environment. For example, \qty{800000} grasping attempts are recorded using \qty{12} robots to learn to grasp rigid objects in \autocite{Levine2018}. This is problematic for robots that are expensive to operate. Supervising multiple robots simultaneously for millions of interactions with cloth is an unpractical and unscalable solution. For real robots to learn to fold clothing articles, there is a need to accelerate the learning process. In this dissertation, we research the following four topics to accelerate learning:
(i) generation of cloth folding datasets for learning;
(ii) using simulations as a safe and fast environment for learning;
(iii) facilitating state estimation of cloth; and
(iv) understanding task intent rather than copying human behaviour.
We discuss these topics in the following sections.

\subsection{Datasets}
A critical piece of the puzzle for building intelligent systems comes in the form of \emph{data}. The standard paradigm in machine learning is to provide a dataset with, for example, images of folded and unfolded shirts and train a model that learns to distinguish between the two. When learning how to solve tasks, datasets in the form of task demonstrations have been used for training autonomous driving cars \autocite{bojarski2016end}, learning quadruped robotic locomotion \autocite{peng2020learning}, drone flight \autocite{Giusti2016} and learning to dishes in a plate rack \autocite{Finn2016}. However, in the field of learning robotic manipulation skills for clothing, no dataset with example demonstrations of people folding clothing exist. The lack of folding datasets can be attributed to the domain being less studied, the difficulty in collecting high-quality, diverse samples and the high state space nature of deformable objects, making it hard to generalize example demonstrations to unseen examples. We hypothesize that collecting a dataset of people folding clothing can be used for bootstrapping learning of manipulation skills. This dataset should be collected \enquote{in the wild} to avoid biases and promote diversity. At the same time, the data collection should be standardized in order to facilitate ad-hoc labelling and ensure data quality.

\subsection{Simulation}
Whereas real dataset generation can be expensive to obtain, synthetic data allows generating massive datasets and quick experimentation. Hypothesizes in robotics are often first tested in a virtual environment to allow fast prototyping and validation. To utilize simulation for robotic learning, realistic modelling of the dynamic behaviour of robots and cloth is required. The development of robot simulations technologies have come a great way since the demonstration of deep learning methods for robotics. However, cloth simulation has primarily been developed for offline simulation used in the movie industry or real-time simulations in games where fidelity for cloth is secondary to game-play simulation. Cloth simulation for robotic learning needs to be fast, but not real-time as is the case in games, and requires realistic dynamics. Realistic robot and cloth simulation allow fast experimentation and facilitate transferring the results from simulation to the real world. We believe integrating physically plausible cloth simulation with robotic simulators enables to learn cloth manipulation skills in simulation with transfer to the real world. 

\subsection{Instrumentation}
When we use our hands to crush a plastic cup, multiple sensors of our body activates: we use our eyes to observe the deformations, our ears register the amplitude of the impact, our hands notify us of how much force we are applying, and our proprioceptive system signal us to which extent our fingers are closed. This rich interplay of multiple modalities in the human cognitive system is in stark contrast to robotic manipulation pipelines that are primarily vision-based. Vision is important for robotic manipulation: it helps infer the object location relative to the robot end-effectors, it helps understand the object's geometry and some of its physical properties. Furthermore, commercial cameras are readily available and accessible compared to other sensors such as tactile sensors. Nevertheless, considering the giant leaps of object recognition using deep learning, robots still struggle to recognize objects in more difficult contexts such as partial occlusion, transparent object and moving objects \autocite{Guo2014,sajjan2019cleargrasp,Ojha2015}.
Incorporating heterogeneous sources of information can alleviate problems when state estimation cannot be directly observed from pixels. For example, finding the occluded corner of a crumbled towel is possible using sensing and simplifies the folding process considerably.
We denote the process of adding sensory information to the learning environment of a robot as \textbf{instrumentation}. The goal is to direct the large focus on using vision-based state estimation to applying other sensor modalities in the environment such as tactile sensing in a cloth and force sensing in the fingers. Our hypothesis is that some modalities encode parts of the state in a compacter way than vision. This semantically more meaningful encoding accelerates learning, which is important in robotics where real rollouts are expensive.

% \subsection{Grippers for folding}
% The human hand is a general-purpose manipulator with formadiable dexterous manipulation capabilities. 
\subsection{Understanding task intent}
Humans their ability to infer task intent from demonstration is in stark contrast to imitation learning approaches from the machine learning field. Imitation learning for autonomous cars, for example, answers the question \enquote{with how many degrees would a human driver turn the steering wheel now that this tree is in front of me?}. 
Although the correct answer is probably avoiding to come into the situation that a tree is in front of the car, no human driver would likely be able to provide an exact answer.

Instead of creating a dataset of task scenarios with associated human actions, the interactive trial-and-error approach of \gls{RL} shares more semantics with learning from a human perspective. For example, humans encourage desirable behaviour in dogs by means of treats or punishment. In this example, the snack is the reward signal. Applying the idea of reward and punishment to robotics is a popular learning approach but requires finding the appropriate reward signal. In the case of cloth folding, it is ambiguous to write down a mathematical formula that describes how well a person is folding a clothing article. How does one quantify the number of wrinkles to a performance measure? Does it matter that a folded towel does not contain perfectly straight lines? What is the definition of a folded shirt? Finding a reward signal requires identifying, measuring and quantifying all aspects of performance. This might not be possible for all problems, like cloth folding.

% Infants are considered to have a different internal learning loop compared to dog training.
Compared to training dogs, infants are known to understand task intent behind human behaviour without requiring explicit reward signals \autocite{warneken2006altruistic}.
We argue that a similar approach can be applied for the case of robotic folding, and learning in general. By learning the reward function from human demonstrations, without labelling, one can use this reward signal in an \gls{RL} setting. This idea is generally known as inverse \gls{RL}. However, inverse RL learns a reward function in conjunction with learning how to behave. We propose decoupling this process and learning a reward function separately to speed-up training and use it for other applications like process monitoring.

\section{Research outline}
Our main goal is to investigate solutions to accelerate learning-based approaches for learning to fold clothes with robots. We focus on the domains described in the previous \cref{sec:intro_acc_learning}: generation of real datasets, the use of simulation technologies, instrumentation and learning a reward function for cloth folding. Our main contributions can be listed as follows:
%TODO explicieter schrijven: afgaande op sectie 1.4 distillen we de volgende contributions
% TODO: wat met simulatie? opeens is dit weg
\begin{itemize}
    \item Collecting a crowdsourced dataset of people folding clothing articles.
    \item Estimating the state of cloth with smart textile and proving its utility in a real-world, low-cost robotic setup.
    \item Providing a methodology to learn a reward function without providing labels.
\end{itemize}

To structure the content of our research, we first provide an overview of the field of robotic manipulation of deformable objects in \cref{ch:sota}. We provide the introduction and state-of-the-art in a tutorial and survey style chapter. As a first experiment, we study the use of robot and cloth simulation for fast experimentation in \cref{ch:simulation}. Next, we deal with a shortcoming when transferring from simulation to the real world: the unavailability of a full state specification of the cloth. We measure the state of the cloth by developing a smart textile in \cref{ch:instrumentation}. This smart cloth consists of tactile sensors that are used in a machine learning model to communicate whether the cloth is crumbled, folded or unfolded. We demonstrate the effectiveness of the smart cloth on a real robotic setup that learns to fold cloth from scratch using RL. To speed up this approach, we collect a crowdsourced dataset of people folding clothing in \cref{ch:data_collection}. We use this dataset in \cref{ch:reward_functions} in which we learn a reward function that expresses how well people are folding clothing in this dataset. Notable in our approach is that no progress labels are required.
In the following \cref{ch:towards_robotic_folding}, we zoom out to take a birds-eye perspective on the field of robotic cloth folding in order to highlight future areas of improvement, based on our experience and the research done in our work.
Finally, we summarize our work and provide concluding remarks in \cref{ch:conclusion}.

\section{Publications}
All chapters in this book are based on publications done during the course of this research. The exceptions are the introduction and conclusion of this book. The work described in \Cref{ch:simulation} is not published as similar research was already being published and the goal of our research is not to provide state-of-the-art simulation technologies for cloth simulation. We provide a list of publications below:
\begin{itemize}
    \item \fullcite{Verleysen2020-dataset}
    \item \fullcite{Verleysen2020-folding}
    \item \fullcite{Proesmans2022}
    \item \fullcite{Verleysen2022-tcn}
    \item \fullcite{Verleysen2022-survey}
\end{itemize}

\end{document}
