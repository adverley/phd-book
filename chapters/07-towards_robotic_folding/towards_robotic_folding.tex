% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

\providecommand{\home}{../..}
\documentclass[\home/main.tex]{subfiles}

\begin{document}

% TODO: check ook nog laatste secties van [2018 IJRR]The limits and potentials of deeplearning for robotics.pdf

\chapter{Towards learning robotic manipulation of clothing}\label{ch:towards_robotic_folding}

In this dissertation, we considered the problem of learning robotic folding of clothing items by using simulation, smart clothing and learning task progression from human demonstrations. In this chapter, we zoom out to take a birds-eye perspective on the field of robotic folding in order to highlight future areas of improvement. Our goal is to describe high-potential areas of research that, according to the research done in this dissertation, require further investigation in order for the field to advance towards learning robotic manipulation of clothing articles.

\section{Improving sample efficiency}

Data-driven approaches have triggered new developments for solving robotic manipulation problems. In the context of robotic cloth folding, we believe the learning paradigm will remain omnipresent to learn cloth dynamics, material properties and sensorimotor skills by interacting with the environment. In order to capitalize on learning-based methods, future research has to focus on improving the sample efficiency of these data-hungry methods. This need is imperative for the robotics domain where experimentation with real robots is expensive. 

% Do not learn everything
% TODO: consider a subsection per "approach, way"
A first way to reduce data requirements evidently is to \emph{not learn every part of the pipeline}. Incorporating prior knowledge that is available is an important practice to avoid allocating learning time on unproductive skills.
The broad knowledge of body described in \cref{sec:lit_cloth_folding_pipelines} can be used as prior for learning. For example, when learning from interaction is used to achieve wrinkle-free folding, the canonical way of defining the reward is a sparse signal indicating whether the cloth is folded or not. However, when end-to-end learning was not prevalent in the robotics folding community, researchers exploited gravity as a way to unfold clothing and remove wrinkles \autocite{Doumanoglou2016,Maitin2010}. This prior knowledge of exploiting gravity can be structured into the reward or task by driving the agent towards lifting the piece of cloth in the air.
Besides incorporating task knowledge, analytical models can be exploited. For example, the work of \textcite{Zhang2015} and our own work in \cref{ch:sim} defines the action space in joint space with each action being a movement of one joint with a fixed delta angle. However, in both cases the inverse kinematics of the robot is known which implies that the action space could be defined as the configuration space of the end-effector. In the case of a 7 \gls{DOF} arm, this reduces the action space from $\mathbb{R}^{21}$ to $\mathbb{R}^6$. However, this example would be difficult to realize in case the kinematics are ill-defined, for example, when the robot arm contains a large degree of kinematic redundancy. 
This perspective can alternatively be considered as deconstructing end-to-end learning pipelines where possible. We have demonstrated this approach in \cref{ch:instrumentation} where we used a smart cloth that autonomously communicates its state. This is in stark contrast to current research that engineers vision pipelines \autocite{Wu2020} which potentially fails outside laboratory environments or incorporate learning the state implicitely in the model \autocite{Matas2018}. 
Another instance of decoupling the pipeline can be done by learning as much as possible offline. Batch reinforcement learning, which was introduced in \cref{subsec:lit_rl}, aims to maximally distill the knowledge in a static dataset. Theoretically, this is what Q-learning achieves: learn an optimal policy with suboptimal demonstrations \autocite{Sutton2018}. However, the deep variants have shown to be susciptible to performance collapse when only off-policy data is used \autocite{hausknecht2016policy}. Current research in batch RL is ongoing and merits high potential for reducing the amount of data required for learning. Using batch RL is also a way to commit to the appeal to make learning look as much as possible as supervised learning. Indeed, the same way that supervised learning has turned data into strong pattern recognizers, batch RL can turn data into strong decision making engines. 
A final implementation of learning offline is shown in this research (\cref{ch:reward_functions}) where we learned a semantic meaningful representation for the task using task demonstrations. It is possible to gain significant learning time by decoupling representation learning from interaction time on the physical robot. We elaborate on learning and using representations in \cref{sec:towards_sensing_representation}. 

% Curriculum learning 
A second approach for improving sample effiency is by adjusting the task instantiation to the skills already acquired by the agent. This principle of gradually ramping up the task difficulty is known as \emph{curriculum learning} \autocite{Selfridge1985}. Different methods exist for introducing curricula. In robotic folding, it is possible to first learn how to fold stiff shirts by for example modeling planes joined with hinge joints. Once the first task is learned, some deformability in the planes can be introduced and eventually scaffold the difficulty to a real shirt. Despite adding domain knowledge about the task difficulty accelerates learning, finding which domain parameters are important to scaffold requires research and fine-tuning per domain. This ambigituity makes learning the curriculum a promising alternative.  
Hindsight experience replay \autocite{andrychowicz2017hindsight}, for example, maps states to goals and uses heuristics to carefully select which goal-trajectory sequences to replay. Other work proposes to learn the curricula via self-play to generate goal states \autocite{sukhbaatar2017intrinsic} or randomize the domain \autocite{raparthy2020generating}. Despite demonstrating successful results, the scope of tasks considered in the former work is limited to artificial environments containing a small state space. As of today, there is to the best of our knowledge no automatic curricula generating method that is able to overcome the instability associated with multi-agent systems where both agents are learning in complex environments.
The two former approaches indicate an incongruity between manually specified and fully learned curricula. In reality, setting the curriculum is a spectrum in which elements of both manual and adaptive specification appear. \textcite{rudin2021learning} for example, learn quadruped locomotion by using a curriculum heuristic that adapts the slope and stairs in the environment. 
We believe combining heuristic curricula with adaptive and manually-tuned environment elements enables to accelerate learning, Sim2Real transfer and uncover the aspects of the task that makes learning difficult. This is in fact the overarching theme; we advocate there is a spectrum to be leveraged between programming and data. 

% LfD and Unify RL and imitation learning
Other potential gains for speeding up learning can be found by \emph{leveraging demonstration data in RL}. 
Learning from demonstration has a longstanding history in robotics \autocite{Argall2009}. Using demonstrations usually incurs one the following problems. 
First, learning from demonstration is known to suffer from generalization and exploration issues in the case of behavioral cloning and dataset aggregation respectively \autocite{Ibarz2021}.
Second, it is not always possible to collect a dataset of demonstrations. For example, recent work for combining RL and imitation learning insert kinesthetically \autocite{vecerik2018leveraging} or teleoperated demonstrations \autocite{Zhu-RSS-18}. However, moving the robot arm manually or teleoperating it is not possible for tasks with difficult dynamics like the cloth folding. This is why we investigated the use of demonstrations external to the body of the robot in this research. However, using external observations as bootstrap for learning requires solving a correspondence problem between demonstrator and learner. For this reason, we believe that further gains can be found in the avenue of using external demonstrations as self-supervised representation learning method to be used for downstream tasks. We discuss possible technical future work of our method in \cref{sec:conc_future_work}. 

An alternative form of providing demonstrations is in the form of supervision using a scripted policy. Residual RL in particular exploits the spectrum between programming and data by learning a policy that is additively combined with a scripted policy. The residual approach retains a scripted component which allows pushing the agent to succesful and important regions in the state space. When folding clothing, for example, a hand-engineered controller can navigate the agent towards the cloth using simple segmentation methods discussed in \cref{ch:sota}. 

\section{Datasets}
Among computational resources and algorithm-centric advances, the availability of large amounts of data was important for deep learning to break through at the turn of the century. Datasets have played a similar role for the success of data-driven methods in robotic manipulation. In the case of learning to grasp objects, Dex-Net \autocite{dexnet2} puts forward a methodology to frame grasping as a supervised learning problem which leads to a qualitative learning signal. The highly successful Dex-Net relies on generating a massive dataset in simulation containing 6.9 million images. This size is approximately 7 times the size of the largest real-life data collection setup of a single institution containing robotic grasping attempts \autocite{Levine2016}. In contrast to rigid object manipulation, there is a dataset scarcity in the domain of cloth folding. The publicly-shared cloth datasets by researchers are primarily vision-based. Augmenting such datasets with tactile sensing is of importance for dealing with occlusions in cloth manipulation. The field could advance forwards with Dex-Net-like datasets containing a data generator containing multiple objects, with different physical properties. A recent, first initiative towards such datasets is executed in \textcite{DefGraspSim} where grasp metrics for volumetric deformables are generated in simulation. Similar implementations for grasping and manipulating cloth would drive the field significantly forward. 

Although synthetic data is easily available with a simulator, it is plagued with transferability problems which makes research for realistic simulations, robust algorithms and system identification crucial. Alternatively, real datasets do not suffer from these problems. However, real datasets of cloth folding are rare. Datasets containing multiple modalities of different types of cloths and materials could help learning representations and understanding cloth interactions. Inspiration can be found in the retail fashion that contains high-quality datasets \autocite{DeepFashion, DeepFashion2, FashionAI}. However, the modalities and item interaction are limited making it non-trivial to find possible application avenues for robotic manipulation tasks. A future direction consists of applying the methodology FashionAI \autocite{FashionAI} uses to scale up the data collection and labeling. FashionAI uses experts to disentangle attributes and label a small fraction of the data. Then, they train a network to label the remainder of the dataset. Training is halted when the network makes too many errors which could jeopardize the label quality. Such approach can be used for labeling and understanding human interaction with cloth. 
Real datasets containing example demonstrations can be used for bootstrapping learning. In our research, we filled in this gap by providing a dataset of humans folding clothing. A large-scale dataset with physical robot-cloth manipulations, similar to \autocite{Levine2016} does not exist at time of writing. Ideally, future efforts are directed towards gathering multimodal data of cloth interaction, possibly enriched with large-scale simulated data similar to Dex-Net. This dataset can then be used for understanding material properties of the cloth, bootstrapping learning from the example demonstrations and learning semantic meaningful representations.

% Staat hier iets in over de rol van self-supervision in datasets? 
%  Nog iets toevoegen over in the wild datasets? 

TODO: BENCHMARKS SETS TOEVOEGEN
    Simulation
        SoftGym
        -> check ook hun related work 

    Real 
        household set https://arxiv.org/pdf/2111.01527.pdf 
            -> check ook hun related work 

\section{Simulation}
Data-driven methods from the machine learning domain have proven extremely powerful for adaptive and robust control. However, they require many examples which are expensive to generate on real, physical systems. The outlook remains that highly parametrized functions such as neural networks will keep on requiring and improving on large amounts of data \autocite{sun2017revisiting}. Hence, simulation will remain an important tool for robotic learning to generate large datasets. As distilled in \cref{ch:sota}, there are roughly two roads for exploiting simulations: learning from interaction or constructing large datasets with realistic grasping examples, similar to Dex-Net \autocite{dexnet}. As of today, it appears the second road, i.e.\ generating datasets, has proven to be commercially exploitable in comparison to learning from interaction. Dex-Net for example, has been commercialized two years after its advent\footnote{https://www.ambirobotics.com/}. Pursuing a similar path for robotic cloth folding is meritful but first requires further research and development in soft body simulators. Specifically,
realistic simulation of friction, material deformation and other physical properties will have to be further developed before the deformable object manipulation domain can follow along the same path as their rigid counterpart. The outlook is promising as  existing and new simulations started supporting soft body simulation, for example MuJoCo 2.0, Isaac\footnote{https://developer.nvidia.com/isaac-gym}, and SoftGym. In addition, state-of-the-art research in cloth simulation recently addressed real-time simulation of realistic cloth and solves inverse control problem like human-assisted dressing material properties estimation \autocite{Junbang2019,li2021diffcloth}. However, the support for robotics integration is limited. It appears that current robotics researchers have to trade-off simulation fidelity and cloth features versus integration possibilities with robotics and control. Hence, future directions should consider a tighter integration between high-fidelty cloth simulation and practicable robot control.

An important characteristic of the cloth simulation is the degree of parallelisation. Running many learning environments in parallel has shown to allow learning quadruped locomotion on a single machine within a single day \autocite{rudin2021learning}. We believe this is a strong example that advocates for future cloth simulations to run on GPU in order to run learning environments massively in parallel. Our research fills addressed this idea with our cloth simulation on \gls{GPU} introduced in \cref{ch:simulation}.    

Deformable bodies require reasoning about the shape, dynamics and material properties of the object. This makes neural networks a viable replacement for analytical models in simulations. Neural networks can learn the forward dynamics of cloth from sensory input. Moreover, forward passes through the network can be faster than a forward pass through all simulation substeps. 
However, dataset bias is also an issue in training machine learning models to predict physics dynamics: world models are trained on a dataset so it is unclear how well they generalize. Complex, learned dynamics models may show visually unrealistic deformations, lose volume over time and cannot deal with occlusion which is bound to happen when manipulating with an end-effector \autocite{Mrowca2018, Li2018}. Therefore, future research efforts should deal with occlusion by incrementally learning the object properties. 
Once again, insights from analytical models of cloth can be exploited in parametrizing the learning model. 
Graph neural networks, for example, reflect the interconnected particle nature of mass-spring simulation approach to cloth and show promising results for realistic and efficient cloth simulation \autocite{pfaff2021learning}.
 
Finally, differentiable simulations allows backpropagating gradients through the physical consequences of actions. Differentiable cloth simulation has been shown to be an effective approach for control problems and estimating material properties REFERNETIES?.
A future avenue worth exploring is the co-optimization of body and brain: how would the robot morphology evolve if taking the end-effector in to the loop. Iets schrijven over research van jurylid? 

To summarize, we strongly advocate for further developing the realism and parallelism of cloth simulations that allow for semi-realistic robot-cloth interactions. Ideally, the simulator exposes numerical or analytical gradients which allow gradient-based optimization of inverse problems like material properties estimation. Nonetheless, the question remains on how far we can drive the realism of the simulation. A high-quality cloth simulation of Blender, for example, is shown to be not transferrable to the real world in the work of \autocite{Tanaka2018}. Hence, Sim2Real transfer remains an important research direction which we discuss next. 

\section{Sim2Real}

\section{Sensing and representation} \label{sec:towards_sensing_representation}

% DL and NN as representation method
%   Bekijk ook https://journals.sagepub.com/doi/10.1177/0278364918770733?icid=int.sj-full-text.similar-articles.2

\section{Hardware for robotic folding}

\section{To categorize delete me later}

\printbibliography

\end{document}