@article{Doumanoglou2016,
  author   = {A. {Doumanoglou} and J. {Stria} and G. {Peleka} and I. {Mariolis} and V. {Petrík} and A. {Kargakos} and L. {Wagner} and V. {Hlaváč} and T. {Kim} and S. {Malassiotis}},
  journal  = {IEEE Transactions on Robotics},
  title    = {Folding Clothes Autonomously: A Complete Pipeline},
  year     = {2016},
  volume   = {32},
  number   = {6},
  pages    = {1461-1478},
  keywords = {clothing;image colour analysis;image texture;manipulators;robot vision;service robots;clothes folding;dual-armed robot;machine vision;robotic manipulation;crumpled garments;color information;texture information;grasping point;hanging garment;polygonal models;T-shirts;towels;shorts;Object segmentation;Robots;Deformable objects;Image segmentation;Grasping;Pose estimation;Active vision;clothes;deformable objects;manipulation;perception;random forests;robotics},
  doi      = {10.1109/TRO.2016.2602376},
  issn     = {1552-3098},
  month    = {Dec}
}

@inproceedings{Maitin2010,
  author    = {J. {Maitin-Shepard} and M. {Cusumano-Towner} and J. {Lei} and P. {Abbeel}},
  booktitle = {2010 IEEE International Conference on Robotics and Automation},
  title     = {Cloth grasp point detection based on multiple-view geometric cues with application to robotic towel folding},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {2308-2315},
  keywords  = {clothing;manipulators;mobile robots;object detection;robot vision;service robots;multiple-view geometric cues;robotic towel folding;vision-based grasp point detection algorithm;general-purpose two-armed mobile robotic platform;general purpose manipulators;Robotics and automation;Robot vision systems;Clothing;Collaborative work;Robustness;Manipulators;Machine learning algorithms;USA Councils;Detection algorithms;Mobile robots},
  doi       = {10.1109/ROBOT.2010.5509439},
  issn      = {1050-4729},
  month     = {May}
}


@inproceedings{Matas2018,
  title     = {Sim-to-Real Reinforcement Learning for Deformable Object Manipulation},
  author    = {Matas, Jan and James, Stephen and Davison, Andrew J.},
  booktitle = {Proceedings of The 2nd Conference on Robot Learning},
  pages     = {734--743},
  year      = {2018},
  editor    = {Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun},
  volume    = {87},
  series    = {Proceedings of Machine Learning Research},
  address   = {},
  month     = {29--31 Oct},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v87/matas18a/matas18a.pdf},
  url       = {http://proceedings.mlr.press/v87/matas18a.html},
  abstract  = {We have seen much recent progress in rigid object manipulation, but interaction with deformable objects has notably lagged behind. Due to the large configuration space of deformable objects, solutions using traditional modelling approaches require significant engineering work. Perhaps then, bypassing the need for explicit modelling and instead learning the control in an end-to-end manner serves as a better approach? Despite the growing interest in the use of end-to-end robot learning approaches, only a small amount of work has focused on their applicability to deformable object manipulation. Moreover, due to the large amount of data needed to learn these end-to-end solutions, an emerging trend is to learn control policies in simulation and then transfer them over to the real world. To date, no work has explored whether it is possible to learn and transfer deformable object policies. We believe that if sim-to-real methods are to be employed further, then it should be possible to learn to interact with a wide variety of objects, and not only rigid objects. In this work, we use a combination of state-of-the-art deep reinforcement learning algorithms to solve the problem of manipulating deformable objects (specifically cloth). We evaluate our approach on three tasks—folding a towel up to a mark, folding a face towel diagonally, and draping a piece of cloth over a hanger. Our agents are fully trained in simulation with domain randomisation, and then successfully deployed in the real world without having seen any real deformable objects.}
}

@article{Seita2018,
  author        = {Daniel Seita and
               Nawid Jamali and
               Michael Laskey and
               Ron Berenstein and
               Ajay Kumar Tanwani and
               Prakash Baskaran and
               Soshi Iba and
               John F. Canny and
               Ken Goldberg},
  title         = {Robot Bed-Making: Deep Transfer Learning Using Depth Sensing of Deformable
               Fabric},
  journal       = {CoRR},
  volume        = {abs/1809.09810},
  year          = {2018},
  url           = {http://arxiv.org/abs/1809.09810},
  archiveprefix = {arXiv},
  eprint        = {1809.09810},
  timestamp     = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1809-09810},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Mahler2019,
  author       = {Mahler, Jeffrey and Matl, Matthew and Satish, Vishal and Danielczuk, Michael and DeRose, Bill and McKinley, Stephen and Goldberg, Ken},
  title        = {Learning ambidextrous robot grasping policies},
  volume       = {4},
  number       = {26},
  elocation-id = {eaau4984},
  year         = {2019},
  doi          = {10.1126/scirobotics.aau4984},
  publisher    = {Science Robotics},
  abstract     = {Universal picking (UP), or reliable robot grasping of a diverse range of novel objects from heaps, is a grand challenge for e-commerce order fulfillment, manufacturing, inspection, and home service robots. Optimizing the rate, reliability, and range of UP is difficult due to inherent uncertainty in sensing, control, and contact physics. This paper explores {\textquotedblleft}ambidextrous{\textquotedblright} robot grasping, where two or more heterogeneous grippers are used. We present Dexterity Network (Dex-Net) 4.0, a substantial extension to previous versions of Dex-Net that learns policies for a given set of grippers by training on synthetic datasets using domain randomization with analytic models of physics and geometry. We train policies for a parallel-jaw and a vacuum-based suction cup gripper on 5 million synthetic depth images, grasps, and rewards generated from heaps of three-dimensional objects. On a physical robot with two grippers, the Dex-Net 4.0 policy consistently clears bins of up to 25 novel objects with reliability greater than 95\% at a rate of more than 300 mean picks per hour.},
  url          = {https://robotics.sciencemag.org/content/4/26/eaau4984},
  eprint       = {https://robotics.sciencemag.org/content/4/26/eaau4984.full.pdf},
  journal      = {Science Robotics}
}


@article{Foresti2004,
  author   = {G. L. {Foresti} and F. A. {Pellegrino}},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  title    = {Automatic visual recognition of deformable objects for grasping and manipulation},
  year     = {2004},
  volume   = {34},
  number   = {3},
  pages    = {325-333},
  keywords = {self-organising feature maps;image segmentation;image texture;object recognition;image recognition;feature extraction;image colour analysis;automatic visual recognition;deformable objects;vision-based system;self-organized neural network;color image segmentation;image texture information;object recognition;points extraction;morphological analysis;object grasping;object manipulation;Deformable models;Data mining;Robot vision systems;Object recognition;Computer vision;Robotics and automation;Principal component analysis;Neural networks;Image segmentation;Color},
  doi      = {10.1109/TSMCC.2003.819701},
  issn     = {1094-6977},
  month    = {Aug}
}

@article{Levine2018,
  author   = {Sergey Levine and Peter Pastor and Alex Krizhevsky and Julian Ibarz and Deirdre Quillen},
  title    = {Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  journal  = {The International Journal of Robotics Research},
  volume   = {37},
  number   = {4-5},
  pages    = {421-436},
  year     = {2018},
  doi      = {10.1177/0278364917710318},
  url      = { 
        https://doi.org/10.1177/0278364917710318
    
},
  eprint   = { 
        https://doi.org/10.1177/0278364917710318
    
},
  abstract = { We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. We describe two large-scale experiments that we conducted on two separate robotic platforms. In the first experiment, about 800,000 grasp attempts were collected over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and gripper wear and tear. In the second experiment, we used a different robotic platform and 8 robots to collect a dataset consisting of over 900,000 grasp attempts. The second robotic platform was used to test transfer between robots, and the degree to which data from a different set of robots can be used to aid learning. Our experimental results demonstrate that our approach achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. Our transfer experiment also illustrates that data from different robots can be combined to learn more reliable and effective grasping. }
}

@inproceedings{Finn2016,
  title     = {Guided cost learning: Deep inverse optimal control via policy optimization},
  author    = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle = {International Conference on Machine Learning},
  pages     = {49--58},
  year      = {2016}
}

@article{Vevcerik2017,
  title   = {Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author  = {Ve{\v{c}}er{\'\i}k, Matej and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal = {arXiv preprint arXiv:1707.08817},
  year    = {2017}
}

@inproceedings{Abbeel2004,
  title        = {Apprenticeship learning via inverse reinforcement learning},
  author       = {Abbeel, Pieter and Ng, Andrew Y},
  booktitle    = {Proceedings of the twenty-first international conference on Machine learning},
  pages        = {1},
  year         = {2004},
  organization = {ACM}
}

@inproceedings{Sharma2018,
  title     = {Multiple Interactions Made Easy (MIME): Large Scale Demonstrations Data for Imitation},
  author    = {Sharma, Pratyusha and Mohan, Lekha and Pinto, Lerrel and Gupta, Abhinav},
  booktitle = {Conference on Robot Learning},
  pages     = {906--915},
  year      = {2018}
}

@article{English2018,
  author   = {English, P.B. and Richardson, M.J. and Garzón-Galvis, C.},
  title    = {From Crowdsourcing to Extreme Citizen Science: Participatory Research for Environmental Health},
  journal  = {Annual Review of Public Health},
  volume   = {39},
  number   = {1},
  pages    = {335-350},
  year     = {2018},
  doi      = {10.1146/annurev-publhealth-040617-013702},
  note     = {PMID: 29608871},
  url      = { 
        https://doi.org/10.1146/annurev-publhealth-040617-013702
    
},
  eprint   = { 
        https://doi.org/10.1146/annurev-publhealth-040617-013702
    
},
  abstract = { Environmental health issues are becoming more challenging, and addressing them requires new approaches to research design and decision-making processes. Participatory research approaches, in which researchers and communities are involved in all aspects of a research study, can improve study outcomes and foster greater data accessibility and utility as well as increase public transparency. Here we review varied concepts of participatory research, describe how it complements and overlaps with community engagement and environmental justice, examine its intersection with emerging environmental sensor technologies, and discuss the strengths and limitations of participatory research. Although participatory research includes methodological challenges, such as biases in data collection and data quality, it has been found to increase the relevance of research questions, result in better knowledge production, and impact health policies. Improved research partnerships among government agencies, academia, and communities can increase scientific rigor, build community capacity, and produce sustainable outcomes. }
}



@article{Fleming2019,
  author   = {Peter Fleming},
  title    = {Robots and Organization Studies: Why Robots Might Not Want to Steal Your Job},
  journal  = {Organization Studies},
  volume   = {40},
  number   = {1},
  pages    = {23-38},
  year     = {2019},
  doi      = {10.1177/0170840618765568},
  url      = { 
        https://doi.org/10.1177/0170840618765568
    
},
  eprint   = { 
        https://doi.org/10.1177/0170840618765568
    
},
  abstract = { A number of recent high-profile studies of robotics and artificial intelligence (or AI) in economics and sociology have predicted that many jobs will soon disappear due to automation, with few new ones replacing them. While techno-optimists and techno-pessimists contest whether a jobless future is a positive development or not, this paper points to the elephant in the room. Despite successive waves of computerization (including advanced machine learning), jobs have not disappeared. And probably won’t in the near future. To explain why, some basic insights from organization studies can make a contribution. I propose the concept of ‘bounded automation’ to demonstrate how organizational forces mould the application of technology in the employment sector. If work does not vanish in the age of AI, then poorly paid jobs will most certainly proliferate, I argue. Finally, a case is made for the scholarly community to engage with wider social justice concerns. This I term public organization studies. }
}


@inproceedings{Liu2016,
  author    = {Liu, Ziwei and Luo, Ping and Qiu, Shi and Wang, Xiaogang and Tang, Xiaoou},
  title     = {DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations},
  booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2016}
}
 
 @misc{Xiang2016,
  author = {Lingzhu Xiang and
                  Florian Echtler and
                  Christian Kerl and
                  Thiemo Wiedemeyer and
                  Lars and
                  hanyazou and
                  Ryan Gordon and
                  Francisco Facioni and
                  laborer2008 and
                  Rich Wareham and
                  Matthias Goldhoorn and
                  alberth and
                  gaborpapp and
                  Steffen Fuchs and
                  jmtatsch and
                  Joshua Blake and
                  Federico and
                  Henning Jungkurth and
                  Yuan Mingze and
                  vinouz and
                  Dave Coleman and
                  Brendan Burns and
                  Rahul Rawat and
                  Serguei Mokhov and
                  Paul Reynolds and
                  P.E. Viau and
                  Matthieu Fraissinet-Tachet and
                  Ludique and
                  James Billingham and
                  Alistair},
  title  = {libfreenect2: Release 0.2},
  month  = apr,
  year   = 2016,
  doi    = {10.5281/zenodo.50641},
  url    = {https://doi.org/10.5281/zenodo.50641}
}

@inproceedings{Fang2017,
  title     = {{RMPE}: Regional Multi-person Pose Estimation},
  author    = {Fang, Hao-Shu and Xie, Shuqin and Tai, Yu-Wing and Lu, Cewu},
  booktitle = {ICCV},
  year      = {2017}
}

@inproceedings{xiu2018poseflow,
  title     = {{Pose Flow}: Efficient Online Pose Tracking},
  author    = {Xiu, Yuliang and Li, Jiefeng and Wang, Haoyu and Fang, Yinghong and Lu, Cewu},
  booktitle = {BMVC},
  year      = {2018}
}


@misc{x265,
  title   = {x265 HEVC Encoder / H.265 Video Codec},
  url     = {http://x265.org/},
  journal = {x265}
}
@article{OpenPose,
  author        = {Zhe Cao and
               Gines Hidalgo and
               Tomas Simon and
               Shih{-}En Wei and
               Yaser Sheikh},
  title         = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity
               Fields},
  journal       = {CoRR},
  volume        = {abs/1812.08008},
  year          = {2018},
  url           = {http://arxiv.org/abs/1812.08008},
  archiveprefix = {arXiv},
  eprint        = {1812.08008},
  timestamp     = {Wed, 02 Jan 2019 14:40:18 +0100},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1812-08008},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Miller2012,
  author  = {Stephen Miller and Jur van den Berg and Mario Fritz and Trevor Darrell and Ken Goldberg and Pieter Abbeel},
  title   = {A geometric approach to robotic laundry folding},
  journal = {The International Journal of Robotics Research},
  volume  = {31},
  number  = {2},
  pages   = {249-267},
  year    = {2012},
  doi     = {10.1177/0278364911430417},
  url     = { 
        https://doi.org/10.1177/0278364911430417
    
},
  eprint  = { 
        https://doi.org/10.1177/0278364911430417
    
}
}
