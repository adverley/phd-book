@article{Doumanoglou2016,
  author   = {A. {Doumanoglou} and J. {Stria} and G. {Peleka} and I. {Mariolis} and V. {Petrík} and A. {Kargakos} and L. {Wagner} and V. {Hlaváč} and T. {Kim} and S. {Malassiotis}},
  journal  = {IEEE Transactions on Robotics},
  title    = {Folding Clothes Autonomously: A Complete Pipeline},
  year     = {2016},
  volume   = {32},
  number   = {6},
  pages    = {1461-1478},
  keywords = {clothing;image colour analysis;image texture;manipulators;robot vision;service robots;clothes folding;dual-armed robot;machine vision;robotic manipulation;crumpled garments;color information;texture information;grasping point;hanging garment;polygonal models;T-shirts;towels;shorts;Object segmentation;Robots;Deformable objects;Image segmentation;Grasping;Pose estimation;Active vision;clothes;deformable objects;manipulation;perception;random forests;robotics},
  doi      = {10.1109/TRO.2016.2602376},
  issn     = {1552-3098},
  month    = {Dec}
}

@inproceedings{Maitin2010,
  author    = {J. {Maitin-Shepard} and M. {Cusumano-Towner} and J. {Lei} and P. {Abbeel}},
  booktitle = {2010 IEEE International Conference on Robotics and Automation},
  title     = {Cloth grasp point detection based on multiple-view geometric cues with application to robotic towel folding},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {2308-2315},
  keywords  = {clothing;manipulators;mobile robots;object detection;robot vision;service robots;multiple-view geometric cues;robotic towel folding;vision-based grasp point detection algorithm;general-purpose two-armed mobile robotic platform;general purpose manipulators;Robotics and automation;Robot vision systems;Clothing;Collaborative work;Robustness;Manipulators;Machine learning algorithms;USA Councils;Detection algorithms;Mobile robots},
  doi       = {10.1109/ROBOT.2010.5509439},
  issn      = {1050-4729},
  month     = {May}
}


@inproceedings{Matas2018,
  title     = {Sim-to-Real Reinforcement Learning for Deformable Object Manipulation},
  author    = {Matas, Jan and James, Stephen and Davison, Andrew J.},
  booktitle = {Proceedings of The 2nd Conference on Robot Learning},
  pages     = {734--743},
  year      = {2018},
  editor    = {Billard, Aude and Dragan, Anca and Peters, Jan and Morimoto, Jun},
  volume    = {87},
  series    = {Proceedings of Machine Learning Research},
  address   = {},
  month     = {29--31 Oct},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v87/matas18a/matas18a.pdf},
  url       = {http://proceedings.mlr.press/v87/matas18a.html},
  abstract  = {We have seen much recent progress in rigid object manipulation, but interaction with deformable objects has notably lagged behind. Due to the large configuration space of deformable objects, solutions using traditional modelling approaches require significant engineering work. Perhaps then, bypassing the need for explicit modelling and instead learning the control in an end-to-end manner serves as a better approach? Despite the growing interest in the use of end-to-end robot learning approaches, only a small amount of work has focused on their applicability to deformable object manipulation. Moreover, due to the large amount of data needed to learn these end-to-end solutions, an emerging trend is to learn control policies in simulation and then transfer them over to the real world. To date, no work has explored whether it is possible to learn and transfer deformable object policies. We believe that if sim-to-real methods are to be employed further, then it should be possible to learn to interact with a wide variety of objects, and not only rigid objects. In this work, we use a combination of state-of-the-art deep reinforcement learning algorithms to solve the problem of manipulating deformable objects (specifically cloth). We evaluate our approach on three tasks—folding a towel up to a mark, folding a face towel diagonally, and draping a piece of cloth over a hanger. Our agents are fully trained in simulation with domain randomisation, and then successfully deployed in the real world without having seen any real deformable objects.}
}

@inproceedings{Lee2019,
  title     = {Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks},
  author    = {Lee, Michelle A and Zhu, Yuke and Srinivasan, Krishnan and Shah, Parth and Savarese, Silvio and Fei-Fei, Li and  Garg, Animesh and Bohg, Jeannette},
  booktitle = {2019 IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2019},
  url       = {https://arxiv.org/abs/1810.10191}
}

@misc{Tian2019,
  author = {Stephen Tian and Frederik Ebert and Dinesh Jayaraman and Mayur Mudigonda and Chelsea Finn and Roberto Calandra and Sergey Levine},
  title  = {Manipulation by Feel: Touch-Based Control with Deep Predictive Models},
  year   = {2019},
  eprint = {arXiv:1903.04128}
}

@inproceedings{Rahmatizadeh2018,
  author    = {R. {Rahmatizadeh} and P. {Abolghasemi} and L. {Bölöni} and S. {Levine}},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {3758-3765},
  keywords  = {learning (artificial intelligence);manipulators;recurrent neural nets;robot vision;nonprehensile manipulation;recurrent neural network;raw images;VAE-GAN-based reconstruction;autoregressive multimodal action prediction;complex manipulation tasks;towel;weight;reconstruction-based regularization;vision-based multitask manipulation;end-to-end learning;multitask learning;low-cost robotic arm;robot arm trajectories;complex picking and placing tasks;Task analysis;Robots;Feature extraction;Neural networks;Image reconstruction;Training;Visualization},
  doi       = {10.1109/ICRA.2018.8461076},
  issn      = {2577-087X},
  month     = {May}
}

@article{Drimus2014,
  title     = {Design of a flexible tactile sensor for classification of rigid and deformable objects},
  author    = {Drimus, Alin and Kootstra, Gert and Bilberg, Arne and Kragic, Danica},
  journal   = {Robotics and Autonomous Systems},
  volume    = {62},
  number    = {1},
  pages     = {3--15},
  year      = {2014},
  publisher = {Elsevier}
}
@article{Tesauro1994,
  title     = {TD-Gammon, a self-teaching backgammon program, achieves master-level play},
  author    = {Tesauro, Gerald},
  journal   = {Neural computation},
  volume    = {6},
  number    = {2},
  pages     = {215--219},
  year      = {1994},
  publisher = {MIT Press}
}

@article{Foresti2004,
  author   = {G. L. {Foresti} and F. A. {Pellegrino}},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  title    = {Automatic visual recognition of deformable objects for grasping and manipulation},
  year     = {2004},
  volume   = {34},
  number   = {3},
  pages    = {325-333},
  keywords = {self-organising feature maps;image segmentation;image texture;object recognition;image recognition;feature extraction;image colour analysis;automatic visual recognition;deformable objects;vision-based system;self-organized neural network;color image segmentation;image texture information;object recognition;points extraction;morphological analysis;object grasping;object manipulation;Deformable models;Data mining;Robot vision systems;Object recognition;Computer vision;Robotics and automation;Principal component analysis;Neural networks;Image segmentation;Color},
  doi      = {10.1109/TSMCC.2003.819701},
  issn     = {1094-6977},
  month    = {Aug}
}

@article{Levine2018,
  author   = {Sergey Levine and Peter Pastor and Alex Krizhevsky and Julian Ibarz and Deirdre Quillen},
  title    = {Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
  journal  = {The International Journal of Robotics Research},
  volume   = {37},
  number   = {4-5},
  pages    = {421-436},
  year     = {2018},
  doi      = {10.1177/0278364917710318},
  url      = { 
        https://doi.org/10.1177/0278364917710318
    
},
  eprint   = { 
        https://doi.org/10.1177/0278364917710318
    
},
  abstract = { We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. We describe two large-scale experiments that we conducted on two separate robotic platforms. In the first experiment, about 800,000 grasp attempts were collected over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and gripper wear and tear. In the second experiment, we used a different robotic platform and 8 robots to collect a dataset consisting of over 900,000 grasp attempts. The second robotic platform was used to test transfer between robots, and the degree to which data from a different set of robots can be used to aid learning. Our experimental results demonstrate that our approach achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. Our transfer experiment also illustrates that data from different robots can be combined to learn more reliable and effective grasping. }
}

@article{Billard2019,
  title     = {Trends and challenges in robot manipulation},
  author    = {Billard, Aude and Kragic, Danica},
  doi       = {10.1126/science.aat8414},
  journal   = {Science},
  volume    = {364},
  number    = {6446},
  pages     = {eaat8414},
  year      = {2019},
  publisher = {American Association for the Advancement of Science}
}

@inproceedings{Morrison2018,
  title        = {Cartman: The low-cost cartesian manipulator that won the amazon robotics challenge},
  author       = {Morrison, Douglas and Tow, Adam W and McTaggart, M and Smith, R and Kelly-Boxall, N and Wade-McCue, S and Erskine, J and Grinover, R and Gurman, A and Hunn, T and others},
  booktitle    = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {7757--7764},
  year         = {2018},
  organization = {IEEE}
}

@inproceedings{Agrawal2016,
  title     = {Learning to poke by poking: Experiential learning of intuitive physics},
  author    = {Agrawal, Pulkit and Nair, Ashvin V and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {5074--5082},
  year      = {2016}
}

@inproceedings{Ghadirzadeh2017,
  title        = {Deep predictive policy training using reinforcement learning},
  author       = {Ghadirzadeh, Ali and Maki, Atsuto and Kragic, Danica and Bj{\"o}rkman, M{\aa}rten},
  booktitle    = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages        = {2351--2358},
  year         = {2017},
  organization = {IEEE}
}

@inproceedings{Bersch2011,
  author    = {C. {Bersch} and B. {Pitzer} and S. {Kammel}},
  booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title     = {Bimanual robotic cloth manipulation for laundry folding},
  year      = {2011},
  volume    = {},
  number    = {},
  pages     = {1413-1419},
  keywords  = {Grippers;Clothing;Three dimensional displays;Grasping;Service robots;Cameras},
  doi       = {10.1109/IROS.2011.6095109},
  issn      = {2153-0866},
  month     = {Sep.}
}

@article{Saha2007,
  title     = {Manipulation planning for deformable linear objects},
  author    = {Saha, Mitul and Isto, Pekka},
  journal   = {IEEE Transactions on Robotics},
  volume    = {23},
  number    = {6},
  pages     = {1141--1150},
  year      = {2007},
  publisher = {IEEE}
}

@inproceedings{Yamakawa2011,
  author    = {Y. {Yamakawa} and A. {Namiki} and M. {Ishikawa}},
  booktitle = {2011 IEEE International Conference on Robotics and Automation},
  title     = {Motion planning for dynamic folding of a cloth with two high-speed robot hands and two high-speed sliders},
  year      = {2011},
  volume    = {},
  number    = {},
  pages     = {5486-5491},
  keywords  = {algebra;dexterous manipulators;motion control;path planning;velocity control;high-speed robot hands;high-speed slider;dynamic manipulation;sheet-like flexible object;dynamic folding;high-speed multifingered hands;linear flexible object model;algebraic equation;high-speed robot motion;motion planning;Mathematical model;Joints;Dynamics;Deformable models;Robot kinematics;Planning},
  doi       = {10.1109/ICRA.2011.5979606},
  issn      = {1050-4729},
  month     = {May}
}

@article{Tobin2017,
  author        = {Joshua Tobin and
               Rachel Fong and
               Alex Ray and
               Jonas Schneider and
               Wojciech Zaremba and
               Pieter Abbeel},
  title         = {Domain Randomization for Transferring Deep Neural Networks from Simulation
               to the Real World},
  journal       = {CoRR},
  volume        = {abs/1703.06907},
  year          = {2017},
  url           = {http://arxiv.org/abs/1703.06907},
  archiveprefix = {arXiv},
  eprint        = {1703.06907},
  timestamp     = {Mon, 13 Aug 2018 16:48:26 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/TobinFRSZA17},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Seita2018,
  author        = {Daniel Seita and
               Nawid Jamali and
               Michael Laskey and
               Ron Berenstein and
               Ajay Kumar Tanwani and
               Prakash Baskaran and
               Soshi Iba and
               John F. Canny and
               Ken Goldberg},
  title         = {Robot Bed-Making: Deep Transfer Learning Using Depth Sensing of Deformable
               Fabric},
  journal       = {CoRR},
  volume        = {abs/1809.09810},
  year          = {2018},
  url           = {http://arxiv.org/abs/1809.09810},
  archiveprefix = {arXiv},
  eprint        = {1809.09810},
  timestamp     = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1809-09810},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{Arnold2019,
  author  = {Arnold, Solvi and Yamazaki, Kimitoshi},
  year    = {2019},
  month   = {05},
  pages   = {},
  title   = {Fast and Flexible Multi-Step Cloth Manipulation Planning Using an Encode-Manipulate-Decode Network (EM*D Net)},
  volume  = {13},
  journal = {Frontiers in Neurorobotics},
  doi     = {10.3389/fnbot.2019.00022}
}

@article{Tanaka2018,
  author   = {D. {Tanaka} and S. {Arnold} and K. {Yamazaki}},
  journal  = {IEEE Robotics and Automation Letters},
  title    = {EMD Net: An Encode–Manipulate–Decode Network for Cloth Manipulation},
  year     = {2018},
  volume   = {3},
  number   = {3},
  pages    = {1771-1778},
  keywords = {clothing;manipulators;mobile robots;neural nets;path planning;robot vision;EMD net;deep neural network;shape changes;multiple operations;single operation;goal state;current shape state;cloth product;automatic operation;motion planning method;cloth manipulation;encode-manipulate-decode network;Shape;Robots;Clothing;Task analysis;Decoding;Planning;Three-dimensional displays;Cloth manipulation;manipulation planning;convolutional auto encoder;neural network},
  doi      = {10.1109/LRA.2018.2800122},
  issn     = {2377-3766},
  month    = {July}
}

@inproceedings{Lee2015,
  author    = {A. X. {Lee} and A. {Gupta} and H. {Lu} and S. {Levine} and P. {Abbeel}},
  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Learning from multiple demonstrations using trajectory-aware non-rigid registration with applications to deformable object manipulation},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {5265-5272},
  keywords  = {generalisation (artificial intelligence);image registration;learning by example;learning systems;manipulators;robot vision;trajectory control;trajectory-aware nonrigid registration;deformable object manipulation;learning from demonstration;nonrigid point cloud registration;nonlinear transformation;visual variation;generalization;towel folding;object grasping;Trajectory;Three-dimensional displays;Robots;Registers;Splines (mathematics);Yttrium;Probabilistic logic},
  doi       = {10.1109/IROS.2015.7354120},
  issn      = {},
  month     = {Sep.}
}

download as .bib file

@article{Laskey2017,
  author        = {Michael Laskey and
               Chris Powers and
               Ruta Joshi and
               Arshan Poursohi and
               Ken Goldberg},
  title         = {Learning Robust Bed Making using Deep Imitation Learning with {DART}},
  journal       = {CoRR},
  volume        = {abs/1711.02525},
  year          = {2017},
  url           = {http://arxiv.org/abs/1711.02525},
  archiveprefix = {arXiv},
  eprint        = {1711.02525},
  timestamp     = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1711-02525},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Balaguer2011,
  author    = {B. {Balaguer} and S. {Carpin}},
  booktitle = {2011 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title     = {Combining imitation and reinforcement learning to fold deformable planar objects},
  year      = {2011},
  volume    = {},
  number    = {},
  pages     = {1405-1412},
  keywords  = {Learning;Humans;Training data;Manipulators;Deformable models;Training},
  doi       = {10.1109/IROS.2011.6094992},
  issn      = {2153-0866},
  month     = {Sep.}
}

@article{Miller2012,
  author   = {Stephen Miller and Jur van den Berg and Mario Fritz and Trevor Darrell and Ken Goldberg and Pieter Abbeel},
  title    = {A geometric approach to robotic laundry folding},
  journal  = {The International Journal of Robotics Research},
  volume   = {31},
  number   = {2},
  pages    = {249-267},
  year     = {2012},
  doi      = {10.1177/0278364911430417},
  url      = { 
        https://doi.org/10.1177/0278364911430417
    
},
  eprint   = { 
        https://doi.org/10.1177/0278364911430417
    
},
  abstract = { We consider the problem of autonomous robotic laundry folding, and propose a solution to the perception and manipulation challenges inherent to the task. At the core of our approach is a quasi-static cloth model which allows us to neglect the complex dynamics of cloth under significant parts of the state space, allowing us to reason instead in terms of simple geometry. We present an algorithm which, given a 2D cloth polygon and a desired sequence of folds, outputs a motion plan for executing the corresponding manipulations, deemed g-folds, on a minimal number of robot grippers. We define parametrized fold sequences for four clothing categories: towels, pants, short-sleeved shirts, and long-sleeved shirts, each represented as polygons. We then devise a model-based optimization approach for visually inferring the class and pose of a spread-out or folded clothing article from a single image, such that the resulting polygon provides a parse suitable for these folding primitives. We test the manipulation and perception tasks individually, and combine them to implement an autonomous folding system on the Willow Garage PR2. This enables the PR2 to identify a clothing article spread out on a table, execute the computed folding sequence, and visually track its progress over successive folds. }
}

@article{Tsurumine2019,
  title    = {Deep reinforcement learning with smooth policy update: Application to robotic cloth manipulation},
  journal  = {Robotics and Autonomous Systems},
  volume   = {112},
  pages    = {72 - 83},
  year     = {2019},
  issn     = {0921-8890},
  doi      = {https://doi.org/10.1016/j.robot.2018.11.004},
  url      = {http://www.sciencedirect.com/science/article/pii/S0921889018303245},
  author   = {Yoshihisa Tsurumine and Yunduan Cui and Eiji Uchibe and Takamitsu Matsubara},
  keywords = {Deep reinforcement learning, Robotic cloth manipulation, Dynamic policy programming},
  abstract = {Deep Reinforcement Learning (DRL), which can learn complex policies with high-dimensional observations as inputs, e.g., images, has been successfully applied to various tasks. Therefore, it may be suitable to apply them for robots to learn and perform daily activities like washing and folding clothes, cooking, and cleaning since such tasks are difficult for non-DRL methods that often require either (1) direct access to state variables or (2) well-designed hand-engineered features extracted from sensory inputs. However, applying DRL to real robots remains very challenging because conventional DRL algorithms require a huge number of training samples for learning, which is arduous in real robots. To alleviate this dilemma, in this paper, we propose two sample efficient DRL algorithms: Deep P-Network (DPN) and Dueling Deep P-Network (DDPN). The core idea is to combine the nature of smooth policy update with the capability of automatic feature extraction in deep neural networks to enhance the sample efficiency and learning stability with fewer samples. The proposed methods were first investigated by a robot-arm reaching task in the simulation that compared previous DRL methods and applied to two real robotic cloth manipulation tasks: (1) flipping a handkerchief and (2) folding a t-shirt with a limited number of samples. All the results suggest that our method outperformed the previous DRL methods.}
}

@article{Mnih2015,
  title     = {Human-level control through deep reinforcement learning},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal   = {Nature},
  volume    = {518},
  number    = {7540},
  pages     = {529},
  year      = {2015},
  publisher = {Nature Publishing Group}
}


@article{Watkins1992,
  title     = {Q-learning},
  author    = {Watkins, Christopher JCH and Dayan, Peter},
  journal   = {Machine learning},
  volume    = {8},
  number    = {3-4},
  pages     = {279--292},
  year      = {1992},
  publisher = {Springer}
}

@article{Lin1992,
  author   = {Lin, Long-Ji},
  title    = {Self-improving reactive agents based on reinforcement learning, planning and teaching},
  journal  = {Machine Learning},
  year     = {1992},
  month    = {May},
  day      = {01},
  volume   = {8},
  number   = {3},
  pages    = {293--321},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
  issn     = {1573-0565},
  doi      = {10.1007/BF00992699},
  url      = {https://doi.org/10.1007/BF00992699}
}

@inproceedings{Abbeel2004,
  title        = {Apprenticeship learning via inverse reinforcement learning},
  author       = {Abbeel, Pieter and Ng, Andrew Y},
  booktitle    = {Proceedings of the twenty-first international conference on Machine learning},
  pages        = {1},
  year         = {2004},
  organization = {ACM}
}

@inproceedings{Finn2016,
  title     = {Guided cost learning: Deep inverse optimal control via policy optimization},
  author    = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle = {International Conference on Machine Learning},
  pages     = {49--58},
  year      = {2016}
}


% TODO: make sure to cite following papers
@inproceedings{Kruse2015,
  title        = {Collaborative human-robot manipulation of highly deformable materials},
  author       = {Kruse, Daniel and Radke, Richard J and Wen, John T},
  booktitle    = {2015 IEEE international conference on robotics and automation (ICRA)},
  pages        = {3782--3787},
  year         = {2015},
  organization = {IEEE}
}

@inproceedings{Willimon2011,
  author    = {B. {Willimon} and S. {Birchfield} and I. {Walker}},
  booktitle = {2011 IEEE International Conference on Robotics and Automation},
  title     = {Classification of clothing using interactive perception},
  year      = {2011},
  volume    = {},
  number    = {},
  pages     = {1862-1868},
  keywords  = {control engineering computing;robot vision;sensors;user interfaces;clothing classification;interactive perception;visual sensors;item classification;low-level image measurements;article extraction;pants;shorts;short-sleeve shirt;long-sleeve shirt;socks;underwear;robot interaction;laundry;Clothing;Robot sensing systems;Image segmentation;Cameras;Databases},
  doi       = {10.1109/ICRA.2011.5980336},
  issn      = {1050-4729},
  month     = {May}
}

@inproceedings{Zhang2018,
  title        = {Deep imitation learning for complex manipulation tasks from virtual reality teleoperation},
  author       = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
  booktitle    = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {1--8},
  year         = {2018},
  organization = {IEEE}
}

@inproceedings{Gu2017,
  title        = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
  author       = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle    = {2017 IEEE international conference on robotics and automation (ICRA)},
  pages        = {3389--3396},
  year         = {2017},
  organization = {IEEE}
}

@inproceedings{Finn2017,
  title        = {Deep visual foresight for planning robot motion},
  author       = {Finn, Chelsea and Levine, Sergey},
  booktitle    = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {2786--2793},
  year         = {2017},
  organization = {IEEE}
}

@inproceedings{Luo2018,
  title        = {Vitac: Feature sharing between vision and tactile sensing for cloth texture recognition},
  author       = {Luo, Shan and Yuan, Wenzhen and Adelson, Edward and Cohn, Anthony G and Fuentes, Raul},
  booktitle    = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages        = {2722--2727},
  year         = {2018},
  organization = {IEEE}
}

@article{Stoppa2014,
  title     = {Wearable electronics and smart textiles: a critical review},
  author    = {Stoppa, Matteo and Chiolerio, Alessandro},
  journal   = {sensors},
  volume    = {14},
  number    = {7},
  pages     = {11957--11992},
  year      = {2014},
  publisher = {Multidisciplinary Digital Publishing Institute}
}


@misc{Baxter,
  title        = {Baxter SDK information},
  url          = {https://sdk.rethinkrobotics.com/wiki/Arms},
  howpublished = {\url{https://sdk.rethinkrobotics.com/wiki/Arms}},
  journal      = {},
  note         = {(Accessed on 02/06/2020)}
}
